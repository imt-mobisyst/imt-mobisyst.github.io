{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pkg-multibot This repository's goal is to study the coordination of a robot fleet using ROS2 (Iron). We will study different communication methods and architectures to achieve a specific scenario, and compare them based on different criteria. Installation To install all the dependencies, check here . 1. Scenario We want to be able to control an heterogeneous fleet of robots (for example robots from different vendors) in an industrial environment. We will consider the following situation : In a warehouse, there are 2 arrivals of packages . At random time intervals, packages arrive at each arrival zone. Each package has a specific color , and for each color there is a corresponding deposit zone in the map. We will consider 2 possible tasks for the robots : - Store : Once a package arrives at a pickup spot, a robot will be selected to pick up the package, and carry it to the correct deposit zone, depending on its color . - Retrieve : An operator can send a request to retrieve a package from a specific color. A robot will be selected to go to the correct storing position, and bring back a package to the retrieval zone There will be 3 robots working together, coordinating , in order to achieve these tasks in the shortest time possible. This coordination will be achieved thanks to an auction/bid system , that would assign the task to the best robot (based on its position and its waypoints queue). This auction system could either be centralized, with an entity listening all the bids and choosing the best one, or distributed, with each robot comparing its bid with the others. First we'll consider that once a task is assigned to a robot, it can't abandon it and give it to another robot. However, we could later add an intermediate zone , where robots would drop the package they are currently carrying to move to another task, and another robot would be assigned that package, if that solution is globally better for the fleet. As a bonus, it would be interesting to see how well the architecture is able to adapt to new robots dynamically added to the fleet or robot failures. Robot failures would be simulated by sending a message containing the robot ID on a global topic. First, we'll consider that all the robots evolve in a known map. At the end, it would also be interesting to study how the fleet could share information to create a common map with multi robot SLAM algorithms. 2. Communication methods A list of communication methods are implemented in this repo, you can see them here 3. Demos Simulation Demo with Rviz (on the left) and the stage simulator on the right If you want to run the simulation demos, click here Real robots Demo on real real robot. If you want to run the real robot demos, click here","title":"Home"},{"location":"#pkg-multibot","text":"This repository's goal is to study the coordination of a robot fleet using ROS2 (Iron). We will study different communication methods and architectures to achieve a specific scenario, and compare them based on different criteria.","title":"pkg-multibot"},{"location":"#installation","text":"To install all the dependencies, check here .","title":"Installation"},{"location":"#1-scenario","text":"We want to be able to control an heterogeneous fleet of robots (for example robots from different vendors) in an industrial environment. We will consider the following situation : In a warehouse, there are 2 arrivals of packages . At random time intervals, packages arrive at each arrival zone. Each package has a specific color , and for each color there is a corresponding deposit zone in the map. We will consider 2 possible tasks for the robots : - Store : Once a package arrives at a pickup spot, a robot will be selected to pick up the package, and carry it to the correct deposit zone, depending on its color . - Retrieve : An operator can send a request to retrieve a package from a specific color. A robot will be selected to go to the correct storing position, and bring back a package to the retrieval zone There will be 3 robots working together, coordinating , in order to achieve these tasks in the shortest time possible. This coordination will be achieved thanks to an auction/bid system , that would assign the task to the best robot (based on its position and its waypoints queue). This auction system could either be centralized, with an entity listening all the bids and choosing the best one, or distributed, with each robot comparing its bid with the others. First we'll consider that once a task is assigned to a robot, it can't abandon it and give it to another robot. However, we could later add an intermediate zone , where robots would drop the package they are currently carrying to move to another task, and another robot would be assigned that package, if that solution is globally better for the fleet. As a bonus, it would be interesting to see how well the architecture is able to adapt to new robots dynamically added to the fleet or robot failures. Robot failures would be simulated by sending a message containing the robot ID on a global topic. First, we'll consider that all the robots evolve in a known map. At the end, it would also be interesting to study how the fleet could share information to create a common map with multi robot SLAM algorithms.","title":"1. Scenario"},{"location":"#2-communication-methods","text":"A list of communication methods are implemented in this repo, you can see them here","title":"2. Communication methods"},{"location":"#3-demos","text":"","title":"3. Demos"},{"location":"#simulation","text":"Demo with Rviz (on the left) and the stage simulator on the right If you want to run the simulation demos, click here","title":"Simulation"},{"location":"#real-robots","text":"Demo on real real robot. If you want to run the real robot demos, click here","title":"Real robots"},{"location":"installation/","text":"Installation ROS2 This repository is based on ROS2 Iron, which you can install here Libraries You will need to install the following libraries in order to launch the demos : Stage Simulator and Stage ROS2 domain_bridge library bash sudo apt install ros-$ROS_DISTRO-domain-bridge zenoh-bridge-ros2dds people_msgs : bash git clone https://github.com/wg-perception/people.git -b ros2 colcon build --packages-select people_msgs nav2_social_costmap_plugin bash git clone https://github.com/robotics-upo/nav2_social_costmap_plugin.git -b humble colcon build --packages-select nav2_social_costmap_plugin","title":"Install"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#ros2","text":"This repository is based on ROS2 Iron, which you can install here","title":"ROS2"},{"location":"installation/#libraries","text":"You will need to install the following libraries in order to launch the demos : Stage Simulator and Stage ROS2 domain_bridge library bash sudo apt install ros-$ROS_DISTRO-domain-bridge zenoh-bridge-ros2dds people_msgs : bash git clone https://github.com/wg-perception/people.git -b ros2 colcon build --packages-select people_msgs nav2_social_costmap_plugin bash git clone https://github.com/robotics-upo/nav2_social_costmap_plugin.git -b humble colcon build --packages-select nav2_social_costmap_plugin","title":"Libraries"},{"location":"methods/","text":"Communication methods Here is the list of the different communication methods that have been implemented in this repository : Namespacing Different domain IDs DDS Discovery servers DDS Partitions Zenoh Namespacing See working demos here Namespaces are prefixes to node names, topics, actions and services. They allow to have multiple elements with the same name but different prefix. In a multi-robot scenario, namespacing is the easiest solution to separate each robot with a unique namespace, in order for robots to not have name conflicts when running the same nodes and using the same topics. An example of this is to prefix the /cmd_vel topic for robots ( /wall-e/cmd_vel and /r2d2/cmd_vel ), to prevent them from having the same velocity command. With our multi-robot architecture, we would have a configuration like the following : Different domain IDs See working demos here ROS2 uses DDS ( D ata D istribution S ervice) as the default middleware for communication. DDS allows nodes to discover other nodes that are on the same network. In order to create different logical networks, DDS provides a feature called the domain ID . Each node is allowed to communicate to nodes that are on the same ID, but can't communicate with nodes on other domain IDs. In ROS2, the default domain ID is 0, but it can be configured using the ROS_DOMAIN_ID env variable (between 0 and 101 inclusive). The domain ID is then mapped to a UDP port, thus creating application isolation. In a multi-robot scenario, assigning a different ROS_DOMAIN_ID to each robot allows to completely isolate them from the others. However, using the domain_bridge library, we can create a bridge between different domain IDs, and specify which topics should be broadcasted towards another domain ID (which would be shared between robots). This library allows us to run multiple nodes in the same OS process, in order to share data and \"bridge\" topics/services/actions from one DOMAIN_ID to another one. With our multi-robot architecture, we would have the following configuration : DDS Discovery servers See working demos here As stated before, DDS is the protocol used by ROS2 for communicating between nodes. One aspect of this protocol is to look for elements that a node can communicate with on the network. It's the \"Discovery protocol\". By default, the Simple Discovery protocol is used, which consists in sending multicast messages to every other node in the network. Fast DDS, one of the DDS middlewares, provides a Discovery server to replace the Simple Discovery protocol . It works similarly to a router and allows to isolate DDS subnets. Each node can choose which DDS Discovery servers (it can be more than 1) it connects to using the ROS_DISCOVERY_SERVER env variable. Its main purpose is to reduce the network traffic induced by the discovery phase. Listener 1 discovers topics from Talker 1 & 2 but Listener 2 only discovers topics from Talker 1 A discovery server is described by : - its IP address - its port - its ID In our multi-robot scenario, we could use this Discovery server to isolate nodes running on the robot, by connecting them to a DDS Discovery server running locally. Nodes that also need to communicate to other robots would connect to both their local DDS server and either a global one or another robot's one. With our multi-robot architecture, we would have the following configuration : DDS partitions See working demos here As stated before, DDS is the protocol used by ROS2 for communicating between nodes. DDS introduced the concept of partitions : each partition is defined by a name ( string ), and only elements that have a partition in common can communicate. Contrary to the DOMAIN_ID, nodes still receive the broadcast discovery messages (since they are on the same DOMAIN_ID they have the same UDP port) but drop them if they don't have a partition in common. Partitions can be applied to specific nodes, but also more precisely publishers/subscribers (DataReaders/DataWriters in DDS terms) . To configure this, you can create an XML file and apply it by setting the FASTRTPS_DEFAULT_PROFILES_FILE=/path/to/file.xml env variable. In our multi-robot scenario, we could have one partition for each robot ( robot_X ). Topics that need to stay local would be published to that partition and topics that need to be shared across robots would be published in the shared partition. With our multi-robot architecture, we would have the following configuration : Zenoh See working demos here Eclipse's Zenoh is a communication protocol based on the Publish/Subscribe mechanism. It has grown in popularity as a potential replacement for DDS in ROS2, thanks to its reduced number of discovery messages. Zenoh works by providing a Router, that centralizes the discovery information, and connects to other Routers across networks to share the information about publishers and subscribers it has access to. That way, Zenoh allows to reduce the discovery traffic, but standard direct TCP communication is used once communication is established. This router can be configured (using a JSON file) to only allow certain publisher/subscriber discovery informations to be shared to other routers, and even choose through which network interface they should be available. That way, Zenoh provides a really powerful level of configuration to achieve isolation. Zenoh provides 2 options to be integrated in ROS2 : - rmw_zenoh - zenoh-bridge-ros2dds Both options provide very similar configuration options. As rmw_zenoh didn't provide binaries for the ARM architecture, running it on a Raspberry PI would have required to cross-compile it since it was taking too long on the Raspberry PI itself. For that reason, the zenoh-bridge-ros2dds will be studied in this paper, but the conclusions should be very similar for rmw_zenoh . zenoh-bridge-ros2dds works by creating a node that listens to the DDS traffic on the local machine, translates the received discovery messages and then send them via the Zenoh Router. In a multi-robot scenario, one zenoh-bridge-ros2dds could be used for each physical computer. Publishers/Subscribers that should stay local would not be on the allow list in the configuration file, whereas those who need to be shared would be. The bridge could even be configured to restrict the network interfaces used to further configure the isolation. With a multi-robot architecture, you would have the following configuration :","title":"Methods"},{"location":"methods/#communication-methods","text":"Here is the list of the different communication methods that have been implemented in this repository : Namespacing Different domain IDs DDS Discovery servers DDS Partitions Zenoh","title":"Communication methods"},{"location":"methods/#namespacing","text":"See working demos here Namespaces are prefixes to node names, topics, actions and services. They allow to have multiple elements with the same name but different prefix. In a multi-robot scenario, namespacing is the easiest solution to separate each robot with a unique namespace, in order for robots to not have name conflicts when running the same nodes and using the same topics. An example of this is to prefix the /cmd_vel topic for robots ( /wall-e/cmd_vel and /r2d2/cmd_vel ), to prevent them from having the same velocity command. With our multi-robot architecture, we would have a configuration like the following :","title":"Namespacing"},{"location":"methods/#different-domain-ids","text":"See working demos here ROS2 uses DDS ( D ata D istribution S ervice) as the default middleware for communication. DDS allows nodes to discover other nodes that are on the same network. In order to create different logical networks, DDS provides a feature called the domain ID . Each node is allowed to communicate to nodes that are on the same ID, but can't communicate with nodes on other domain IDs. In ROS2, the default domain ID is 0, but it can be configured using the ROS_DOMAIN_ID env variable (between 0 and 101 inclusive). The domain ID is then mapped to a UDP port, thus creating application isolation. In a multi-robot scenario, assigning a different ROS_DOMAIN_ID to each robot allows to completely isolate them from the others. However, using the domain_bridge library, we can create a bridge between different domain IDs, and specify which topics should be broadcasted towards another domain ID (which would be shared between robots). This library allows us to run multiple nodes in the same OS process, in order to share data and \"bridge\" topics/services/actions from one DOMAIN_ID to another one. With our multi-robot architecture, we would have the following configuration :","title":"Different domain IDs"},{"location":"methods/#dds-discovery-servers","text":"See working demos here As stated before, DDS is the protocol used by ROS2 for communicating between nodes. One aspect of this protocol is to look for elements that a node can communicate with on the network. It's the \"Discovery protocol\". By default, the Simple Discovery protocol is used, which consists in sending multicast messages to every other node in the network. Fast DDS, one of the DDS middlewares, provides a Discovery server to replace the Simple Discovery protocol . It works similarly to a router and allows to isolate DDS subnets. Each node can choose which DDS Discovery servers (it can be more than 1) it connects to using the ROS_DISCOVERY_SERVER env variable. Its main purpose is to reduce the network traffic induced by the discovery phase. Listener 1 discovers topics from Talker 1 & 2 but Listener 2 only discovers topics from Talker 1 A discovery server is described by : - its IP address - its port - its ID In our multi-robot scenario, we could use this Discovery server to isolate nodes running on the robot, by connecting them to a DDS Discovery server running locally. Nodes that also need to communicate to other robots would connect to both their local DDS server and either a global one or another robot's one. With our multi-robot architecture, we would have the following configuration :","title":"DDS Discovery servers"},{"location":"methods/#dds-partitions","text":"See working demos here As stated before, DDS is the protocol used by ROS2 for communicating between nodes. DDS introduced the concept of partitions : each partition is defined by a name ( string ), and only elements that have a partition in common can communicate. Contrary to the DOMAIN_ID, nodes still receive the broadcast discovery messages (since they are on the same DOMAIN_ID they have the same UDP port) but drop them if they don't have a partition in common. Partitions can be applied to specific nodes, but also more precisely publishers/subscribers (DataReaders/DataWriters in DDS terms) . To configure this, you can create an XML file and apply it by setting the FASTRTPS_DEFAULT_PROFILES_FILE=/path/to/file.xml env variable. In our multi-robot scenario, we could have one partition for each robot ( robot_X ). Topics that need to stay local would be published to that partition and topics that need to be shared across robots would be published in the shared partition. With our multi-robot architecture, we would have the following configuration :","title":"DDS partitions"},{"location":"methods/#zenoh","text":"See working demos here Eclipse's Zenoh is a communication protocol based on the Publish/Subscribe mechanism. It has grown in popularity as a potential replacement for DDS in ROS2, thanks to its reduced number of discovery messages. Zenoh works by providing a Router, that centralizes the discovery information, and connects to other Routers across networks to share the information about publishers and subscribers it has access to. That way, Zenoh allows to reduce the discovery traffic, but standard direct TCP communication is used once communication is established. This router can be configured (using a JSON file) to only allow certain publisher/subscriber discovery informations to be shared to other routers, and even choose through which network interface they should be available. That way, Zenoh provides a really powerful level of configuration to achieve isolation. Zenoh provides 2 options to be integrated in ROS2 : - rmw_zenoh - zenoh-bridge-ros2dds Both options provide very similar configuration options. As rmw_zenoh didn't provide binaries for the ARM architecture, running it on a Raspberry PI would have required to cross-compile it since it was taking too long on the Raspberry PI itself. For that reason, the zenoh-bridge-ros2dds will be studied in this paper, but the conclusions should be very similar for rmw_zenoh . zenoh-bridge-ros2dds works by creating a node that listens to the DDS traffic on the local machine, translates the received discovery messages and then send them via the Zenoh Router. In a multi-robot scenario, one zenoh-bridge-ros2dds could be used for each physical computer. Publishers/Subscribers that should stay local would not be on the allow list in the configuration file, whereas those who need to be shared would be. The bridge could even be configured to restrict the network interfaces used to further configure the isolation. With a multi-robot architecture, you would have the following configuration :","title":"Zenoh"},{"location":"real/","text":"Real robot demos These are the commands to run the warehouse demo on real robots. Setup We are using 3 Kobuki robots, each with a Raspberry PI 3B mounted on top and running the drivers (for the motors and the 2D Lidar). Each robot is connected (either with a cable or trough the wifi network) to a controller PC (Dell XPS 13 7390) running the high level control (nav2, multi-robot communication etc.). [!NOTE] To run these demos, you'll have to : - clone the mb6-space repository on the PIs, and checkout on the multibot branch - clone this repository on the operator and controller PCs Methods 1. Namespaces \u26a0\ufe0f NOT WORKING On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ROS_DOMAIN_ID=99 ros2 launch multibot multibot.launch.py type:=\"namespace\" On the operator PC : ros2 launch multibot warehouse_namespace_operator.py On the Controller PCs : ros2 launch multibot warehouse_namespace_launch.py robot_id:=22 ros2 launch multibot warehouse_namespace_launch.py robot_id:=23 ros2 launch multibot warehouse_namespace_launch.py robot_id:=24 2. Domain ID Bridge On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ros2 launch multibot multibot.launch.py type:=\"domain_id\" On the operator PC : ros2 launch multibot warehouse_bridge_operator.py On the Controller PCs : ros2 launch multibot warehouse_bridge_launch.py robot_id:=22 operator_domain_id:=99 ros2 launch multibot warehouse_bridge_launch.py robot_id:=23 operator_domain_id:=99 ros2 launch multibot warehouse_bridge_launch.py robot_id:=24 operator_domain_id:=99 3. FastDDS Discovery servers \u26a0\ufe0f HALF WORKING On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ROS_DOMAIN_ID=99 ros2 launch multibot multibot.launch.py type:=\"discovery\" On the operator PC : ros2 launch multibot warehouse_dds_operator.py On the Controller PCs : ros2 launch multibot warehouse_dds_launch.py robot_id:=22 robot_ip:=10.89.5.22 subnet_dds_server:=\"10.89.5.90:11811\" use_bridge:=false ros2 launch multibot warehouse_dds_launch.py robot_id:=23 robot_ip:=10.89.5.23 subnet_dds_server:=\"10.89.5.90:11811\" use_bridge:=false ros2 launch multibot warehouse_dds_launch.py robot_id:=24 robot_ip:=10.89.5.24 subnet_dds_server:=\"10.89.5.90:11811\" use_bridge:=false Notes : - robot_ip should be 192.168.1.1 when connected in ethernet - use_bridge:=true creates an infinite loop 4. DDS partitions \u26a0\ufe0f NOT WORKING On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ROS_DOMAIN_ID=99 ros2 launch multibot multibot.launch.py type:=\"partitions\" On the operator PC : ros2 launch multibot warehouse_partition_operator.py On the Controller PCs : ros2 launch multibot warehouse_partition_launch.py robot_id:=22 ros2 launch multibot warehouse_partition_launch.py robot_id:=23 ros2 launch multibot warehouse_partition_launch.py robot_id:=24 5. Zenoh DDS Bridge On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file : ros2 launch multibot multibot.launch.py type:=\"zenoh\" On the operator PC : ros2 launch multibot warehouse_zenoh_operator.py On the Controller PCs : ros2 launch multibot warehouse_zenoh_launch.py robot_id:=22 robot_ip:=10.89.5.22 operator_ip:=10.89.5.90 ros2 launch multibot warehouse_zenoh_launch.py robot_id:=23 robot_ip:=10.89.5.23 operator_ip:=10.89.5.90 ros2 launch multibot warehouse_zenoh_launch.py robot_id:=24 robot_ip:=10.89.5.24 operator_ip:=10.89.5.90 Note : robot_ip should be 192.168.1.1 when connected in ethernet","title":"In Real"},{"location":"real/#real-robot-demos","text":"These are the commands to run the warehouse demo on real robots.","title":"Real robot demos"},{"location":"real/#setup","text":"We are using 3 Kobuki robots, each with a Raspberry PI 3B mounted on top and running the drivers (for the motors and the 2D Lidar). Each robot is connected (either with a cable or trough the wifi network) to a controller PC (Dell XPS 13 7390) running the high level control (nav2, multi-robot communication etc.). [!NOTE] To run these demos, you'll have to : - clone the mb6-space repository on the PIs, and checkout on the multibot branch - clone this repository on the operator and controller PCs","title":"Setup"},{"location":"real/#methods","text":"","title":"Methods"},{"location":"real/#1-namespaces","text":"\u26a0\ufe0f NOT WORKING On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ROS_DOMAIN_ID=99 ros2 launch multibot multibot.launch.py type:=\"namespace\" On the operator PC : ros2 launch multibot warehouse_namespace_operator.py On the Controller PCs : ros2 launch multibot warehouse_namespace_launch.py robot_id:=22 ros2 launch multibot warehouse_namespace_launch.py robot_id:=23 ros2 launch multibot warehouse_namespace_launch.py robot_id:=24","title":"1. Namespaces"},{"location":"real/#2-domain-id-bridge","text":"On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ros2 launch multibot multibot.launch.py type:=\"domain_id\" On the operator PC : ros2 launch multibot warehouse_bridge_operator.py On the Controller PCs : ros2 launch multibot warehouse_bridge_launch.py robot_id:=22 operator_domain_id:=99 ros2 launch multibot warehouse_bridge_launch.py robot_id:=23 operator_domain_id:=99 ros2 launch multibot warehouse_bridge_launch.py robot_id:=24 operator_domain_id:=99","title":"2. Domain ID Bridge"},{"location":"real/#3-fastdds-discovery-servers","text":"\u26a0\ufe0f HALF WORKING On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ROS_DOMAIN_ID=99 ros2 launch multibot multibot.launch.py type:=\"discovery\" On the operator PC : ros2 launch multibot warehouse_dds_operator.py On the Controller PCs : ros2 launch multibot warehouse_dds_launch.py robot_id:=22 robot_ip:=10.89.5.22 subnet_dds_server:=\"10.89.5.90:11811\" use_bridge:=false ros2 launch multibot warehouse_dds_launch.py robot_id:=23 robot_ip:=10.89.5.23 subnet_dds_server:=\"10.89.5.90:11811\" use_bridge:=false ros2 launch multibot warehouse_dds_launch.py robot_id:=24 robot_ip:=10.89.5.24 subnet_dds_server:=\"10.89.5.90:11811\" use_bridge:=false Notes : - robot_ip should be 192.168.1.1 when connected in ethernet - use_bridge:=true creates an infinite loop","title":"3. FastDDS Discovery servers"},{"location":"real/#4-dds-partitions","text":"\u26a0\ufe0f NOT WORKING On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file ROS_DOMAIN_ID=99 ros2 launch multibot multibot.launch.py type:=\"partitions\" On the operator PC : ros2 launch multibot warehouse_partition_operator.py On the Controller PCs : ros2 launch multibot warehouse_partition_launch.py robot_id:=22 ros2 launch multibot warehouse_partition_launch.py robot_id:=23 ros2 launch multibot warehouse_partition_launch.py robot_id:=24","title":"4. DDS partitions"},{"location":"real/#5-zenoh-dds-bridge","text":"On the Raspberry PI : Uncomment this line in the mb6-space/bin/start-tbot.sh file : ros2 launch multibot multibot.launch.py type:=\"zenoh\" On the operator PC : ros2 launch multibot warehouse_zenoh_operator.py On the Controller PCs : ros2 launch multibot warehouse_zenoh_launch.py robot_id:=22 robot_ip:=10.89.5.22 operator_ip:=10.89.5.90 ros2 launch multibot warehouse_zenoh_launch.py robot_id:=23 robot_ip:=10.89.5.23 operator_ip:=10.89.5.90 ros2 launch multibot warehouse_zenoh_launch.py robot_id:=24 robot_ip:=10.89.5.24 operator_ip:=10.89.5.90 Note : robot_ip should be 192.168.1.1 when connected in ethernet","title":"5. Zenoh DDS Bridge"},{"location":"simulation/","text":"Simulation demos In this README you'll see all the commands needed to run all the demos. For each solution, multiple demos are implemented. There are 3 type of demos (with different complexity levels) : 1. A very simple talker/listener example (except for the namespacing) 2. Multiple turtlesims running in parallel : you can place a goal_pose in Rviz and one of the robots will be assigned to it 3. One with the stage simulator, with 3 robots in a warehouse environment. It perfectly illustrates the scenario explained here : - when you add a clicked_point in Rviz, packages start to spawn at specific places in the map, and the robots coordinate to bring the packages to the correct deposit spot depending on their color. - when you send a message on the retrieve topic, one of the robots retrieves a package from the correct deposit zone and brings it to the retrieval zone 1. Robot separation using namespaces Namespacing allows to add a prefix before every node, topic, service... in a launchfile. That way, they allow to avoid conflicts between data from different robots. a. Turtlesim demo To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator and rviz ) : ros2 launch multibot turtlesim_namespace_launch.py nb_robots:=\"3\" b. Stage demo To start the demo, you can use the following commands, that will take care of starting everything ( rviz , stage , controller and operator ) : ros2 launch multibot rviz_launch.py config:=config/stage.rviz ros2 launch multibot stage_namespace_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) . 2. Multi DOMAIN_ID communication We will be using the domain_bridge library that allows us to run multiple nodes in the same OS process, in order to \"bridge\" topics/services/actions from one DOMAIN_ID to another one. To install it : apt install ros-iron-domain-bridge Note : The library doesn't seem to be maintained any more : last commit on January 2nd 2023. Maybe have a look at DDS Router or try to reproduce how it works inside our own communication nodes. a. Simple test with a talker and a listener We only need to create a configuration file, telling the library which topics/services/actions need to be transmitted, from which ROS_DOMAIN_ID and to which ROS_DOMAIN_ID (see talker_bridge_config.yaml ) To start the bridge, we use the following command : ros2 run domain_bridge domain_bridge <path_to>/bridge_config.yaml We can run this command from any terminal, regardless of the ROS_DOMAIN_ID In another terminal, start the talker node : ROS_DOMAIN_ID=2 ros2 run demo_nodes_cpp talker In another terminal, start the listener node : ROS_DOMAIN_ID=3 ros2 run demo_nodes_cpp listener b. Test with multiple turtlesim on the same computer Each \"robot\" will be linked to a unique domain ID ( bot_domain_id ) . In each domain ID, there will be a turtlesim_node and a turtlesim_controller (which will control the movement of the turtle towards the goal poses). To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator et rviz ) : ros2 launch multibot turtlesim_bridge_launch.py nb_robots:=\"3\" However, if you want to manually start the nodes yourself, here are the commands : Starting the turtle in different terminals (nodes will be started with the correct ROS_DOMAIN_ID given as an argument): ros2 launch multibot turtlesim_bridge_robot_launch.py bot_domain_id:=\"10\" operator_domain_id:=\"1\" ros2 launch multibot turtlesim_bridge_robot_launch.py bot_domain_id:=\"11\" operator_domain_id:=\"1\" Bridge nodes will be started by the launchfiles to transmit the topics needed by nodes running in the operator_domain_id . Then, we'll start the operator node (which is responsible for managing priority between turtles) in this domain : ROS_DOMAIN_ID=1 ros2 run multibot static_operator.py --ros-args -p nb_robots:=2 Running rviz in the operator's ROS_DOMAIN_ID : ROS_DOMAIN_ID=1 ros2 launch multibot rviz_turtlesim_launch.py In order to send goal points for the turtles to go to, you can press the 2D Goal Pose in Rviz or run the following command : ROS_DOMAIN_ID=1 ros2 topic pub /goal_pose geometry_msgs/msg/PoseStamped \"{pose: {position: {x: 9, y: 9.0, z: 0.0}}}\" --once c. Test with the stage simulator We'll use the following domain IDs : - 0,1,2... for the different robots - 99 for the operator/rviz - 100 for the simulation Start Rviz in the correct domain ID : export ROS_DOMAIN_ID=99 export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp ros2 launch multibot rviz_launch.py config:=config/stage.rviz Note : We use CycloneDDS here because there is a bug in the domain_bridge library that causes the bidirectional bridge configuration to create an infinite loop when using rmw_fastrtps_cpp Launch the demo (with the simulator, the controllers, the nav2 stacks...): export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp ros2 launch multibot stage_bridge_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) . 3. Network isolation with FastDDS Discovery server DDS is the default protocol used by ROS2 for communicating between nodes. One aspect of this protocol is to look for elements that a node can communicate with on the newtwork. It's the \"Discovery protocol\". In our use case, we'll use Fast DDS Discovery server , which works similarly to a router and allows to isolate DDS subnets. a. Simple test with a talker and a listener We are going to start multiple DDS Discovery servers : - one to isolate a \"local\" network, which port is 11811 . This one will emulate one that would be on a robot computer. - one on the common network between robots, which port is 11812 . This one will emulate one that would be on the operator computer. fastdds discovery -i 0 -l 127.0.0.1 -p 11811 # Local fastdds discovery -i 1 -l 127.0.0.1 -p 11812 # Shared Test 1 : We check that a local talker node can be listened by a \"local\" node and a \"common\" node BUT not on a node on the operator # Emulates a local node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811\" ros2 run demo_nodes_cpp talker # Emulates a local node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811\" ros2 run demo_nodes_cpp listener # Emulates a subnet node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811;127.0.0.1:11812\" ros2 run demo_nodes_cpp listener # # Emulates a node on the operator export ROS_DISCOVERY_SERVER=\";127.0.0.1:11812\" ros2 run demo_nodes_cpp listener The first 2 listeners should receive the published messages, but not the \"operator\" node. Test 2 : We check that a talker in the common network can be listened by a local node and also another node in the common network # Emulates a subnet node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811;127.0.0.1:11812\" ros2 run demo_nodes_cpp talker # Emulates a local node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811\" ros2 run demo_nodes_cpp listener # Emulates another subnet node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811;127.0.0.1:11812\" ros2 run demo_nodes_cpp listener # Emulates a node on the operator export ROS_DISCOVERY_SERVER=\";127.0.0.1:11812\" ros2 run demo_nodes_cpp listener All 3 listeners should receive the published messages. Note : By default, the ROS2 CLI creates a node in order to listen for other nodes/topics in the network. For this to work with the DDS Discovery server architecture, we need to configure ROS2 as a \"Super client\" . This can be done thanks to the super_client_config.xml configuration file and the following command : bash export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/super_client_config.xml ros2 daemon stop && ros2 daemon start # We restart the daemon to take the changes into account However, be careful because you will now have access to ALL nodes in the graph, without any isolation of the network . In order to choose the discovery servers you want to connect to, you can comment the <RemoteServer> that don't interest you. If you only want access to the common network on the operator , run the following commands : bash export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/super_client_operator_config.xml ros2 daemon stop && ros2 daemon start # We restart the daemon to take the changes into account b. Test with multiple turtlesim on the same computer Each robot will host its own \"local\" DDS Discovery server, allowing communication between its internal nodes. Nodes that need to communicate with the outside elements (other robots/operator) will also connect to the operator's DDS Discovery server. To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator et rviz ) : ros2 launch multibot turtlesim_dds_launch.py nb_robots:=\"3\" However, if you want to manually start the nodes yourself, here are the commands : Starting the DDS Discovery servers : fastdds discovery -i 0 -l 127.0.0.1 -p 11811 # Local DDS discovery server for robot 1 fastdds discovery -i 1 -l 127.0.0.1 -p 11812 # Local DDS discovery server for robot 1 fastdds discovery -i 2 -l 127.0.0.1 -p 11813 # Local DDS discovery server for robot 1 fastdds discovery -i 3 -l 127.0.0.1 -p 11814 # Common DDS discovery server, on the operator Note : Here, in this demo, we use different ports to emulate different machines. In reality, each robot will only host one local DDS Discovery server, and the last one will be hosted on the operator. Starting the turtle ( turtlesim et turtle_controller ) in different terminals (nodes will be started connecting automatically to the correct DDS Discovery server(s)) : ros2 launch multibot turtlesim_dds_robot_launch.py local_dds_server:=\"127.0.0.1:11811\" subnet_dds_server:=\"127.0.0.1:11814\" nb_robots:=\"3\" robot_id:=\"1\" ros2 launch multibot turtlesim_dds_robot_launch.py local_dds_server:=\"127.0.0.1:11812\" subnet_dds_server:=\"127.0.0.1:11814\" nb_robots:=\"3\" robot_id:=\"2\" ros2 launch multibot turtlesim_dds_robot_launch.py local_dds_server:=\"127.0.0.1:11813\" subnet_dds_server:=\"127.0.0.1:11814\" nb_robots:=\"3\" robot_id:=\"3\" Starting the operator node : export ROS_DISCOVERY_SERVER=\";;;127.0.0.1:11814\" ros2 run multibot static_operator.py --ros-args -p nb_robots:=3 Starting rviz : export ROS_DISCOVERY_SERVER=\";;;127.0.0.1:11814\" ros2 launch multibot rviz_turtlesim_launch.py In order to send goal points for the turtles to go to, you can press the 2D Goal Pose in Rviz or run the following command : ros2 topic pub /goal_pose geometry_msgs/msg/PoseStamped \"{pose: {position: {x: 9, y: 9.0, z: 0.0}}}\" --once c. Test with multiple turtlesim on different computers (pibot) First we need to install the turtlesim library on each pibot : sudo apt install ros-iron-turtlesim They will all be setup on ROS_DOMAIN_ID=99 . The operator PC will have a static IP address (here it's 10.89.5.90 ). On the operator PC : ros2 launch multibot pibot_dds_operator.py nb_robots:=\"2\" common_dds_ip:=\"10.89.5.90\" common_dds_port:=\"11811\" On each pibot : ros2 launch multibot pibot_turtlesim_dds_launch.py nb_robots:=2 operator_server:=\"10.89.5.90:11811\" d. Test with the stage simulator All DDS servers will be hosted on the current machine, with IP 127.0.0.1 . We'll use the following DDS server ports : - 11811 for the simulation - 11812 for the operator/rviz - 11813,111814,11815... for the different robots To launch rviz, run the following commands : export FASTRTPS_DEFAULT_PROFILES_FILE=/path/to/super_client_subnet_config.xml ros2 daemon stop && ros2 daemon start ros2 launch multibot rviz_launch.py config:=config/stage.rviz In another terminal, launch the demo (with the simulator, the controllers, the nav2 stacks...): ros2 launch multibot stage_dds_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) . 4. Robot isolation using DDS partitions DDS is the protocol used by ROS2 for communicating between nodes. DDS introduced a way to isolate DataWriters (Publishers) and DataReaders (Subscribers) called DDS partitions For a Publisher to communicate with a Subscriber, they have to belong at least to one common partition. a. Simple test with a talker and a listener In a first terminal, run a first talker (in the chatter1 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/talker_config.xml ros2 run demo_nodes_cpp talker --ros-args -r chatter:=chatter1 In a second terminal, run a second talker (in the chatter2 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/talker_config.xml ros2 run demo_nodes_cpp talker --ros-args -r chatter:=chatter2 In a third terminal, run a first listener (to the chatter1 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/listener_config.xml ros2 run demo_nodes_cpp listener --ros-args -r chatter:=chatter1 In a last terminal, run a second listener (to the chatter2 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/listener_config.xml ros2 run demo_nodes_cpp listener --ros-args -r chatter:=chatter2 Only the first listener should receive data, since the second one doesn't have a partition in common with the publisher. All of the nodes and topics will be visible with ros2 node list and ros2 topic list However, ros2 topic echo <topic_name> will only work on data published on the default partition ( \"\" ). If you want to echo topics published on other partitions, just use the configuration file that connects to every partition ( \"*\" ) except the default one ( \"\" ) : export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/cli_config.xml ros2 topic echo /chatter2 b. Test with multiple turtlesim on the same computer Each \"robot\" will have its own DDS partition ( robotX ). Their nodes will be communicating through topics with that partition. Topics that need to be shared across robots will be in the shared partition To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator et rviz ) : ros2 launch multibot turtlesim_partition_launch.py nb_robots:=\"3\" However, if you want to manually start the nodes yourself, here are the commands : Starting the turtle ( turtlesim et turtle_controller ) in different terminals (nodes will be started with the created config files to use their own partition robotX and the shared partition): ros2 launch multibot turtlesim_partition_robot_launch.py nb_robots:=\"3\" robot_id:=\"1\" ros2 launch multibot turtlesim_partition_robot_launch.py nb_robots:=\"3\" robot_id:=\"2\" ros2 launch multibot turtlesim_partition_robot_launch.py nb_robots:=\"3\" robot_id:=\"3\" Starting the operator node : export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/operator_config.xml ros2 run multibot static_operator.py --ros-args -p nb_robots:=3 Starting rviz : export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/operator_config.xml ros2 launch multibot rviz_turtlesim_launch.py c. Test with the stage simulator We'll use the following DDS partitions : - robot_0 , robot_1 , robot_2 ... for the different robots - shared for the subnet (operator/rviz) To launch rviz, run the following commands : export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/operator_config.xml ros2 launch multibot rviz_launch.py config:=config/stage.rviz In another terminal, launch the demo (with the simulator, the controllers, the nav2 stacks...): ros2 launch multibot stage_partition_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) . 5. Robot isolation with domain ID and Zenoh a. Simple test with a talker and a listener First we will start the 2 Zenoh bridges, in 2 different terminals : ROS_DOMAIN_ID=1 zenoh-bridge-ros2dds -c /path/to/bridge_config_talker.json5 ROS_DOMAIN_ID=2 zenoh-bridge-ros2dds -c /path/to/bridge_config_listener.json5 Then we will launch the talker and listener nodes, and prevent the DDS communication between nodes by starting them in different domain IDs ROS_DOMAIN_ID=1 ros2 run demo_nodes_cpp talker ROS_DOMAIN_ID=2 ros2 run demo_nodes_cpp listener b. Test with the stage simulator Each \"robot\" will have its own Zenoh bridge as well as its own namespace ( robotX ). Topics that need to be shared across robots will be specified in the Zenoh bridge allow configuration. To launch rviz, run the following commands : ROS_DOMAIN_ID=99 ros2 launch multibot rviz_launch.py config:=config/stage.rviz In another terminal, launch the demo (with the simulator, the controllers, the nav2 stacks...): ros2 launch multibot stage_zenoh_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) .","title":"Simulation demos"},{"location":"simulation/#simulation-demos","text":"In this README you'll see all the commands needed to run all the demos. For each solution, multiple demos are implemented. There are 3 type of demos (with different complexity levels) : 1. A very simple talker/listener example (except for the namespacing) 2. Multiple turtlesims running in parallel : you can place a goal_pose in Rviz and one of the robots will be assigned to it 3. One with the stage simulator, with 3 robots in a warehouse environment. It perfectly illustrates the scenario explained here : - when you add a clicked_point in Rviz, packages start to spawn at specific places in the map, and the robots coordinate to bring the packages to the correct deposit spot depending on their color. - when you send a message on the retrieve topic, one of the robots retrieves a package from the correct deposit zone and brings it to the retrieval zone","title":"Simulation demos"},{"location":"simulation/#1-robot-separation-using-namespaces","text":"Namespacing allows to add a prefix before every node, topic, service... in a launchfile. That way, they allow to avoid conflicts between data from different robots.","title":"1. Robot separation using namespaces"},{"location":"simulation/#a-turtlesim-demo","text":"To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator and rviz ) : ros2 launch multibot turtlesim_namespace_launch.py nb_robots:=\"3\"","title":"a. Turtlesim demo"},{"location":"simulation/#b-stage-demo","text":"To start the demo, you can use the following commands, that will take care of starting everything ( rviz , stage , controller and operator ) : ros2 launch multibot rviz_launch.py config:=config/stage.rviz ros2 launch multibot stage_namespace_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) .","title":"b. Stage demo"},{"location":"simulation/#2-multi-domain_id-communication","text":"We will be using the domain_bridge library that allows us to run multiple nodes in the same OS process, in order to \"bridge\" topics/services/actions from one DOMAIN_ID to another one. To install it : apt install ros-iron-domain-bridge Note : The library doesn't seem to be maintained any more : last commit on January 2nd 2023. Maybe have a look at DDS Router or try to reproduce how it works inside our own communication nodes.","title":"2. Multi DOMAIN_ID communication"},{"location":"simulation/#a-simple-test-with-a-talker-and-a-listener","text":"We only need to create a configuration file, telling the library which topics/services/actions need to be transmitted, from which ROS_DOMAIN_ID and to which ROS_DOMAIN_ID (see talker_bridge_config.yaml ) To start the bridge, we use the following command : ros2 run domain_bridge domain_bridge <path_to>/bridge_config.yaml We can run this command from any terminal, regardless of the ROS_DOMAIN_ID In another terminal, start the talker node : ROS_DOMAIN_ID=2 ros2 run demo_nodes_cpp talker In another terminal, start the listener node : ROS_DOMAIN_ID=3 ros2 run demo_nodes_cpp listener","title":"a. Simple test with a talker and a listener"},{"location":"simulation/#b-test-with-multiple-turtlesim-on-the-same-computer","text":"Each \"robot\" will be linked to a unique domain ID ( bot_domain_id ) . In each domain ID, there will be a turtlesim_node and a turtlesim_controller (which will control the movement of the turtle towards the goal poses). To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator et rviz ) : ros2 launch multibot turtlesim_bridge_launch.py nb_robots:=\"3\" However, if you want to manually start the nodes yourself, here are the commands : Starting the turtle in different terminals (nodes will be started with the correct ROS_DOMAIN_ID given as an argument): ros2 launch multibot turtlesim_bridge_robot_launch.py bot_domain_id:=\"10\" operator_domain_id:=\"1\" ros2 launch multibot turtlesim_bridge_robot_launch.py bot_domain_id:=\"11\" operator_domain_id:=\"1\" Bridge nodes will be started by the launchfiles to transmit the topics needed by nodes running in the operator_domain_id . Then, we'll start the operator node (which is responsible for managing priority between turtles) in this domain : ROS_DOMAIN_ID=1 ros2 run multibot static_operator.py --ros-args -p nb_robots:=2 Running rviz in the operator's ROS_DOMAIN_ID : ROS_DOMAIN_ID=1 ros2 launch multibot rviz_turtlesim_launch.py In order to send goal points for the turtles to go to, you can press the 2D Goal Pose in Rviz or run the following command : ROS_DOMAIN_ID=1 ros2 topic pub /goal_pose geometry_msgs/msg/PoseStamped \"{pose: {position: {x: 9, y: 9.0, z: 0.0}}}\" --once","title":"b. Test with multiple turtlesim on the same computer"},{"location":"simulation/#c-test-with-the-stage-simulator","text":"We'll use the following domain IDs : - 0,1,2... for the different robots - 99 for the operator/rviz - 100 for the simulation Start Rviz in the correct domain ID : export ROS_DOMAIN_ID=99 export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp ros2 launch multibot rviz_launch.py config:=config/stage.rviz Note : We use CycloneDDS here because there is a bug in the domain_bridge library that causes the bidirectional bridge configuration to create an infinite loop when using rmw_fastrtps_cpp Launch the demo (with the simulator, the controllers, the nav2 stacks...): export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp ros2 launch multibot stage_bridge_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) .","title":"c. Test with the stage simulator"},{"location":"simulation/#3-network-isolation-with-fastdds-discovery-server","text":"DDS is the default protocol used by ROS2 for communicating between nodes. One aspect of this protocol is to look for elements that a node can communicate with on the newtwork. It's the \"Discovery protocol\". In our use case, we'll use Fast DDS Discovery server , which works similarly to a router and allows to isolate DDS subnets.","title":"3. Network isolation with FastDDS Discovery server"},{"location":"simulation/#a-simple-test-with-a-talker-and-a-listener_1","text":"We are going to start multiple DDS Discovery servers : - one to isolate a \"local\" network, which port is 11811 . This one will emulate one that would be on a robot computer. - one on the common network between robots, which port is 11812 . This one will emulate one that would be on the operator computer. fastdds discovery -i 0 -l 127.0.0.1 -p 11811 # Local fastdds discovery -i 1 -l 127.0.0.1 -p 11812 # Shared Test 1 : We check that a local talker node can be listened by a \"local\" node and a \"common\" node BUT not on a node on the operator # Emulates a local node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811\" ros2 run demo_nodes_cpp talker # Emulates a local node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811\" ros2 run demo_nodes_cpp listener # Emulates a subnet node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811;127.0.0.1:11812\" ros2 run demo_nodes_cpp listener # # Emulates a node on the operator export ROS_DISCOVERY_SERVER=\";127.0.0.1:11812\" ros2 run demo_nodes_cpp listener The first 2 listeners should receive the published messages, but not the \"operator\" node. Test 2 : We check that a talker in the common network can be listened by a local node and also another node in the common network # Emulates a subnet node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811;127.0.0.1:11812\" ros2 run demo_nodes_cpp talker # Emulates a local node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811\" ros2 run demo_nodes_cpp listener # Emulates another subnet node on the robot export ROS_DISCOVERY_SERVER=\"127.0.0.1:11811;127.0.0.1:11812\" ros2 run demo_nodes_cpp listener # Emulates a node on the operator export ROS_DISCOVERY_SERVER=\";127.0.0.1:11812\" ros2 run demo_nodes_cpp listener All 3 listeners should receive the published messages. Note : By default, the ROS2 CLI creates a node in order to listen for other nodes/topics in the network. For this to work with the DDS Discovery server architecture, we need to configure ROS2 as a \"Super client\" . This can be done thanks to the super_client_config.xml configuration file and the following command : bash export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/super_client_config.xml ros2 daemon stop && ros2 daemon start # We restart the daemon to take the changes into account However, be careful because you will now have access to ALL nodes in the graph, without any isolation of the network . In order to choose the discovery servers you want to connect to, you can comment the <RemoteServer> that don't interest you. If you only want access to the common network on the operator , run the following commands : bash export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/super_client_operator_config.xml ros2 daemon stop && ros2 daemon start # We restart the daemon to take the changes into account","title":"a. Simple test with a talker and a listener"},{"location":"simulation/#b-test-with-multiple-turtlesim-on-the-same-computer_1","text":"Each robot will host its own \"local\" DDS Discovery server, allowing communication between its internal nodes. Nodes that need to communicate with the outside elements (other robots/operator) will also connect to the operator's DDS Discovery server. To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator et rviz ) : ros2 launch multibot turtlesim_dds_launch.py nb_robots:=\"3\" However, if you want to manually start the nodes yourself, here are the commands : Starting the DDS Discovery servers : fastdds discovery -i 0 -l 127.0.0.1 -p 11811 # Local DDS discovery server for robot 1 fastdds discovery -i 1 -l 127.0.0.1 -p 11812 # Local DDS discovery server for robot 1 fastdds discovery -i 2 -l 127.0.0.1 -p 11813 # Local DDS discovery server for robot 1 fastdds discovery -i 3 -l 127.0.0.1 -p 11814 # Common DDS discovery server, on the operator Note : Here, in this demo, we use different ports to emulate different machines. In reality, each robot will only host one local DDS Discovery server, and the last one will be hosted on the operator. Starting the turtle ( turtlesim et turtle_controller ) in different terminals (nodes will be started connecting automatically to the correct DDS Discovery server(s)) : ros2 launch multibot turtlesim_dds_robot_launch.py local_dds_server:=\"127.0.0.1:11811\" subnet_dds_server:=\"127.0.0.1:11814\" nb_robots:=\"3\" robot_id:=\"1\" ros2 launch multibot turtlesim_dds_robot_launch.py local_dds_server:=\"127.0.0.1:11812\" subnet_dds_server:=\"127.0.0.1:11814\" nb_robots:=\"3\" robot_id:=\"2\" ros2 launch multibot turtlesim_dds_robot_launch.py local_dds_server:=\"127.0.0.1:11813\" subnet_dds_server:=\"127.0.0.1:11814\" nb_robots:=\"3\" robot_id:=\"3\" Starting the operator node : export ROS_DISCOVERY_SERVER=\";;;127.0.0.1:11814\" ros2 run multibot static_operator.py --ros-args -p nb_robots:=3 Starting rviz : export ROS_DISCOVERY_SERVER=\";;;127.0.0.1:11814\" ros2 launch multibot rviz_turtlesim_launch.py In order to send goal points for the turtles to go to, you can press the 2D Goal Pose in Rviz or run the following command : ros2 topic pub /goal_pose geometry_msgs/msg/PoseStamped \"{pose: {position: {x: 9, y: 9.0, z: 0.0}}}\" --once","title":"b. Test with multiple turtlesim on the same computer"},{"location":"simulation/#c-test-with-multiple-turtlesim-on-different-computers-pibot","text":"First we need to install the turtlesim library on each pibot : sudo apt install ros-iron-turtlesim They will all be setup on ROS_DOMAIN_ID=99 . The operator PC will have a static IP address (here it's 10.89.5.90 ). On the operator PC : ros2 launch multibot pibot_dds_operator.py nb_robots:=\"2\" common_dds_ip:=\"10.89.5.90\" common_dds_port:=\"11811\" On each pibot : ros2 launch multibot pibot_turtlesim_dds_launch.py nb_robots:=2 operator_server:=\"10.89.5.90:11811\"","title":"c. Test with multiple turtlesim on different computers (pibot)"},{"location":"simulation/#d-test-with-the-stage-simulator","text":"All DDS servers will be hosted on the current machine, with IP 127.0.0.1 . We'll use the following DDS server ports : - 11811 for the simulation - 11812 for the operator/rviz - 11813,111814,11815... for the different robots To launch rviz, run the following commands : export FASTRTPS_DEFAULT_PROFILES_FILE=/path/to/super_client_subnet_config.xml ros2 daemon stop && ros2 daemon start ros2 launch multibot rviz_launch.py config:=config/stage.rviz In another terminal, launch the demo (with the simulator, the controllers, the nav2 stacks...): ros2 launch multibot stage_dds_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) .","title":"d. Test with the stage simulator"},{"location":"simulation/#4-robot-isolation-using-dds-partitions","text":"DDS is the protocol used by ROS2 for communicating between nodes. DDS introduced a way to isolate DataWriters (Publishers) and DataReaders (Subscribers) called DDS partitions For a Publisher to communicate with a Subscriber, they have to belong at least to one common partition.","title":"4. Robot isolation using DDS partitions"},{"location":"simulation/#a-simple-test-with-a-talker-and-a-listener_2","text":"In a first terminal, run a first talker (in the chatter1 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/talker_config.xml ros2 run demo_nodes_cpp talker --ros-args -r chatter:=chatter1 In a second terminal, run a second talker (in the chatter2 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/talker_config.xml ros2 run demo_nodes_cpp talker --ros-args -r chatter:=chatter2 In a third terminal, run a first listener (to the chatter1 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/listener_config.xml ros2 run demo_nodes_cpp listener --ros-args -r chatter:=chatter1 In a last terminal, run a second listener (to the chatter2 topic): export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/listener_config.xml ros2 run demo_nodes_cpp listener --ros-args -r chatter:=chatter2 Only the first listener should receive data, since the second one doesn't have a partition in common with the publisher. All of the nodes and topics will be visible with ros2 node list and ros2 topic list However, ros2 topic echo <topic_name> will only work on data published on the default partition ( \"\" ). If you want to echo topics published on other partitions, just use the configuration file that connects to every partition ( \"*\" ) except the default one ( \"\" ) : export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/cli_config.xml ros2 topic echo /chatter2","title":"a. Simple test with a talker and a listener"},{"location":"simulation/#b-test-with-multiple-turtlesim-on-the-same-computer_2","text":"Each \"robot\" will have its own DDS partition ( robotX ). Their nodes will be communicating through topics with that partition. Topics that need to be shared across robots will be in the shared partition To start the demo, you can use the following command, that will take care of starting everything ( turtle , operator et rviz ) : ros2 launch multibot turtlesim_partition_launch.py nb_robots:=\"3\" However, if you want to manually start the nodes yourself, here are the commands : Starting the turtle ( turtlesim et turtle_controller ) in different terminals (nodes will be started with the created config files to use their own partition robotX and the shared partition): ros2 launch multibot turtlesim_partition_robot_launch.py nb_robots:=\"3\" robot_id:=\"1\" ros2 launch multibot turtlesim_partition_robot_launch.py nb_robots:=\"3\" robot_id:=\"2\" ros2 launch multibot turtlesim_partition_robot_launch.py nb_robots:=\"3\" robot_id:=\"3\" Starting the operator node : export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/operator_config.xml ros2 run multibot static_operator.py --ros-args -p nb_robots:=3 Starting rviz : export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/operator_config.xml ros2 launch multibot rviz_turtlesim_launch.py","title":"b. Test with multiple turtlesim on the same computer"},{"location":"simulation/#c-test-with-the-stage-simulator_1","text":"We'll use the following DDS partitions : - robot_0 , robot_1 , robot_2 ... for the different robots - shared for the subnet (operator/rviz) To launch rviz, run the following commands : export RMW_IMPLEMENTATION=rmw_fastrtps_cpp export RMW_FASTRTPS_USE_QOS_FROM_XML=1 export FASTRTPS_DEFAULT_PROFILES_FILE=path/to/operator_config.xml ros2 launch multibot rviz_launch.py config:=config/stage.rviz In another terminal, launch the demo (with the simulator, the controllers, the nav2 stacks...): ros2 launch multibot stage_partition_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) .","title":"c. Test with the stage simulator"},{"location":"simulation/#5-robot-isolation-with-domain-id-and-zenoh","text":"","title":"5. Robot isolation with domain ID and Zenoh"},{"location":"simulation/#a-simple-test-with-a-talker-and-a-listener_3","text":"First we will start the 2 Zenoh bridges, in 2 different terminals : ROS_DOMAIN_ID=1 zenoh-bridge-ros2dds -c /path/to/bridge_config_talker.json5 ROS_DOMAIN_ID=2 zenoh-bridge-ros2dds -c /path/to/bridge_config_listener.json5 Then we will launch the talker and listener nodes, and prevent the DDS communication between nodes by starting them in different domain IDs ROS_DOMAIN_ID=1 ros2 run demo_nodes_cpp talker ROS_DOMAIN_ID=2 ros2 run demo_nodes_cpp listener","title":"a. Simple test with a talker and a listener"},{"location":"simulation/#b-test-with-the-stage-simulator","text":"Each \"robot\" will have its own Zenoh bridge as well as its own namespace ( robotX ). Topics that need to be shared across robots will be specified in the Zenoh bridge allow configuration. To launch rviz, run the following commands : ROS_DOMAIN_ID=99 ros2 launch multibot rviz_launch.py config:=config/stage.rviz In another terminal, launch the demo (with the simulator, the controllers, the nav2 stacks...): ros2 launch multibot stage_zenoh_launch.py To start spawning packages, use the Publish Point button in rviz (it will toggle the package spawning). To retrieve a package with a specific color, run ros2 topic pub /retrieve std_msgs/msg/String \"{data: 'green'}\" --once (other colors: yellow , blue , red ) .","title":"b. Test with the stage simulator"}]}