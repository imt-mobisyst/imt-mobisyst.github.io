<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Camera Driver - Lct. Mobile-Robot</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../style.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Camera Driver";
        var mkdocs_page_input_path = "tuto-level-up/camera-driver.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Lct. Mobile-Robot
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="" href="../../..">MobiSyst <</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Intro</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Kick-Off</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-kick-off/basics/">OS and Shell</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-kick-off/first-contact/">Nodes and Topics</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-kick-off/package/">ROS Package</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-kick-off/move/">Move the Robot</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-kick-off/simulation/">Simulation</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Level-up</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../range-sensor/">Range Sensor</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../parameters/">Parameters</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Camera Driver</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Mastering</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-mastering/slam/">S.L.A.M</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-mastering/transform/">Move To</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuto-mastering/vision/">Vision</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Challenge</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../challenge/kick-off/">Kick-off</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../challenge/challenge-1on2/">Challenge 1</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../challenge/challenge-2on2/">Challenge 2</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendix</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../appendix/installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../appendix/faq/">F.A.Q</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../pdf/"></a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Lct. Mobile-Robot</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Level-up</li>
      <li class="breadcrumb-item active">Camera Driver</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="camera-driver">Camera Driver</h1>
<p>This tutorial cover the basis of the integration of a new sensor in <em>ROS 2</em> environment.
In our case we will integrate <a href="https://www.intelrealsense.com/introducing-intel-realsense-d400-product-family/">Realsense D400 RGBDi</a> camera.</p>
<h2 id="drivers">Drivers</h2>
<p>First, the integration of a sensor require to identify the driver (the piece of code permiting to communicate with a devices - hardware level) and the API (Application Programming interface).</p>
<p>Concretly, we mostly seek for the appropriate <a href="https://en.wikipedia.org/wiki/Library_(computing)">librairies</a> correctly integrated to our system.</p>
<p>Id√©aly the community already support the desired librairies (like for <a href="https://www.libsdl.org/">libsdl2</a> for instance, simple C lib "to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware").
By searching for <code>libsdl2</code> with Ubuntu-Aptitude we will find several packages ready to be installed:</p>
<pre><code class="language-console">apt search libsdl2
</code></pre>
<ul>
<li><code>libsdl2-x.x</code> runing librairies (installed if programs use <em>SDL2</em>)</li>
<li><code>libsdl2-dev</code> development file (to install if you plan to develop a program based on <em>SDL2</em>)</li>
<li>and some extra libs.</li>
</ul>
<p>Other wise, we have to build/compile the driver from source code.
In case of realsense, <a href="https://github.com/IntelRealSense/librealsense">librealsense</a> recommand to use <code>vcpkg</code> to build and install it.</p>
<p>Normally, after installation, you can run a small script to request the cam (more on <a href="https://dev.intelrealsense.com/docs/python2">dev.intelrealsense.com</a>):</p>
<pre><code class="language-python">#!/usr/bin/env python3

###############################################
##              Simple Request               ##
###############################################

import pyrealsense2 as rs

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()

# Get device product line for setting a supporting resolution
pipeline_wrapper = rs.pipeline_wrapper(pipeline)
pipeline_profile = config.resolve(pipeline_wrapper)
device = pipeline_profile.get_device()
device_product_line = str(device.get_info(rs.camera_info.product_line))

print( f&quot;Connect: {device_product_line}&quot; )
for s in device.sensors:
    print( &quot;Name:&quot; + s.get_info(rs.camera_info.name) )
</code></pre>
<p>Copy the code on a <code>test-camera.py</code> file and process it (<code>python3 test-camera.py</code>).
Try this script with severals cameras, not all the Realsense provide for IMU (accelerations) information.</p>
<h2 id="opencv2-the-queen-of-the-vision-librairie">OpenCV2 - the queen of the vision librairie.</h2>
<p>Next we can try to visualise the image flux in a windows.
For that we will use <a href="https://opencv.org/">OpenCV2</a> librairy (an open source computer vision library).</p>
<p>The next script, adapted from the oficial documentation, connect the camera, and display both the image and distance image in an infinite loop (<code>while True</code>).</p>
<ol>
<li>Based on librealsense, the script activates the expected data flux, with the wanted configuration (848x480 imagein a given format at 60 Hertz):</li>
</ol>
<pre><code class="language-python">config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60)
config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)
</code></pre>
<ol>
<li>
<p>Start the aquisition process (<code>pipeline.start(config)</code>)</p>
</li>
<li>
<p>Still with librealsense, the script wait for incomming data and get them:</p>
</li>
</ol>
<pre><code>frames = pipeline.wait_for_frames()

depth_frame = frames.first(rs.stream.depth)
color_frame = frames.first(rs.stream.color)
</code></pre>
<ol>
<li>Then, the reminder of the script consists in converting and displaying the data based on <em>Numpy</em> and <em>OpenCV</em></li>
</ol>
<pre><code class="language-python">#!/usr/bin/env python3
## Doc: https://dev.intelrealsense.com/docs/python2

###############################################
##      Open CV and Numpy integration        ##
###############################################

import pyrealsense2 as rs
import signal, time, numpy as np
import sys, cv2, rclpy

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()

# Get device product line for setting a supporting resolution
pipeline_wrapper = rs.pipeline_wrapper(pipeline)
pipeline_profile = config.resolve(pipeline_wrapper)
device = pipeline_profile.get_device()
device_product_line = str(device.get_info(rs.camera_info.product_line))

print( f&quot;Connect: {device_product_line}&quot; )
found_rgb = True
for s in device.sensors:
    print( &quot;Name:&quot; + s.get_info(rs.camera_info.name) )
    if s.get_info(rs.camera_info.name) == 'RGB Camera':
        found_rgb = True

if not (found_rgb):
    print(&quot;Depth camera equired !!!&quot;)
    exit(0)

config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60)
config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)

# Capture ctrl-c event
isOk= True
def signalInteruption(signum, frame):
    global isOk
    print( &quot;\nCtrl-c pressed&quot; )
    isOk= False

signal.signal(signal.SIGINT, signalInteruption)

# Start streaming
pipeline.start(config)

count= 1
refTime= time.process_time()
freq= 60

sys.stdout.write(&quot;-&quot;)

while isOk:
    # Wait for a coherent tuple of frames: depth, color and accel
    frames = pipeline.wait_for_frames()

    color_frame = frames.first(rs.stream.color)
    depth_frame = frames.first(rs.stream.depth)

    if not (depth_frame and color_frame):
        continue

    # Convert images to numpy arrays
    depth_image = np.asanyarray(depth_frame.get_data())
    color_image = np.asanyarray(color_frame.get_data())

    # Apply colormap on depth image (image must be converted to 8-bit per pixel first)
    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

    depth_colormap_dim = depth_colormap.shape
    color_colormap_dim = color_image.shape

    sys.stdout.write( f&quot;\r- {color_colormap_dim} - {depth_colormap_dim} - ({round(freq)} fps)&quot; )

    # Show images
    images = np.hstack((color_image, depth_colormap))

    # Show images
    cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)
    cv2.imshow('RealSense', images)
    cv2.waitKey(1)

    # Frequency:
    if count == 10 :
        newTime= time.process_time()
        freq= 10/((newTime-refTime))
        refTime= newTime
        count= 0
    count+= 1

# Stop streaming
print(&quot;\nEnding...&quot;)
pipeline.stop()
</code></pre>
<p>To notice the use of <code>signal</code> python librairy that permit to catch and process interuption signal (<code>ctrl-c</code>).</p>
<h2 id="publish-sensor-data">Publish sensor-data</h2>
<p>Stating from the previous script, the goal is to encapsulated the connection to the camera into a <em>ROS2</em> <em>Node</em> in a <code>tuto_vision</code> package (only the camera image flux).</p>
<p>Considering previous developed <em>ROS2</em> <em>Node</em> :</p>
<ul>
<li>We want to keep the control on the infinite loop.</li>
<li>We will publish <code>sensor_msgs/image</code>. The <a href="https://github.com/ros2/common_interfaces/tree/galactic/sensor_msgs">ros2 documentation</a> is verry poor, but <a href="http://wiki.ros.org/sensor_msgs?distro=noetic"><em>ROS1</em> wiki</a> remains valuable.</li>
</ul>
<h3 id="node-structure">Node structure:</h3>
<p>To control the infinite loop we will prefer <code>spin_once</code> to <code>spin</code>.
Notice that <code>spin_once</code> process once the ROS2 instructions.
It blocks until an event occurs.
It is possible to overpass that by specifying a timeout (<code>spin_once(myNode, timeout_sec=0.01)</code>).</p>
<p>At the end we want a ROS2 Node that connect the camera and publish continously the images (color and depth images).
The python function of the Node will look like:</p>
<pre><code class="language-python"># Node processes:
def process_img(args=None):
    rclpy.init(args=args)
    rsNode= Realsense()
    while isOk:
        rsNode.read_imgs()
        rsNode.publish_imgs()
        rclpy.spin_once(rsNode, timeout_sec=0.001)
    # Stop streaming
    print(&quot;Ending...&quot;)
    rsNode.pipeline.stop()
    # Clean end
    rsNode.destroy_node()
    rclpy.shutdown()
</code></pre>
<p>You ca start from a blanc class <code>Realsence</code> and fill the differents methods step by steps (testing your code at each steps):</p>
<pre><code class="language-python"># Realsense Node:
class Realsense(Node):
    def __init__(self, fps= 60):
        super().__init__('realsense')

    def read_imgs(self):
        pass

    def publish_imgs(self):
        pass
</code></pre>
<h3 id="publish-images">Publish Images:</h3>
<ul>
<li><code>sensor_msgs</code> include <a href="http://docs.ros.org/en/api/std_msgs/html/msg/Header.html">header</a> to state for spacio-temporal information. Mainly the reference frame (ie. <code>cam</code> for instance) and time. For time stamp, <code>get_clock()</code> permits to get a clock of a Node instance (<code>node= Node()</code> or <code>self</code> in case of ineritance) then <code>now()</code> and <code>to_msg()</code> methods respectivelly provide curent <code>time()</code> and convert it into a msg compliant format.</li>
</ul>
<pre><code class="language-python">msg.header.stamp = node.get_clock().now().to_msg()
</code></pre>
<p>Then it is possible to feed <code>sensor_msgs/image</code> attributs (starting with <code>msg.encoding= "bgr8"</code> seems a good idea.)</p>
<p>However, a librairy provides some tool to work both with ROS and OpenCV (<a href="http://wiki.ros.org/cv_bridge">cv_bridge</a>).
The code for image to ROS message is:</p>
<pre><code class="language-python">from cv_bridge import CvBridge

self.bridge=CvBridge()

msg_image = self.bridge.cv2_to_imgmsg(color_image,&quot;bgr8&quot;)
msg_image.header.stamp = self.get_clock().now().to_msg()
msg_image.header.frame_id = &quot;image&quot;
self.image_publisher.publish(msg_image)
</code></pre>
<p>The code for depth image to ROS message is:</p>
<pre><code class="language-python">from cv_bridge import CvBridge

self.bridge=CvBridge()

# Utilisation de colormap sur l'image depth de la Realsense (image convertie en 8-bit par pixel)
depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,&quot;bgr8&quot;)
msg_depth.header.stamp = msg_image.header.stamp
msg_depth.header.frame_id = &quot;depth&quot;
self.depth_publisher.publish(msg_depth)
</code></pre>
<h3 id="some-test">Some test:</h3>
<p>At this point it is not relevant any more to show the images inside a <code>CV2</code> window or to compute and print a frequency.</p>
<p>The frequency can be conputed with ROS2 tool: <code>ros2 topic hz \img</code>.</p>
<p>The images are displayable into rviz2 program.</p>
<h2 id="going-futher">Going futher:</h2>
<p>Play with the other streams provided with the <em>RealSens</em> sensor.</p>
<p>Code to add both infrared channels</p>
<pre><code class="language-python">self.config.enable_stream(rs.stream.infrared, 1, 848, 480, rs.format.y8, 60)
self.config.enable_stream(rs.stream.infrared, 2, 848, 480, rs.format.y8, 60)

self.infra_publisher_1 = self.create_publisher(Image, 'infrared_1',10)
self.infra_publisher_2 = self.create_publisher(Image, 'infrared_2',10)
infra_image_1 = np.asanyarray(infra_frame_1.get_data())
infra_image_2 = np.asanyarray(infra_frame_2.get_data())

in the loop :

    infra_frame_1 = frames.get_infrared_frame(1)
    infra_frame_2 = frames.get_infrared_frame(2)
    # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel)
    infra_colormap_1 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_1, alpha=0.03), cv2.COLORMAP_JET)

    # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel)
    infra_colormap_2 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_2, alpha=0.03), cv2.COLORMAP_JET)

    msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_1,&quot;bgr8&quot;)
    msg_infra.header.stamp = msg_image.header.stamp
    msg_infra.header.frame_id = &quot;infrared_1&quot;
    self.infra_publisher_1.publish(msg_infra)

    msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_2,&quot;bgr8&quot;)
    msg_infra.header.stamp = msg_image.header.stamp
    msg_infra.header.frame_id = &quot;infrared_2&quot;
    self.infra_publisher_2.publish(msg_infra)

</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../parameters/" class="btn btn-neutral float-left" title="Parameters"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../tuto-mastering/slam/" class="btn btn-neutral float-right" title="S.L.A.M">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../parameters/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../tuto-mastering/slam/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../javascripts/mathjax.js"></script>
      <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
