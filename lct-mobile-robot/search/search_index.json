{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lecture - Software and Architecture for Mobile-Robotics Here is a lecture on software and architecture for mobile robots at the IMT-Nord-Europe engineering school. This lecture is an introduction on how to develop a modular control and supervision software for a mobile platform. The notions are illustrated with tutorials based on Linux/ ROS and Kobuki/Turtlebot2 robot. This support is shared on github and published thanks to GitHub pages . Furthermore, the purpose of this lecture and its tutorials is not to cover all ROS elements. The tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. To notice that other solutions are provided by ROS2 with potentially more efficient results. Tutorials Tutorials are organized on several levels of complexity: Kick-Off focus on basic concept 'terminal commands', interconnected software developments on simulations and with a real mobile platform. Level-up deep into __ROS2_ to develop a complete packaged solution of reactive control. Mastering , with a good understanding of ROS2 tools and API, the goal now is to achieve complex missions including mapping and localization. Challenge The evaluation mainly consists in the realization of an application involving specific challenges: Autonomous Control of an AGV (Automated Guided Vehicle) Mapping and Localization Research and recognition of an object Going further Most of the content and supports for learning robotics architecture are already shared on the internet. We try to guide the students through project realizations rather than to provide an exhaustive definition of concepts and implementations. This course relies on the ROS middleware for practical sessions, the ROS doc tutorials and ros-packages' descriptions: docs.ros.org . You also can find an excellent virtual working environment and resources on TheConstruct . Contact For comments, questions, corrections, feel free to contact: Guillaume Lozenguez (coordinator, but not the only author here).","title":"Intro"},{"location":"#lecture-software-and-architecture-for-mobile-robotics","text":"Here is a lecture on software and architecture for mobile robots at the IMT-Nord-Europe engineering school. This lecture is an introduction on how to develop a modular control and supervision software for a mobile platform. The notions are illustrated with tutorials based on Linux/ ROS and Kobuki/Turtlebot2 robot. This support is shared on github and published thanks to GitHub pages . Furthermore, the purpose of this lecture and its tutorials is not to cover all ROS elements. The tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. To notice that other solutions are provided by ROS2 with potentially more efficient results.","title":"Lecture - Software and Architecture for Mobile-Robotics"},{"location":"#tutorials","text":"Tutorials are organized on several levels of complexity: Kick-Off focus on basic concept 'terminal commands', interconnected software developments on simulations and with a real mobile platform. Level-up deep into __ROS2_ to develop a complete packaged solution of reactive control. Mastering , with a good understanding of ROS2 tools and API, the goal now is to achieve complex missions including mapping and localization.","title":"Tutorials"},{"location":"#challenge","text":"The evaluation mainly consists in the realization of an application involving specific challenges: Autonomous Control of an AGV (Automated Guided Vehicle) Mapping and Localization Research and recognition of an object","title":"Challenge"},{"location":"#going-further","text":"Most of the content and supports for learning robotics architecture are already shared on the internet. We try to guide the students through project realizations rather than to provide an exhaustive definition of concepts and implementations. This course relies on the ROS middleware for practical sessions, the ROS doc tutorials and ros-packages' descriptions: docs.ros.org . You also can find an excellent virtual working environment and resources on TheConstruct .","title":"Going further"},{"location":"#contact","text":"For comments, questions, corrections, feel free to contact: Guillaume Lozenguez (coordinator, but not the only author here).","title":"Contact"},{"location":"appendix/faq/","text":"Frequent Asked Question... Multi-computer configuration Configure a ROS_DOMAIN_ID betwwen 1 and 100 in your bashrc file to limit the topics area, and set ROS_LOCALHOST_ONLY to zero to permits several computer to share ROS ressources. gedit ~/.bashrc add at the end of the file (with 42 for instance): export ROS_LOCALHOST_ONLY= 0 export ROS_DOMAIN_ID= 42 ROS2 tbot driver How to get rid of ROS1 ;-) Install ROS2 tbot driver : cd ~/ros2_ws/pkg-tbot git pull ./script/install-kobuki_ros.sh cd ~/ros2_ws colcon build Launch ROS2 tbot driver : # base only ros2 launch tbot_start base.launch.py # base + laser ros2 launch tbot_start minimal.launch.py # base + with laser + camera ros2 launch tbot_start full.launch.py ros2 topic list Fix malformed packets Info there: https://github.com/kobuki-base/kobuki_core/commit/2bc11a1bf0ff364b37eb812a404f124dae9c0699 sudo cp /home/bot/ros2_ws/pkg-kobuki/kobuki_core/60-kobuki.rules /lib/udev/rules.d/ Then unplug / replug the robot. To ensure it worked, the followwing command should display 1: cat /sys/bus/usb-serial/devices/ttyUSB0/tty/ttyUSB0/device/latency_timer How to install Ubuntu ? Ubuntu is a fork of the Debian project, a Linux-based desktop operating system. Official website : https://ubuntu.com/ French community : https://www.ubuntu-fr.org/ Notice that, ubuntu can be installed in double boot mode in parallel to a Microsoft OS on your personal computer. It is not recommended to use Ubuntu+ROS in a virtual machine (the performances would be poor). To-do: Install Ubuntu 20.04 LTS (Long Term Supported version) from live USB-Key Ideally, use all the hard disk (you can split the disk in advance for double-boot install ) Configure \"bot\" username and \"bot\" password. Configure network Login and upgrade your installation sudo apt update sudo apt upgrade There is no Wifi on my dell-xps13 ? Connect with cable Get the appropriate drivers: killer driver sudo add-apt-repository ppa:canonical-hwe-team/ backport-iwlwifi sudo apt-get update sudo apt-get install backport-iwlwifi-dkms reboot Remove password asking for docker commands sudo echo \"\\n%sudo ALL=(ALL) NOPASSWD: /usr/bin/docker\\n\" >> /etc/sudoers Catkin_create_pkg - invalid email ? you can use the -m option to force an author name. catin_create_pkg -m AuthorName package_name [dependencies...] opencv_createsamples The latest OpenCV does not include opencv_createsamples . Let's compile an older version (~5 min on labs PC). git clone https://github.com/opencv/opencv.git cd opencv git checkout 3.4.17 mkdir build cd build cmake -D'CMAKE_BUILD_TYPE=RELEASE' .. make -j8 ls -l bin/opencv_createsamples How to get an aligned depth image to the color image ? you can use the align_depth:=true ROS parameter. The aligned image is streamed in a specific topic. (tks Orange group) roslaunch realsense2_camera rs_camera.launch align_depth:=true ROS1 vs ROS2 commands cheatsheet ROS1 ROS2 rostopic list ros2 topic list rqt_graph rqt_graph rviz rviz2 rosrun tf view_frames ros2 run tf2_tools view_frames colcon build --packages-select my_package colcon build --symlink-install colcon build --paths path/to/some/package other/path/to/other/packages/* colcon build --event-handlers console_direct+ --cmake-args -DCMAKE_VERBOSE_MAKEFILE=ON --packages-select my_package .bashrc ROS additions # ROS export ROS_LOCALHOST_ONLY=1 export PS1=\"\\${ROS_VERSION:+(ros\\$ROS_VERSION) }$PS1\" alias rosify1=\"source /opt/ros/noetic/setup.bash && source $HOME/ros1_ws/devel/setup.bash\" alias rosify2=\"source /opt/ros/iron/setup.bash && source $HOME/ros2_ws/install/setup.bash\" Flash kobuki https://kobuki.readthedocs.io/en/devel/firmware.html#linux ROS2 Package with custom Python Library suppose we are in packge my-package Inside the package folder my-package create a folder for my library. For example libs/my_lib Inside myLib folder add __init__.py file that imports resources from other python files. Add the CMakeList.txt install( DIRECTORY libs/my_lib DESTINATION lib/${PROJECT_NAME}) build the package colcon build More on ros2 tutorial by readthedocs :","title":"F.A.Q"},{"location":"appendix/faq/#frequent-asked-question","text":"","title":"Frequent Asked Question..."},{"location":"appendix/faq/#multi-computer-configuration","text":"Configure a ROS_DOMAIN_ID betwwen 1 and 100 in your bashrc file to limit the topics area, and set ROS_LOCALHOST_ONLY to zero to permits several computer to share ROS ressources. gedit ~/.bashrc add at the end of the file (with 42 for instance): export ROS_LOCALHOST_ONLY= 0 export ROS_DOMAIN_ID= 42","title":"Multi-computer configuration"},{"location":"appendix/faq/#ros2-tbot-driver","text":"How to get rid of ROS1 ;-) Install ROS2 tbot driver : cd ~/ros2_ws/pkg-tbot git pull ./script/install-kobuki_ros.sh cd ~/ros2_ws colcon build Launch ROS2 tbot driver : # base only ros2 launch tbot_start base.launch.py # base + laser ros2 launch tbot_start minimal.launch.py # base + with laser + camera ros2 launch tbot_start full.launch.py ros2 topic list","title":"ROS2 tbot driver"},{"location":"appendix/faq/#fix-malformed-packets","text":"Info there: https://github.com/kobuki-base/kobuki_core/commit/2bc11a1bf0ff364b37eb812a404f124dae9c0699 sudo cp /home/bot/ros2_ws/pkg-kobuki/kobuki_core/60-kobuki.rules /lib/udev/rules.d/ Then unplug / replug the robot. To ensure it worked, the followwing command should display 1: cat /sys/bus/usb-serial/devices/ttyUSB0/tty/ttyUSB0/device/latency_timer","title":"Fix malformed packets"},{"location":"appendix/faq/#how-to-install-ubuntu","text":"Ubuntu is a fork of the Debian project, a Linux-based desktop operating system. Official website : https://ubuntu.com/ French community : https://www.ubuntu-fr.org/ Notice that, ubuntu can be installed in double boot mode in parallel to a Microsoft OS on your personal computer. It is not recommended to use Ubuntu+ROS in a virtual machine (the performances would be poor).","title":"How to install Ubuntu ?"},{"location":"appendix/faq/#to-do","text":"Install Ubuntu 20.04 LTS (Long Term Supported version) from live USB-Key Ideally, use all the hard disk (you can split the disk in advance for double-boot install ) Configure \"bot\" username and \"bot\" password. Configure network Login and upgrade your installation sudo apt update sudo apt upgrade","title":"To-do:"},{"location":"appendix/faq/#there-is-no-wifi-on-my-dell-xps13","text":"Connect with cable Get the appropriate drivers: killer driver sudo add-apt-repository ppa:canonical-hwe-team/ backport-iwlwifi sudo apt-get update sudo apt-get install backport-iwlwifi-dkms reboot","title":"There is no Wifi on my dell-xps13 ?"},{"location":"appendix/faq/#remove-password-asking-for-docker-commands","text":"sudo echo \"\\n%sudo ALL=(ALL) NOPASSWD: /usr/bin/docker\\n\" >> /etc/sudoers","title":"Remove password asking for docker commands"},{"location":"appendix/faq/#catkin_create_pkg-invalid-email","text":"you can use the -m option to force an author name. catin_create_pkg -m AuthorName package_name [dependencies...]","title":"Catkin_create_pkg - invalid email ?"},{"location":"appendix/faq/#opencv_createsamples","text":"The latest OpenCV does not include opencv_createsamples . Let's compile an older version (~5 min on labs PC). git clone https://github.com/opencv/opencv.git cd opencv git checkout 3.4.17 mkdir build cd build cmake -D'CMAKE_BUILD_TYPE=RELEASE' .. make -j8 ls -l bin/opencv_createsamples","title":"opencv_createsamples"},{"location":"appendix/faq/#how-to-get-an-aligned-depth-image-to-the-color-image","text":"you can use the align_depth:=true ROS parameter. The aligned image is streamed in a specific topic. (tks Orange group) roslaunch realsense2_camera rs_camera.launch align_depth:=true","title":"How to get an aligned depth image to the color image ?"},{"location":"appendix/faq/#ros1-vs-ros2-commands-cheatsheet","text":"ROS1 ROS2 rostopic list ros2 topic list rqt_graph rqt_graph rviz rviz2 rosrun tf view_frames ros2 run tf2_tools view_frames colcon build --packages-select my_package colcon build --symlink-install colcon build --paths path/to/some/package other/path/to/other/packages/* colcon build --event-handlers console_direct+ --cmake-args -DCMAKE_VERBOSE_MAKEFILE=ON --packages-select my_package","title":"ROS1 vs ROS2 commands cheatsheet"},{"location":"appendix/faq/#bashrc-ros-additions","text":"# ROS export ROS_LOCALHOST_ONLY=1 export PS1=\"\\${ROS_VERSION:+(ros\\$ROS_VERSION) }$PS1\" alias rosify1=\"source /opt/ros/noetic/setup.bash && source $HOME/ros1_ws/devel/setup.bash\" alias rosify2=\"source /opt/ros/iron/setup.bash && source $HOME/ros2_ws/install/setup.bash\"","title":".bashrc ROS additions"},{"location":"appendix/faq/#flash-kobuki","text":"https://kobuki.readthedocs.io/en/devel/firmware.html#linux","title":"Flash kobuki"},{"location":"appendix/faq/#ros2-package-with-custom-python-library","text":"suppose we are in packge my-package Inside the package folder my-package create a folder for my library. For example libs/my_lib Inside myLib folder add __init__.py file that imports resources from other python files. Add the CMakeList.txt install( DIRECTORY libs/my_lib DESTINATION lib/${PROJECT_NAME}) build the package colcon build More on ros2 tutorial by readthedocs :","title":"ROS2 Package with custom Python Library"},{"location":"appendix/simulation_ros1/","text":"Simulation in ROS Gazebo Simulator Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI system using realistic scenarios. Gazebo is integrated with ROS (cf. Gazebo ROS ) and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available. Gazebo Installation Verify that Gazebo is installed. $ dpkg -l | grep gazebo ii gazebo11 11.12.0-1~focal amd64 Open Source Robotics Simulator ii gazebo11-common 11.12.0-1~focal all Open Source Robotics Simulator - Shared files ii gazebo11-plugin-base 11.12.0-1~focal amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo11:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - shared library ii libgazebo11-dev:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - Development Files ii ros-iron-gazebo-dev 3.5.3-1focal.20220829.174620 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.5.3-1focal.20221012.224922 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.5.3-1focal.20221021.150213 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.5.3-1focal.20221013.010602 amd64 Utilities to interface with Gazebo through ROS. To notice that you can install missing packages with the command line: sudo apt install <pakage_name> . Install larm_material ROS1 packages The better way to use Gazebo with ROS is to launch the simulator using ROS launch files. LARM Material is a git repository containing ROS1 resources for this lecture and therefore already done launch files. {% hint style=\"warning\" %} In this lecture, we will still a ROS1 environment to launch Gazebo because the tbot simulated model is not yet ready for ROS2. Cloning and installing the larm_material repository: #ensure ros1_ws catkin workspace is created mkdir -p ~/ros1_ws/src cd ~/ros1_ws/src # clone the LARM repo git clone https://bitbucket.org/imt-mobisyst/larm_material.git # compile cd ~/ros1_ws catkin_make # let your shell know about new ROS packages source devel/setup.bash Launch your first Gazebo Simulation Finally you can launch a preconfigured simulation: $ rosify1 # cf. FAQ to have the rosify1 shell command (ros1) $ roslaunch larm challenge-1.launch Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera. The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipments. Try to list and explore the different topics in a new terminal: (ros1) $ rostopic list (ros1) $ rostopic info <topic_name> ... Question: In which topic are laser scans published ? and camera images ? Connect to ROS2 and Visualize Topics, like the other tools of ROS, has evolved from ROS1 to ROS2 . This evolution makes them incompatible, but it is possble to start a bridge node to transfer information from ROS1 to ROS2 and vise-versa. The ROS1_bridge (a ros2 package) with its dynamic_bridge listen to connection to topics and transfers the data. Then, Rviz2 will be capable of reading and showing the data simulated by Gazebo. rviz2 is a very useful and versatile tool to visualize data that goes through topics. Start the dynamic bridge, Rviz2 and visualize the laser scan data, and the camera data. Frame and transformations Test the main ROS2 GUI tools Rviz and Rqt (in new terminals): Configure rviz2 to visualize the laser scans. Be carreful, ensure that Global Option / Fixed frame is correctly configured to base_link . Question: why is this important? (hint: check your tf using ros2 run tf_tools view_frames.py ) Use rqt to see the graph of ROS nodes and the topics they use to communicate (ref of ROS Qt: RQt ). Controlling the Simulated Robot Launch a simple node to control a robot using keyboard: rosify2 ros2 run teleop_twist_keyboard teleop_twist_keyboard Why can't you control the robot ? Use the tbot_pytool multiplexer and command the robot from the /multi/cmd_teleop topic. # First terminal: ros2 run tbot_pytool multiplexer # Second terminal: ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args --remap /cmd_vel:=/multi/cmd_teleop How many terminals are open ? tuto_sim Create a new package (python or cmake as you want) tuto_sim in your ROS2 workspace and create a launch file that starts with the apropriate configration: the dynamic_bridge , rviz2 , multiplexer and the teleop . All the information you need are in the tutorials on docs.ros.org .","title":"Simulation in ROS"},{"location":"appendix/simulation_ros1/#simulation-in-ros","text":"","title":"Simulation in ROS"},{"location":"appendix/simulation_ros1/#gazebo-simulator","text":"Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI system using realistic scenarios. Gazebo is integrated with ROS (cf. Gazebo ROS ) and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available.","title":"Gazebo Simulator"},{"location":"appendix/simulation_ros1/#gazebo-installation","text":"Verify that Gazebo is installed. $ dpkg -l | grep gazebo ii gazebo11 11.12.0-1~focal amd64 Open Source Robotics Simulator ii gazebo11-common 11.12.0-1~focal all Open Source Robotics Simulator - Shared files ii gazebo11-plugin-base 11.12.0-1~focal amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo11:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - shared library ii libgazebo11-dev:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - Development Files ii ros-iron-gazebo-dev 3.5.3-1focal.20220829.174620 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.5.3-1focal.20221012.224922 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.5.3-1focal.20221021.150213 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.5.3-1focal.20221013.010602 amd64 Utilities to interface with Gazebo through ROS. To notice that you can install missing packages with the command line: sudo apt install <pakage_name> .","title":"Gazebo Installation"},{"location":"appendix/simulation_ros1/#install-larm_material-ros1-packages","text":"The better way to use Gazebo with ROS is to launch the simulator using ROS launch files. LARM Material is a git repository containing ROS1 resources for this lecture and therefore already done launch files. {% hint style=\"warning\" %} In this lecture, we will still a ROS1 environment to launch Gazebo because the tbot simulated model is not yet ready for ROS2. Cloning and installing the larm_material repository: #ensure ros1_ws catkin workspace is created mkdir -p ~/ros1_ws/src cd ~/ros1_ws/src # clone the LARM repo git clone https://bitbucket.org/imt-mobisyst/larm_material.git # compile cd ~/ros1_ws catkin_make # let your shell know about new ROS packages source devel/setup.bash","title":"Install larm_material ROS1 packages"},{"location":"appendix/simulation_ros1/#launch-your-first-gazebo-simulation","text":"Finally you can launch a preconfigured simulation: $ rosify1 # cf. FAQ to have the rosify1 shell command (ros1) $ roslaunch larm challenge-1.launch Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera. The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipments. Try to list and explore the different topics in a new terminal: (ros1) $ rostopic list (ros1) $ rostopic info <topic_name> ... Question: In which topic are laser scans published ? and camera images ?","title":"Launch your first Gazebo Simulation"},{"location":"appendix/simulation_ros1/#connect-to-ros2-and-visualize","text":"Topics, like the other tools of ROS, has evolved from ROS1 to ROS2 . This evolution makes them incompatible, but it is possble to start a bridge node to transfer information from ROS1 to ROS2 and vise-versa. The ROS1_bridge (a ros2 package) with its dynamic_bridge listen to connection to topics and transfers the data. Then, Rviz2 will be capable of reading and showing the data simulated by Gazebo. rviz2 is a very useful and versatile tool to visualize data that goes through topics. Start the dynamic bridge, Rviz2 and visualize the laser scan data, and the camera data.","title":"Connect to ROS2 and Visualize"},{"location":"appendix/simulation_ros1/#frame-and-transformations","text":"Test the main ROS2 GUI tools Rviz and Rqt (in new terminals): Configure rviz2 to visualize the laser scans. Be carreful, ensure that Global Option / Fixed frame is correctly configured to base_link . Question: why is this important? (hint: check your tf using ros2 run tf_tools view_frames.py ) Use rqt to see the graph of ROS nodes and the topics they use to communicate (ref of ROS Qt: RQt ).","title":"Frame and transformations"},{"location":"appendix/simulation_ros1/#controlling-the-simulated-robot","text":"Launch a simple node to control a robot using keyboard: rosify2 ros2 run teleop_twist_keyboard teleop_twist_keyboard Why can't you control the robot ? Use the tbot_pytool multiplexer and command the robot from the /multi/cmd_teleop topic. # First terminal: ros2 run tbot_pytool multiplexer # Second terminal: ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args --remap /cmd_vel:=/multi/cmd_teleop How many terminals are open ?","title":"Controlling the Simulated Robot"},{"location":"appendix/simulation_ros1/#tuto_sim","text":"Create a new package (python or cmake as you want) tuto_sim in your ROS2 workspace and create a launch file that starts with the apropriate configration: the dynamic_bridge , rviz2 , multiplexer and the teleop . All the information you need are in the tutorials on docs.ros.org .","title":"tuto_sim"},{"location":"appendix/tbot/","text":"Tbot In this lecture, we use a tbot. It is a turtlebot2 base equipped with: a kobuki base an hokuyo 2d lidar (potentially, a realsense RGBD camera D435I) Robot configuration : ROS2 driver is availlable on Tbot git repository","title":"Tbot"},{"location":"appendix/tbot/#tbot","text":"In this lecture, we use a tbot. It is a turtlebot2 base equipped with: a kobuki base an hokuyo 2d lidar (potentially, a realsense RGBD camera D435I)","title":"Tbot"},{"location":"appendix/tbot/#robot-configuration","text":"ROS2 driver is availlable on Tbot git repository","title":"Robot configuration :"},{"location":"appendix/turtlebot2/","text":"Simulate a turtlebot2 The first step in the challenge is to get control over a turtlebot2 robot equipped with a standard plan laser. Robot configuration Turtlebot is a simple robot very similar than small home-cleaning robot, but with a connection panel allowing hacking its sensors and actuators. More detail on the official web site . Basically, the turtlebot is equipped with sonars and a 3D camera. The robot version to use is also equipped with a scanning rangefinder like the one proposed by hokuyo . Those solutions are also well supported in ROS . On TheConstruct RDS TheConstruct RDS provides built in the gazebo simulation with turtlebot2 robots (on Melodic). You can start the simulation from your fresh gited ROSject (Melodic - No template) through the simulation button. Then select the turtlebot robot in an empty world. Finally, you will be capable of spawning some obstacles to avoid. Avoid obstacles The solution development for the challenge can begin. The first mission would be to permit the robot to move from its start position toward a goal position by avoiding the obstacles. A quick look at the available ROS topics permit to identifiate entrances for control and scan. rostopic list The goal position (in odom frame) is transmeted to the robot by using Rviz.","title":"Simulate a turtlebot2"},{"location":"appendix/turtlebot2/#simulate-a-turtlebot2","text":"The first step in the challenge is to get control over a turtlebot2 robot equipped with a standard plan laser.","title":"Simulate a turtlebot2"},{"location":"appendix/turtlebot2/#robot-configuration","text":"Turtlebot is a simple robot very similar than small home-cleaning robot, but with a connection panel allowing hacking its sensors and actuators. More detail on the official web site . Basically, the turtlebot is equipped with sonars and a 3D camera. The robot version to use is also equipped with a scanning rangefinder like the one proposed by hokuyo . Those solutions are also well supported in ROS .","title":"Robot configuration"},{"location":"appendix/turtlebot2/#on-theconstruct-rds","text":"TheConstruct RDS provides built in the gazebo simulation with turtlebot2 robots (on Melodic). You can start the simulation from your fresh gited ROSject (Melodic - No template) through the simulation button. Then select the turtlebot robot in an empty world. Finally, you will be capable of spawning some obstacles to avoid.","title":"On TheConstruct RDS"},{"location":"appendix/turtlebot2/#avoid-obstacles","text":"The solution development for the challenge can begin. The first mission would be to permit the robot to move from its start position toward a goal position by avoiding the obstacles. A quick look at the available ROS topics permit to identifiate entrances for control and scan. rostopic list The goal position (in odom frame) is transmeted to the robot by using Rviz.","title":"Avoid obstacles"},{"location":"challenge/challenge-1on2/","text":"Random Search in a Small Environment The goal of the challenge is to demonstrate the capability of a robot to move in a cluttered environment with a possibility to see what the robot see. Expected The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area while avoiding the obstacles (i.e. area limits and some obstacles randomly set in the area). The sensor data (scan, vision) and potentially other information (considered obstacles, frames,...) is visible on rviz. The robot trajectory would permit the robot to explore the overall area. In other words, the robot will go everywhere (i.e. the probability that the robot will reach a specific reachable position in the area is equal to 1 at infinite time). One first approach can be to develop a ricochet robot that changes its direction randomly each time an obstacle prevent the robot to move forward. consigns Each group commit the minimal required files in a specific grp_'machine' ros2 package inside their git repository. Release: Monday afternoon of week-3 (Monday 15-01-2024) The required files: At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_'machine' matching the ROS2 package where the elements will be found ( machine matches the name of the machine embedded on the robot). Inside the grp_'machine' package, a launch file simulation_launch.yaml starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_launch.yaml starting the appropriate nodes for demonstrating with a tbot. Finally, a launch file visualize.launch.py starting the appropriate nodes including rviz2 to visualize (the idea is that we could use to computer on a same domain id. one to control the robot, one for visualization). In simulations, we will work with the configuration set in challenge-1.launch.py . Criteria Minimal: The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured. The robot moves everywhere in it environment. A String message is sent in a detection topic each time a bottle is front of the robot. Optional: An operator can take the control of the robot at any time. The robot movement is fluid (no stop), fast and the robot moves smoothly even in small areas Evaluation protocol (for evaluators...) Here the evaluation protocol applied. It is highly recommended to process it yourself before the submission... clone the group\u2019s repository Take a look to what is inside the repository and read the README.md file (normally it states that the project depends on mb6-tbot , make sure that mb6-tbot project is already installed aside). make it: colcon build and source from the workspace directory. Launch the simulation demonstration: ros2 launch challenge1 simulation.launch.py and appreciate the solution. Stop everything. Connect the computer to the robot, the hokuyo and the camera. Launch the Turtlebot demonstration, and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Challenge 1"},{"location":"challenge/challenge-1on2/#random-search-in-a-small-environment","text":"The goal of the challenge is to demonstrate the capability of a robot to move in a cluttered environment with a possibility to see what the robot see.","title":"Random Search in a Small Environment"},{"location":"challenge/challenge-1on2/#expected","text":"The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area while avoiding the obstacles (i.e. area limits and some obstacles randomly set in the area). The sensor data (scan, vision) and potentially other information (considered obstacles, frames,...) is visible on rviz. The robot trajectory would permit the robot to explore the overall area. In other words, the robot will go everywhere (i.e. the probability that the robot will reach a specific reachable position in the area is equal to 1 at infinite time). One first approach can be to develop a ricochet robot that changes its direction randomly each time an obstacle prevent the robot to move forward.","title":"Expected"},{"location":"challenge/challenge-1on2/#consigns","text":"Each group commit the minimal required files in a specific grp_'machine' ros2 package inside their git repository. Release: Monday afternoon of week-3 (Monday 15-01-2024)","title":"consigns"},{"location":"challenge/challenge-1on2/#the-required-files","text":"At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_'machine' matching the ROS2 package where the elements will be found ( machine matches the name of the machine embedded on the robot). Inside the grp_'machine' package, a launch file simulation_launch.yaml starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_launch.yaml starting the appropriate nodes for demonstrating with a tbot. Finally, a launch file visualize.launch.py starting the appropriate nodes including rviz2 to visualize (the idea is that we could use to computer on a same domain id. one to control the robot, one for visualization). In simulations, we will work with the configuration set in challenge-1.launch.py .","title":"The required files:"},{"location":"challenge/challenge-1on2/#criteria","text":"Minimal: The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured. The robot moves everywhere in it environment. A String message is sent in a detection topic each time a bottle is front of the robot. Optional: An operator can take the control of the robot at any time. The robot movement is fluid (no stop), fast and the robot moves smoothly even in small areas","title":"Criteria"},{"location":"challenge/challenge-1on2/#evaluation-protocol-for-evaluators","text":"Here the evaluation protocol applied. It is highly recommended to process it yourself before the submission... clone the group\u2019s repository Take a look to what is inside the repository and read the README.md file (normally it states that the project depends on mb6-tbot , make sure that mb6-tbot project is already installed aside). make it: colcon build and source from the workspace directory. Launch the simulation demonstration: ros2 launch challenge1 simulation.launch.py and appreciate the solution. Stop everything. Connect the computer to the robot, the hokuyo and the camera. Launch the Turtlebot demonstration, and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Evaluation protocol (for evaluators...)"},{"location":"challenge/challenge-2on2/","text":"Efficient Exploration The goal of the challenge is to demonstrate the capability a robot has to navigate in a cluttered environment and locate specific objects. The localization requires to build a map of the environment. Expected The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area by avoiding obstacles. The robot knowledge is extended with new incoming data. In other words, the robot build a map and localizes itself in it. The robot detects NukaCola bottle in the vision flux. Messages are sent in topics one to state the detection and another one to mark the position in the map (cf. marker_msgs ) Experiments can be performed with 2 computers, one on the robot (Control PC) and a second for visualization and human control (Operator PC). consigns Each group commit the minimal required files in a specific grp_'machine' ros2 package inside their git repository. Release: Monday of week-4, Monday 22 of January The required files: At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_'machine' matching the ROS2 package where the elements will be found ( machine matches the name of the machine embedded in the robot). Inside the grp_'machine' package, a launch file simulation_launch starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_launch starting the appropriate nodes for demonstrating with a Turtlebot. Finally, a launch file operator_launch start a visualization + control solution. In simulations, we will work with the configuration set in challenge-2.launch.py . Criteria Minimal: The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured in a second PC and display the built map. It is possible to visualize a marker for detected bottles at the position of the bottle in the environment. The bottles are identified with a number and the robot is capable of recognizing a bottle on a second passage (it should be easy to count the bottle in the map or by reading the topic). The bottle detection detects all the bottles, in any position but with robustness to false positives. Optional (the order does not matter): The robot movement is oriented toward the unknown areas to speed up the exploration. Processes are clearly established (start and stop the robot, save the map, get the lists of bottles, set the xp in pause, ...) Developed nodes are based on ROS2 Parameters (for speed, obstacle detections, ...) The Kobuki features are integrated to the scenario (robot button, contact to the ground, void detection, bips,...) The list is not exhaustive, be inventive ! The challenge2 tbot.launch.py launch file may take an initial map. Evaluation protocol Here the evaluation protocol to apply. It is highly recommended to process it yourself before the submission... Clone/update the group\u2019s repository on both machines (Control and Operator) Take a look to what is inside the repository and read the README.md file (normally it states that the project depends on mb6-tbot , make sure that mb6-tbot project is already installed aside). Build it: colcon build and source from the workspace directory. Set the appropriate ROS configuration (domain ID, etc.). Launch the simulation demonstration: ros2 launch challenge1 simulation.launch.py and appreciate the solution. Stop everything. Configure the computers for working with the Tbot. Launch the Tbot demonstration, and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Challenge 2"},{"location":"challenge/challenge-2on2/#efficient-exploration","text":"The goal of the challenge is to demonstrate the capability a robot has to navigate in a cluttered environment and locate specific objects. The localization requires to build a map of the environment.","title":"Efficient Exploration"},{"location":"challenge/challenge-2on2/#expected","text":"The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area by avoiding obstacles. The robot knowledge is extended with new incoming data. In other words, the robot build a map and localizes itself in it. The robot detects NukaCola bottle in the vision flux. Messages are sent in topics one to state the detection and another one to mark the position in the map (cf. marker_msgs ) Experiments can be performed with 2 computers, one on the robot (Control PC) and a second for visualization and human control (Operator PC).","title":"Expected"},{"location":"challenge/challenge-2on2/#consigns","text":"Each group commit the minimal required files in a specific grp_'machine' ros2 package inside their git repository. Release: Monday of week-4, Monday 22 of January","title":"consigns"},{"location":"challenge/challenge-2on2/#the-required-files","text":"At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_'machine' matching the ROS2 package where the elements will be found ( machine matches the name of the machine embedded in the robot). Inside the grp_'machine' package, a launch file simulation_launch starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_launch starting the appropriate nodes for demonstrating with a Turtlebot. Finally, a launch file operator_launch start a visualization + control solution. In simulations, we will work with the configuration set in challenge-2.launch.py .","title":"The required files:"},{"location":"challenge/challenge-2on2/#criteria","text":"Minimal: The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured in a second PC and display the built map. It is possible to visualize a marker for detected bottles at the position of the bottle in the environment. The bottles are identified with a number and the robot is capable of recognizing a bottle on a second passage (it should be easy to count the bottle in the map or by reading the topic). The bottle detection detects all the bottles, in any position but with robustness to false positives. Optional (the order does not matter): The robot movement is oriented toward the unknown areas to speed up the exploration. Processes are clearly established (start and stop the robot, save the map, get the lists of bottles, set the xp in pause, ...) Developed nodes are based on ROS2 Parameters (for speed, obstacle detections, ...) The Kobuki features are integrated to the scenario (robot button, contact to the ground, void detection, bips,...) The list is not exhaustive, be inventive ! The challenge2 tbot.launch.py launch file may take an initial map.","title":"Criteria"},{"location":"challenge/challenge-2on2/#evaluation-protocol","text":"Here the evaluation protocol to apply. It is highly recommended to process it yourself before the submission... Clone/update the group\u2019s repository on both machines (Control and Operator) Take a look to what is inside the repository and read the README.md file (normally it states that the project depends on mb6-tbot , make sure that mb6-tbot project is already installed aside). Build it: colcon build and source from the workspace directory. Set the appropriate ROS configuration (domain ID, etc.). Launch the simulation demonstration: ros2 launch challenge1 simulation.launch.py and appreciate the solution. Stop everything. Configure the computers for working with the Tbot. Launch the Tbot demonstration, and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Evaluation protocol"},{"location":"challenge/kick-off/","text":"Challenge Kick-Off The challenge aims at making learners develop a first robotic project. In the end, both the execution of the proposed solution and the source code with documentation will be evaluated. The main objectives of the project consist of: Control a robot in a cluttered environment Map a static environment Detect all the Nuka-Cola bottles Estimate the position of all the Nuka-Cola in the map Optimize the exploration strategy Challenges are proposed to increase sequentially the complexity of the expected solution, but first the students have to structure their developping environment... Create a group As a first move, you have to constitute a group of 2 developers. Record the created group on a shared document: 2023-2024 groups . Create on items in the enumerate list per groups Record the name of the machine you use and the names of each member of the group. The number of the line matches the number identifying a group starting from \\(1.\\) . The group gets a unique number it can use as ROS_DOMAIN_ID . Generate a working environment We ask each group to use git and share their work with professors through github (because it is the reference) (or gitlab at the student request, because it is open). Git is a versioning program working in a distributed way. \"Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development.\" Wikipedia-2021-12-6 . Official web site: git-scm.com git basics: in video Potentially the developers can share some of the versions there developed (or all) by installing an extra git on a shared server. github is certainly the most famous web git-solution (but you can install for instance a web application as gitlab on your own server for instance). To notice that gitlab is both an open solution you can deploy on your own server and a web-service ( gitlab.com ) you can use. For this lecture, you will use github or gitlab.com , no other solution would be accepted. (Each of the developers) Create a github account. (One of the developers) Create a new repository on github and invite your teammate. Choose a name referring the group (for instance uvlarm-machinename ), the idea is to propose a repository name clear but different from a group to another. (Each of the developers) Clone locally. In a termal: cd mb6-space git clone https://my.github.url/uvlarm-machinename.git (One of the developers) Invite the professor (or set your repository public) - github account: lozenguez LucFabresse bouraqadi SebAmb (One of the developers) Reccord the url in the shared document: (2023-2024 groups). You can then, work with visual studio code. By opening a the project uv-larm-machinename , VSCode will recognise the git repository (i.e. the presence of hiden .git directory). VSCode is also capable of managing a secure connection between your machine and github. code uv-larm-machinename Optionaly, you can configure ssh access: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/about-ssh Initialize: Your repository has to match a meta ROS-package (i.e. a directory composed by other directories each of them matching a ros-package). The repository would be cloned aside of the larm packages ( pkg-tbot ). So clone your repository in the ros workspace mb6-space and then create as many packages you want inside. cd ~/mb6-space git clone github/uvlarm-machinename.git (One of the developers) Initialize a README.md file in Mardown at least with a simple tittle and refering the developers (cf. Markdown syntax ): echo \"# grp-`machinename` repository for the UV-LARM\" > README.md git add README.md It is possible then to push on the server your first commit. One commit refer to one version of your project, you can (must) generate has versions has you want. A commit do not have to serve a functional version of your works. It could be transitive commit. And most importantly git is capable to re-generate any of your commits, so do not hesitate to commit continuously... git commit -am \"Initialize README file\" git pull git push All the other developers can now pull the new version (typically, in the Develter computers).... New package: Then you can go inside your repository and create a new ros package: cd larm-machinename ros2 pkg create ... # cf. package tutorial A new directory my_amasing_pkg appears: The git status command informs you that this package is not followed by git . Let correct that. git add my_amasing_pkg git commit -am \"Initializing my_amasing_pkg\" Strange, nothing changes on my github repo. The git repo on github is a different entity than the one on your computer. You have to manually synchronize then when you want to share your work with the command pull and push . Try-it. Now you can commit , pull , push as often as possible (and add if you have some new files...). Build your package On a fresh machine, you can clone then build your new ros-package: mkdir mysuper_ros_workspace cd mysuper_ros_workspace mkdir src git clone github/repo.git src/grp-color colcon build To notice that colconn build have to be performed from your workspace directory ( mysuper_ros_workspace in this example, mb6-space in this courses aside of tbot and tsim packages).","title":"Kick-off"},{"location":"challenge/kick-off/#challenge-kick-off","text":"The challenge aims at making learners develop a first robotic project. In the end, both the execution of the proposed solution and the source code with documentation will be evaluated. The main objectives of the project consist of: Control a robot in a cluttered environment Map a static environment Detect all the Nuka-Cola bottles Estimate the position of all the Nuka-Cola in the map Optimize the exploration strategy Challenges are proposed to increase sequentially the complexity of the expected solution, but first the students have to structure their developping environment...","title":"Challenge Kick-Off"},{"location":"challenge/kick-off/#create-a-group","text":"As a first move, you have to constitute a group of 2 developers. Record the created group on a shared document: 2023-2024 groups . Create on items in the enumerate list per groups Record the name of the machine you use and the names of each member of the group. The number of the line matches the number identifying a group starting from \\(1.\\) . The group gets a unique number it can use as ROS_DOMAIN_ID .","title":"Create a group"},{"location":"challenge/kick-off/#generate-a-working-environment","text":"We ask each group to use git and share their work with professors through github (because it is the reference) (or gitlab at the student request, because it is open). Git is a versioning program working in a distributed way. \"Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development.\" Wikipedia-2021-12-6 . Official web site: git-scm.com git basics: in video Potentially the developers can share some of the versions there developed (or all) by installing an extra git on a shared server. github is certainly the most famous web git-solution (but you can install for instance a web application as gitlab on your own server for instance). To notice that gitlab is both an open solution you can deploy on your own server and a web-service ( gitlab.com ) you can use. For this lecture, you will use github or gitlab.com , no other solution would be accepted. (Each of the developers) Create a github account. (One of the developers) Create a new repository on github and invite your teammate. Choose a name referring the group (for instance uvlarm-machinename ), the idea is to propose a repository name clear but different from a group to another. (Each of the developers) Clone locally. In a termal: cd mb6-space git clone https://my.github.url/uvlarm-machinename.git (One of the developers) Invite the professor (or set your repository public) - github account: lozenguez LucFabresse bouraqadi SebAmb (One of the developers) Reccord the url in the shared document: (2023-2024 groups). You can then, work with visual studio code. By opening a the project uv-larm-machinename , VSCode will recognise the git repository (i.e. the presence of hiden .git directory). VSCode is also capable of managing a secure connection between your machine and github. code uv-larm-machinename Optionaly, you can configure ssh access: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/about-ssh","title":"Generate a working environment"},{"location":"challenge/kick-off/#initialize","text":"Your repository has to match a meta ROS-package (i.e. a directory composed by other directories each of them matching a ros-package). The repository would be cloned aside of the larm packages ( pkg-tbot ). So clone your repository in the ros workspace mb6-space and then create as many packages you want inside. cd ~/mb6-space git clone github/uvlarm-machinename.git (One of the developers) Initialize a README.md file in Mardown at least with a simple tittle and refering the developers (cf. Markdown syntax ): echo \"# grp-`machinename` repository for the UV-LARM\" > README.md git add README.md It is possible then to push on the server your first commit. One commit refer to one version of your project, you can (must) generate has versions has you want. A commit do not have to serve a functional version of your works. It could be transitive commit. And most importantly git is capable to re-generate any of your commits, so do not hesitate to commit continuously... git commit -am \"Initialize README file\" git pull git push All the other developers can now pull the new version (typically, in the Develter computers)....","title":"Initialize:"},{"location":"challenge/kick-off/#new-package","text":"Then you can go inside your repository and create a new ros package: cd larm-machinename ros2 pkg create ... # cf. package tutorial A new directory my_amasing_pkg appears: The git status command informs you that this package is not followed by git . Let correct that. git add my_amasing_pkg git commit -am \"Initializing my_amasing_pkg\" Strange, nothing changes on my github repo. The git repo on github is a different entity than the one on your computer. You have to manually synchronize then when you want to share your work with the command pull and push . Try-it. Now you can commit , pull , push as often as possible (and add if you have some new files...).","title":"New package:"},{"location":"challenge/kick-off/#build-your-package","text":"On a fresh machine, you can clone then build your new ros-package: mkdir mysuper_ros_workspace cd mysuper_ros_workspace mkdir src git clone github/repo.git src/grp-color colcon build To notice that colconn build have to be performed from your workspace directory ( mysuper_ros_workspace in this example, mb6-space in this courses aside of tbot and tsim packages).","title":"Build your package"},{"location":"pdf/","text":"PDF resources:","title":""},{"location":"pdf/#pdf-resources","text":"","title":"PDF resources:"},{"location":"tuto-kick-off/basics/","text":"Operating System: Linux/Ubuntu for ROS2 ROS stand for R obot O perating S ystem however it extend a classical O perating S ystem, a Linux/Ubuntu OS. The goal of this tutorial is to come back on basic O perating S ystem notions and mainly the Shell offering the simplest way to interact with your computer. For an introduction to OS read the wikipedia page Play with Linux Ubuntu is a Linux distribution (Operating System + Window Managment + Tools) derived from Debian distribution, in the galaxies of Open source solutions. Open Source means that every user can deep into the source code of the programs (open source is not necessarily free of charge). Linux is mostly an OS kernel. It exposes computer capability (computing, memory access and other devices) to developers. Typically C/C++ language was developed for the Linux kernel. Distribution will configure together an OS kernel, software management (packages), an IHM (display server + desktop environment) and a bench of softwares. Typically, ROS is designed to work on Ubuntu : Linux kernel , GNOME desktop, aptitude (apt) package manager. Ubuntu is a good example of Open Source project: It is based on another Open Source project: Debian OS to capitalize on the Debian community works, while offering a more easy-to-use solution. It searches for profitability with Ubuntu Pro . It is used in return for other Linux-based OS: Pop!_os or Zorin for instance. Visit distrowatch.com to get an idea of the galaxy of interconnected Linux OSs . The shell or Terminal In the variety of program running on Linux, we are mostly interested in the terminal emulator, permitting to manipulate our system directly from command (moving in the directory tree, read and organize files, execute scripts or programs, manage computer resources as a network, ect.). On Ubuntu, default terminal emulator is gnome-terminal ( help.gnome.org ) configured with bash ( on wikipedia ) command interpreter. Commands are interpretable instruction generally executing programs. It is expected that the following commands are known before to start working with ROS . To notice that tabulation allows for autocompletion. Explore the following command (i.e. what for, how to use it), then play with the command in your terminal. The File System ls : list directory elements cd : change directory cat : read all a text file more : read a text file step by step touch : touch a resource (create if absent) nano : edit a text file rm : remove a file (permanently) mkdir : create (make) a directory rmdir : remove a directory mv : move a resource cp : copy-paste a file clear : clear your shell Manipulate pathes : / : the root directory in Linux system - example: du /bin/bash (du - estimate file space usage) . : the current directory - example: mkdir ./workspace .. : the parent directory - example: cd ../../media ~ : the user directory - example: cd ~/Documents CApitAl Letters maTer - example: cd ~/Documents \\(\\neq\\) cd ~/documents Mastering Commands The classical syntax is: command --option -o argument1 - example: ls -a ~ . Commands exits for helping to master all the other commands: man : command and library manual - example: man cp apropos : search in the manuals - example: apropos search whereis : find the location of a resource - example: whereis python3 alias : create our own command - example: alias listall=\"ls -a\" Sessions and environment A session is an active connection to a system or program. who : List the open session in a computer ssh : Open a session in a distant computer following the ssh ( secure shell ) protocol. In return, an environment is a computer system or set of systems in which a computer program or software component is deployed and executed. ( Wikipedia Sept. 2023 ) env : list the environment variable. $ : access a variable - example: echo $PATH export : create a new variable - example: export PATH=$PATH:~/bin ~/.bashrc (file): user run-commands' configuration. User, Group and Rules User are identified and attached to groups User and group have a name and a number Resources are owned by a user and a group Specific access can be tuned for each resource r : read \\(\\quad\\) w : write \\(\\quad\\) x : executed/open For the user , the group members and all the others The ls -l command lists the content of a repertory with ownership and authorization - Example with ls -l /etc . With cat /etc/passwd it is possible to list all users ( /etc/group for the groups). Somme commands: chmod : change authorizations - Examples: chmod +x aFile - add x authorization on aFile chmod 752 aFile - set authorization on aFile on a binary style chown : change the owner Manage Processes ps : similar to ls but for processes- exemple: gedit & - & permits to get back to the prompt ps - List the local processes (ie. children of the bash shell) ps -e - list all processes top : interactive process monitoring ( Q to quit ) kill : sent a signal to a process kill 19482 - send TERM signal to process 19482 kill -s KILL 19482 - send KILL signal to process 19482 For managing processes, OS attaches some elements to each one: the PID (Process IDentifier), a parent, ... Some Bash tools and shortcuts bash is one of the shell solutions on Unix systems. tabulation : auto-complete the command line or list the possibilities !xx : run again the last command starting with xx ctrl-r : search for a command in command history Q : quit a runnig program ctr-c : terminate a running program ~/.bashrc (file): again, user run-commands' configuration. And more on ( Wikipedia ).","title":"OS and Shell"},{"location":"tuto-kick-off/basics/#operating-system-linuxubuntu-for-ros2","text":"ROS stand for R obot O perating S ystem however it extend a classical O perating S ystem, a Linux/Ubuntu OS. The goal of this tutorial is to come back on basic O perating S ystem notions and mainly the Shell offering the simplest way to interact with your computer. For an introduction to OS read the wikipedia page","title":"Operating System: Linux/Ubuntu for ROS2"},{"location":"tuto-kick-off/basics/#play-with-linux","text":"Ubuntu is a Linux distribution (Operating System + Window Managment + Tools) derived from Debian distribution, in the galaxies of Open source solutions. Open Source means that every user can deep into the source code of the programs (open source is not necessarily free of charge). Linux is mostly an OS kernel. It exposes computer capability (computing, memory access and other devices) to developers. Typically C/C++ language was developed for the Linux kernel. Distribution will configure together an OS kernel, software management (packages), an IHM (display server + desktop environment) and a bench of softwares. Typically, ROS is designed to work on Ubuntu : Linux kernel , GNOME desktop, aptitude (apt) package manager. Ubuntu is a good example of Open Source project: It is based on another Open Source project: Debian OS to capitalize on the Debian community works, while offering a more easy-to-use solution. It searches for profitability with Ubuntu Pro . It is used in return for other Linux-based OS: Pop!_os or Zorin for instance. Visit distrowatch.com to get an idea of the galaxy of interconnected Linux OSs .","title":"Play with Linux"},{"location":"tuto-kick-off/basics/#the-shell-or-terminal","text":"In the variety of program running on Linux, we are mostly interested in the terminal emulator, permitting to manipulate our system directly from command (moving in the directory tree, read and organize files, execute scripts or programs, manage computer resources as a network, ect.). On Ubuntu, default terminal emulator is gnome-terminal ( help.gnome.org ) configured with bash ( on wikipedia ) command interpreter. Commands are interpretable instruction generally executing programs. It is expected that the following commands are known before to start working with ROS . To notice that tabulation allows for autocompletion. Explore the following command (i.e. what for, how to use it), then play with the command in your terminal.","title":"The shell or Terminal"},{"location":"tuto-kick-off/basics/#the-file-system","text":"ls : list directory elements cd : change directory cat : read all a text file more : read a text file step by step touch : touch a resource (create if absent) nano : edit a text file rm : remove a file (permanently) mkdir : create (make) a directory rmdir : remove a directory mv : move a resource cp : copy-paste a file clear : clear your shell Manipulate pathes : / : the root directory in Linux system - example: du /bin/bash (du - estimate file space usage) . : the current directory - example: mkdir ./workspace .. : the parent directory - example: cd ../../media ~ : the user directory - example: cd ~/Documents CApitAl Letters maTer - example: cd ~/Documents \\(\\neq\\) cd ~/documents","title":"The File System"},{"location":"tuto-kick-off/basics/#mastering-commands","text":"The classical syntax is: command --option -o argument1 - example: ls -a ~ . Commands exits for helping to master all the other commands: man : command and library manual - example: man cp apropos : search in the manuals - example: apropos search whereis : find the location of a resource - example: whereis python3 alias : create our own command - example: alias listall=\"ls -a\"","title":"Mastering Commands"},{"location":"tuto-kick-off/basics/#sessions-and-environment","text":"A session is an active connection to a system or program. who : List the open session in a computer ssh : Open a session in a distant computer following the ssh ( secure shell ) protocol. In return, an environment is a computer system or set of systems in which a computer program or software component is deployed and executed. ( Wikipedia Sept. 2023 ) env : list the environment variable. $ : access a variable - example: echo $PATH export : create a new variable - example: export PATH=$PATH:~/bin ~/.bashrc (file): user run-commands' configuration.","title":"Sessions and environment"},{"location":"tuto-kick-off/basics/#user-group-and-rules","text":"User are identified and attached to groups User and group have a name and a number Resources are owned by a user and a group Specific access can be tuned for each resource r : read \\(\\quad\\) w : write \\(\\quad\\) x : executed/open For the user , the group members and all the others The ls -l command lists the content of a repertory with ownership and authorization - Example with ls -l /etc . With cat /etc/passwd it is possible to list all users ( /etc/group for the groups). Somme commands: chmod : change authorizations - Examples: chmod +x aFile - add x authorization on aFile chmod 752 aFile - set authorization on aFile on a binary style chown : change the owner","title":"User, Group and Rules"},{"location":"tuto-kick-off/basics/#manage-processes","text":"ps : similar to ls but for processes- exemple: gedit & - & permits to get back to the prompt ps - List the local processes (ie. children of the bash shell) ps -e - list all processes top : interactive process monitoring ( Q to quit ) kill : sent a signal to a process kill 19482 - send TERM signal to process 19482 kill -s KILL 19482 - send KILL signal to process 19482 For managing processes, OS attaches some elements to each one: the PID (Process IDentifier), a parent, ...","title":"Manage Processes"},{"location":"tuto-kick-off/basics/#some-bash-tools-and-shortcuts","text":"bash is one of the shell solutions on Unix systems. tabulation : auto-complete the command line or list the possibilities !xx : run again the last command starting with xx ctrl-r : search for a command in command history Q : quit a runnig program ctr-c : terminate a running program ~/.bashrc (file): again, user run-commands' configuration. And more on ( Wikipedia ).","title":"Some Bash tools and shortcuts"},{"location":"tuto-kick-off/first-contact/","text":"ROS Basics The goal of this tutorial is to set up a workspace for all the resources we need to install and develop in our journey to control robots. We choose to work with ROS (the most used open middleware for robotics) on a Linux-Ubuntu computer (which is the best-supported OS by ROS ). This tutorial supposes that you have a Ubuntu-Like 22.04 computers with configured Python and C++ APIs. The tutorial relies on ROS2 Iron . To notice that the wiki.ros.org website refers to ROS1 . ROS1 is very similar to ROS2 but with important differences. So be careful about the internet research result. Nodes and Topics Robots are complex cybernetic systems that require complex software to handle complex missions. That for, the main feature proposed by ROS is to develop programs in a modular way. Pieces of programs are dedicated to atomic functionalities and the overall control program is composed of several interconnected pieces. A piece of program is called node and nodes exchange data by communicating through topics . This tutorial part relies directly on ROS documentation docs.ros.org . Configuring environment : You should well understand how to configure your Linux environment. First contact : Then, you can play with the hello world example on ROS2 . To notice that sudo commands suppose you have administrator privilege on the computer. Understanding nodes : well, it is on the title. Understanding topics : ... At this point you should be comfortable with the core component of ROS , nodes and topics. You realized that you will interact a lot with your software architecture throuh your terminal. So a good practice consists of turning your terminal prompt to clearly state your ROS configuration at any time... Add the following 2 lines into your ~/.bashrc file. The bash run command file ( ~/.bashrc ) is a good tool to configure all your terminal with common environment variables and configurations. # Tunned prompt: PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:($ROS_AUTOMATIC_DISCOVERY_RANGE::$ROS_DOMAIN_ID)\\[\\033[01;34m\\]\\w\\[\\033[00m\\].\\n\\$ ' Python API ROS include more elements, components like services and actions , features like parameters or tools to debug ( rqt_console ), visualize ( rviz2 ), launch configurations of nodes (launch) or recording topics ( bag ). But let see the Python API ( Application Programming Interface ) on nodes and topics first, before to visit other ROS elements. To notice that the purpose of this lecture and its tutorials is not to cover all ROS elements. Furthermore, the tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. Other solutions are provided by ROS2 . Setup a Dev. Environment First you should work on a dedicated directory to regroup the ROS things you need and you develop, let say your ros_space or ros_ws . The things would generally be packages but we will see that later. In this work space, we will work for now on a playground directory and mode specifically on ROS talker and listener python script. To set up our environment in a terminal: cd # Go to the home directory of the current user. mkdir ros_space # Create a directory cd ros_space # Enter it mkdir playground # Create a directory touch playground/talker.py # Create a file (if it does not exist) touch playground/listener.py # Create a file (if it does not exist) We recommend working with a complete extensible IDE, typically Visual Studio Code . And open the entire ros_space as a project, plus a terminal inside the IDE. At this point, your IDE should have \\(3\\) areas: the explorer on the left side, a text editor on top of a terminal . The explorer should contain the playground directory with your \\(2\\) python scripts inside. Talker: a simple topic publisher The goal of our first program is to send a message on a topic. For that we need the ROS client Python package ( rclpy ) for initializing ROS and creating nodes, and all the packages defining all the types of messages the program should manipulate (only simple string message on our example). At the end the first hello world program will look like: import rclpy from rclpy.node import Node from std_msgs.msg import String def oneTalk(): # Initialize ROS client rclpy.init() # Create a node aNode= Node( \"simpleTalker\" ) # Attach a publisher to the node, with a specific type, the name of the topic, a history depth aPublisher= aNode.create_publisher( String, 'testTopic', 10 ) # Create a message to send msg = String() msg.data = 'Hello World' # Add the message to the list of messages to publish aPublisher.publish(msg) # Activate the ROS client with the node # (that will publish the message on testTopic topic) rclpy.spin_once(aNode, timeout_sec= 10.0) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() # Execute the function. if __name__ == \"__main__\": oneTalk() Run the solution on a first terminal. python3 playground/talker.py Listen to the topic in another terminal (you have to run again the talker to see a message...). ros2 topic echo testTopic The talker node need to be active when echo testTopic is started otherwise ros2 topic end because no node is connected to testTopic . Also, the String message description (ie. composed by a unique data attribute) come from index.ros.org , aside from the other standard message type of std_msgs . The main ROS packages defining message types are grouped on common_interfaces with, for example geometry_msgs for objects defined on the Cartesian coordinate system, visualization_msgs focused on message types for visualization or sensor_msgs , well, for sensor-based data. Continuous Talker In a second version, we aim to create a node publishing continuously messages. For that purpose we move on Event-Driven Programming. Technically in ROS, it is managed by the ros client, with the spin_once or spin functions. These \\(2\\) functions connect events (the reception of a message for instance) with the appropriate callback. spin_once activates once the ros client and wait for the next event. It should be called in an infinite loop. spin handle the infinite loop directly (and should be preferred). For the talker, you have to generate events with timers: aNode.create_timer( duration, aCallback ) The call back function aCallBack will be called after a duration is terminated. Furthermore, the call-back functions generally require a context of execution. Some already defined and accessible elements (for instance in our case, the publisher). For that purpose, we implement our aCallBack as a method of a class. The new talker script will lookalike: def infiniteTalk(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"infTalker\" ) talker= ROSTalker(aNode) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSTalker: def __init__(self, rosNode): self._publisher= rosNode.create_publisher( String, 'testTopic', 10 ) self._timer = rosNode.create_timer(0.5, self.timer_callback) self._i = 0 def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self._i self._publisher.publish(msg) self._i += 1 # Execute the function. if __name__ == \"__main__\": infiniteTalk() Now python3 playground/talker.py wil start a ROS node publishing \\(2\\) hello messages per second. A Ctr-C in the terminal should stop the process. Listener: a simple topic subcriber In a very similar way, listener node is implemented with topics subscription mechanisms associated with a callback. import rclpy from rclpy.node import Node from std_msgs.msg import String def listen(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"listener\" ) listener= ROSListener(aNode) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSListener(): def __init__(self, rosNode): self._logger= rosNode.get_logger() self._subscription= rosNode.create_subscription( String, 'testTopic', self.listener_callback, 10 ) def listener_callback(self, msg): self._logger.info( 'I heard: ' + msg.data) if __name__ == '__main__': listen() Run your listener as you run your talker. The \\(2\\) nodes work together and define a very simple first software architecture. Logs To notice that ROS provides its own, \" print \" function with a logger instance. The logger, attached to a node instance ( aRosNode.get_logger() ), records logs through different channels: info , debug , warning or error . More on docs.ros.org . Logs can be followed with rqt_console tool. Follow the official tutorial to learn how to use this tool.","title":"Nodes and Topics"},{"location":"tuto-kick-off/first-contact/#ros-basics","text":"The goal of this tutorial is to set up a workspace for all the resources we need to install and develop in our journey to control robots. We choose to work with ROS (the most used open middleware for robotics) on a Linux-Ubuntu computer (which is the best-supported OS by ROS ). This tutorial supposes that you have a Ubuntu-Like 22.04 computers with configured Python and C++ APIs. The tutorial relies on ROS2 Iron . To notice that the wiki.ros.org website refers to ROS1 . ROS1 is very similar to ROS2 but with important differences. So be careful about the internet research result.","title":"ROS Basics"},{"location":"tuto-kick-off/first-contact/#nodes-and-topics","text":"Robots are complex cybernetic systems that require complex software to handle complex missions. That for, the main feature proposed by ROS is to develop programs in a modular way. Pieces of programs are dedicated to atomic functionalities and the overall control program is composed of several interconnected pieces. A piece of program is called node and nodes exchange data by communicating through topics . This tutorial part relies directly on ROS documentation docs.ros.org . Configuring environment : You should well understand how to configure your Linux environment. First contact : Then, you can play with the hello world example on ROS2 . To notice that sudo commands suppose you have administrator privilege on the computer. Understanding nodes : well, it is on the title. Understanding topics : ... At this point you should be comfortable with the core component of ROS , nodes and topics. You realized that you will interact a lot with your software architecture throuh your terminal. So a good practice consists of turning your terminal prompt to clearly state your ROS configuration at any time... Add the following 2 lines into your ~/.bashrc file. The bash run command file ( ~/.bashrc ) is a good tool to configure all your terminal with common environment variables and configurations. # Tunned prompt: PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:($ROS_AUTOMATIC_DISCOVERY_RANGE::$ROS_DOMAIN_ID)\\[\\033[01;34m\\]\\w\\[\\033[00m\\].\\n\\$ '","title":"Nodes and Topics"},{"location":"tuto-kick-off/first-contact/#python-api","text":"ROS include more elements, components like services and actions , features like parameters or tools to debug ( rqt_console ), visualize ( rviz2 ), launch configurations of nodes (launch) or recording topics ( bag ). But let see the Python API ( Application Programming Interface ) on nodes and topics first, before to visit other ROS elements. To notice that the purpose of this lecture and its tutorials is not to cover all ROS elements. Furthermore, the tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. Other solutions are provided by ROS2 .","title":"Python API"},{"location":"tuto-kick-off/first-contact/#setup-a-dev-environment","text":"First you should work on a dedicated directory to regroup the ROS things you need and you develop, let say your ros_space or ros_ws . The things would generally be packages but we will see that later. In this work space, we will work for now on a playground directory and mode specifically on ROS talker and listener python script. To set up our environment in a terminal: cd # Go to the home directory of the current user. mkdir ros_space # Create a directory cd ros_space # Enter it mkdir playground # Create a directory touch playground/talker.py # Create a file (if it does not exist) touch playground/listener.py # Create a file (if it does not exist) We recommend working with a complete extensible IDE, typically Visual Studio Code . And open the entire ros_space as a project, plus a terminal inside the IDE. At this point, your IDE should have \\(3\\) areas: the explorer on the left side, a text editor on top of a terminal . The explorer should contain the playground directory with your \\(2\\) python scripts inside.","title":"Setup a Dev. Environment"},{"location":"tuto-kick-off/first-contact/#talker-a-simple-topic-publisher","text":"The goal of our first program is to send a message on a topic. For that we need the ROS client Python package ( rclpy ) for initializing ROS and creating nodes, and all the packages defining all the types of messages the program should manipulate (only simple string message on our example). At the end the first hello world program will look like: import rclpy from rclpy.node import Node from std_msgs.msg import String def oneTalk(): # Initialize ROS client rclpy.init() # Create a node aNode= Node( \"simpleTalker\" ) # Attach a publisher to the node, with a specific type, the name of the topic, a history depth aPublisher= aNode.create_publisher( String, 'testTopic', 10 ) # Create a message to send msg = String() msg.data = 'Hello World' # Add the message to the list of messages to publish aPublisher.publish(msg) # Activate the ROS client with the node # (that will publish the message on testTopic topic) rclpy.spin_once(aNode, timeout_sec= 10.0) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() # Execute the function. if __name__ == \"__main__\": oneTalk() Run the solution on a first terminal. python3 playground/talker.py Listen to the topic in another terminal (you have to run again the talker to see a message...). ros2 topic echo testTopic The talker node need to be active when echo testTopic is started otherwise ros2 topic end because no node is connected to testTopic . Also, the String message description (ie. composed by a unique data attribute) come from index.ros.org , aside from the other standard message type of std_msgs . The main ROS packages defining message types are grouped on common_interfaces with, for example geometry_msgs for objects defined on the Cartesian coordinate system, visualization_msgs focused on message types for visualization or sensor_msgs , well, for sensor-based data.","title":"Talker: a simple topic publisher"},{"location":"tuto-kick-off/first-contact/#continuous-talker","text":"In a second version, we aim to create a node publishing continuously messages. For that purpose we move on Event-Driven Programming. Technically in ROS, it is managed by the ros client, with the spin_once or spin functions. These \\(2\\) functions connect events (the reception of a message for instance) with the appropriate callback. spin_once activates once the ros client and wait for the next event. It should be called in an infinite loop. spin handle the infinite loop directly (and should be preferred). For the talker, you have to generate events with timers: aNode.create_timer( duration, aCallback ) The call back function aCallBack will be called after a duration is terminated. Furthermore, the call-back functions generally require a context of execution. Some already defined and accessible elements (for instance in our case, the publisher). For that purpose, we implement our aCallBack as a method of a class. The new talker script will lookalike: def infiniteTalk(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"infTalker\" ) talker= ROSTalker(aNode) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSTalker: def __init__(self, rosNode): self._publisher= rosNode.create_publisher( String, 'testTopic', 10 ) self._timer = rosNode.create_timer(0.5, self.timer_callback) self._i = 0 def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self._i self._publisher.publish(msg) self._i += 1 # Execute the function. if __name__ == \"__main__\": infiniteTalk() Now python3 playground/talker.py wil start a ROS node publishing \\(2\\) hello messages per second. A Ctr-C in the terminal should stop the process.","title":"Continuous Talker"},{"location":"tuto-kick-off/first-contact/#listener-a-simple-topic-subcriber","text":"In a very similar way, listener node is implemented with topics subscription mechanisms associated with a callback. import rclpy from rclpy.node import Node from std_msgs.msg import String def listen(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"listener\" ) listener= ROSListener(aNode) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSListener(): def __init__(self, rosNode): self._logger= rosNode.get_logger() self._subscription= rosNode.create_subscription( String, 'testTopic', self.listener_callback, 10 ) def listener_callback(self, msg): self._logger.info( 'I heard: ' + msg.data) if __name__ == '__main__': listen() Run your listener as you run your talker. The \\(2\\) nodes work together and define a very simple first software architecture.","title":"Listener: a simple topic subcriber"},{"location":"tuto-kick-off/first-contact/#logs","text":"To notice that ROS provides its own, \" print \" function with a logger instance. The logger, attached to a node instance ( aRosNode.get_logger() ), records logs through different channels: info , debug , warning or error . More on docs.ros.org . Logs can be followed with rqt_console tool. Follow the official tutorial to learn how to use this tool.","title":"Logs"},{"location":"tuto-kick-off/move/","text":"Move a robot This tutorial aims to take control of a pibot robot. A pibot is a turtlebot2 robot (based on a kobuki platform) equipped with a laser range to navigate in a cluttered environment and a Rapberry Pi3 to provide a ROS2 interface to the robot. Connect the pibot : The Rapberry Pi3 on each pibot are pre-configured with Ubuntu Server 22.04 , ROS2 Iron , and all the necessary drivers for the robot. By default, the pibot connects to the IoT WiFi network . The robot's control nodes are automatically launched at pibot startup via the mb6-tbot service, which executes the minimal_launch.yaml launch file from a tbot_node package. (More details about the IMT driver packages for turtlebot2 on github ) To avoid any conflict in mesages, each pibot has its own ROS_DOMAIN_ID . ROS environment has to be configured proprely to interact with a robot. For instance for the pibot20: export ROS_AUTOMATIC_DISCOVERY_RANGE=SUBNET export ROS_DOMAIN_ID=20 ros2 node list ros2 topic list Reminder: You can also explore the existing node with rqt_graph . Finally, you can try to take control in an available terminal with keyboard control: ros2 run teleop_twist_keyboard teleop_twist_keyboard cmd_vel:=/multi/cmd_teleop Close everything with ctrl-c . The teleop publishes a geometry_msgs twist message. It is composed of two vectors \\((x, y, z)\\) , one for linear speed \\((m/s)\\) , and the second for angular speed \\((rad/s)\\) . However a nonholonomic ground robot as the tbot would move only on x and turn only on z (It is not as free as a drone). Try to control the robot with ros2 topic pub command publishing in the navigation topic ( /multi/cmd_nav ). Tbot drivers integrate a subsumption multiplexer. The node listens different topics with different priorities (by default: /multi/cmd_nav and /multi/cmd_telop ) and filter the appropriate commands to send to the robot. The topics cmd_nav and cmd_telop stend for autonomous navigation and operator teleoperate. The human operator has a higher priority. Publishing messages into cmd_telop makes the multiplexer to trash the cmd_nav commands. A dedicated node to control The goal is to create a process connecting a topic and publishing velocities as a twist message: This tutorial supposes that you already perform the previous package tutorial and that you have a first package tutorial_pkg to work on it. First we have to create a file for our script. touch tutorial_pkg/scripts/test_move The script state that it requires python3 interpreter and it depends on several ROS2 resources. #!/usr/bin/python3 import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist print(\"test_move :: START...\") You can try that everything is correct by turning the script file executable and execute it: In a shell: chmod +x tutorial_pkg/scripts/test_move ./tutorial_pkg/scripts/test_move Finally, we must declare our node as one of the package node into CMakeList.txt and build again your packages. You can also add geometry_msgs as dependency on the package.xml file. colcon build source ./install/local_setup.bash Then, our node should be accessible from ros2 commands. ros2 run tutorial_pkg test_move A Simple Move order The node we aim to generate is similar to talker , except that we publish a twist message on the appropriate topic. The callback method of our class would be def timer_callback(self): velocity = Twist() # Feed Twist velocity values ... # Publish self._publisher.publish(velocity) A look at the geometry_msgs at index.ros.org Inform that Twist is composed by \\(2\\) Vector3 attributs also in geometry_msgs . Velocities are in \\(m.s^{-1}\\) and \\(\\math{rad}.s^{-1}\\) . Now, a ros2 run tutorial_pkg test_move should continuously move the robot, with a constant speed. Notice that you can also test your code on turtlesim by changing the name of the topic. Move Script At the end we want our robot to follow a predefined choreography. Something like moving forward for \\(X \\mathit{cm}\\) backward, turn left then turn right, etc. The choreography should end on the exact position it begins to perform an infinite loop safely.","title":"Move the Robot"},{"location":"tuto-kick-off/move/#move-a-robot","text":"This tutorial aims to take control of a pibot robot. A pibot is a turtlebot2 robot (based on a kobuki platform) equipped with a laser range to navigate in a cluttered environment and a Rapberry Pi3 to provide a ROS2 interface to the robot.","title":"Move a robot"},{"location":"tuto-kick-off/move/#connect-the-pibot","text":"The Rapberry Pi3 on each pibot are pre-configured with Ubuntu Server 22.04 , ROS2 Iron , and all the necessary drivers for the robot. By default, the pibot connects to the IoT WiFi network . The robot's control nodes are automatically launched at pibot startup via the mb6-tbot service, which executes the minimal_launch.yaml launch file from a tbot_node package. (More details about the IMT driver packages for turtlebot2 on github ) To avoid any conflict in mesages, each pibot has its own ROS_DOMAIN_ID . ROS environment has to be configured proprely to interact with a robot. For instance for the pibot20: export ROS_AUTOMATIC_DISCOVERY_RANGE=SUBNET export ROS_DOMAIN_ID=20 ros2 node list ros2 topic list Reminder: You can also explore the existing node with rqt_graph . Finally, you can try to take control in an available terminal with keyboard control: ros2 run teleop_twist_keyboard teleop_twist_keyboard cmd_vel:=/multi/cmd_teleop Close everything with ctrl-c . The teleop publishes a geometry_msgs twist message. It is composed of two vectors \\((x, y, z)\\) , one for linear speed \\((m/s)\\) , and the second for angular speed \\((rad/s)\\) . However a nonholonomic ground robot as the tbot would move only on x and turn only on z (It is not as free as a drone). Try to control the robot with ros2 topic pub command publishing in the navigation topic ( /multi/cmd_nav ). Tbot drivers integrate a subsumption multiplexer. The node listens different topics with different priorities (by default: /multi/cmd_nav and /multi/cmd_telop ) and filter the appropriate commands to send to the robot. The topics cmd_nav and cmd_telop stend for autonomous navigation and operator teleoperate. The human operator has a higher priority. Publishing messages into cmd_telop makes the multiplexer to trash the cmd_nav commands.","title":"Connect the pibot:"},{"location":"tuto-kick-off/move/#a-dedicated-node-to-control","text":"The goal is to create a process connecting a topic and publishing velocities as a twist message: This tutorial supposes that you already perform the previous package tutorial and that you have a first package tutorial_pkg to work on it. First we have to create a file for our script. touch tutorial_pkg/scripts/test_move The script state that it requires python3 interpreter and it depends on several ROS2 resources. #!/usr/bin/python3 import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist print(\"test_move :: START...\") You can try that everything is correct by turning the script file executable and execute it: In a shell: chmod +x tutorial_pkg/scripts/test_move ./tutorial_pkg/scripts/test_move Finally, we must declare our node as one of the package node into CMakeList.txt and build again your packages. You can also add geometry_msgs as dependency on the package.xml file. colcon build source ./install/local_setup.bash Then, our node should be accessible from ros2 commands. ros2 run tutorial_pkg test_move","title":"A dedicated node to control"},{"location":"tuto-kick-off/move/#a-simple-move-order","text":"The node we aim to generate is similar to talker , except that we publish a twist message on the appropriate topic. The callback method of our class would be def timer_callback(self): velocity = Twist() # Feed Twist velocity values ... # Publish self._publisher.publish(velocity) A look at the geometry_msgs at index.ros.org Inform that Twist is composed by \\(2\\) Vector3 attributs also in geometry_msgs . Velocities are in \\(m.s^{-1}\\) and \\(\\math{rad}.s^{-1}\\) . Now, a ros2 run tutorial_pkg test_move should continuously move the robot, with a constant speed. Notice that you can also test your code on turtlesim by changing the name of the topic.","title":"A Simple Move order"},{"location":"tuto-kick-off/move/#move-script","text":"At the end we want our robot to follow a predefined choreography. Something like moving forward for \\(X \\mathit{cm}\\) backward, turn left then turn right, etc. The choreography should end on the exact position it begins to perform an infinite loop safely.","title":"Move Script"},{"location":"tuto-kick-off/package/","text":"ROS Packages A ROS package regroups node definitions and other resources into a same location. It relies on colcon tool to build, install and manage the package dependancies. Documention on colcon.readthedocs.io . colcon cmds colcon is a meta command with several sub-command like ros2 or git . Its main sub-command is build . The colcon build command searches for all compliant packages included in a directory tree, at any detph. It should be processes at the root directory, so in your ros_space . cd ~/ros_space colcon build ls The build process generates tree directories: build with temporaly generated files by the build process ; install with generated ressources of the packages ; log for logs. At this time, there is no packages on your workspace, the build , install and log directories are almost empty. The results of colcon list (listing packages in the directory tree) or colcon graph (list with dependancies) are also empty. colcon list colcon graph Create a new package. A ROS package interpreted by colcon build is simplely a directory with configurations files. There is \\(2\\) files by using cmake build tool. package.xml : describing the package and its dependancies CMakeLists.txt : describing the build process (based on famous cmake tool) mkdir a-package touch a-package/package.xml touch a-package/CMakeLists.txt Naturally the package.xml and the CMakeLists.txt should include minimal informations in a correct way. rm -fr a-package colcon build ROS2 , with the pkg sub-command, provide a tool to generate the directory structure of a new package. Let create a tutorial_pkg package for instance. ros2 pkg create --build-type ament_cmake tutorial_pkg colcon list colcon buil Configuration files The package.xml provides metadata on the package (title, description, author licence). It should be manually completed. Its also give the build tool to use (a variation of cmake named ament-cmake in our exemple) and permit to specify dependancies with the mark <depend> . Typically, our new tutorial_pkg should depend from rclpy and std_msgs package and lines <depend>rclpy</depend> and <depend>std_msgs</depend> need to be added after the license information. To learn more about XML format: www.w3schools.com/xml Open the CMakeLists.txt file. At this step, it is almost empty. It only include information about the project to build, the C/C++ compiler to use, and some element relative to ament tool trigering action relative to ROS working environement. This file need to be update every time a new ressource (node or other) is added to the package project. Python Based Nodes For convenient reason, our python scripts, coding new ros nodes, will be saved on a scripts directory. Typically, we can install there your talker and listerner from previous tutorial. mkdir tutorial_pkg/scripts cp playground/talker.py tutorial_pkg/scripts/talker cp playground/listener.py tutorial_pkg/scripts/listener Also, we need to inform whitch interpreter can process our script (ie python3). Add a #!/usr/bin/python3 at the first line in talker and listerner scripts. The #! states that the code interpreter follow. /usr/bin/python3 is simplely the result of the command whereis python3 . Next, we have to modify CMakeLists.txt to state that talker and listerner should be installed as program (ie. in the appropriate destination to make them reachable by ros2 command). So add a python scripts section to your CMakeLists.txt file: # Python scripts install( PROGRAMS scripts/talker DESTINATION lib/${PROJECT_NAME} ) install( PROGRAMS scripts/listener DESTINATION lib/${PROJECT_NAME} ) You can now build again your ros workspace. The install directory contain a tutorial_pkg with every think=gs inside. colcon build ls install/ ls install/tutorial_pkg/lib/tutorial_pkg/ To use your package with ros2 commands, you have to update your bash environment. Then it would be possible to start your node with ros2 run command. source ./install/local_setup.bash ros2 run tutorial_pkg talker #or listener To notice that, if your python node script relies on a local python package ( myLocalPkg for instance). This one should be install aside of your script with the following cmake instruction: install( DIRECTORY scripts/myLocalPkg DESTINATION lib/${PROJECT_NAME} ) Launch file. ROS propose a launch mechanism to start in one command a configuration of several nodes. Launch file can be defined with markup language ( XML or YAML ) or python3 for more complex launch scenarios. Yaml purposes the simplest syntax to write launch files. The yaml.org give an example of a yaml resources. Similarly to Python , it relies on indentation to mark the ownership of elements. Another example: aMark: aMarkedAttribute: \"a string value\" aSecondAttribut: [\"a\", \"list\", \"of\", \"string\", \"element\"] 'a third attribute': - 1 - 2 - 12 again: anotherMark: aMarkedAttribute: 36.8 ... ROS yaml relies on predefinite marks, key works: 'launch' as the first element and composed by a list of node s. Minimal configuration for a node include pkg and exec attributes to identify the node to start. A simple launch file for talker/listener will be: launch: - node: pkg: \"tutorial_pkg\" exec: \"talker\" - node: pkg: \"tutorial_pkg\" exec: \"listener\" The file has to be set on a launch directory into your package and with a name ending by _launch.yaml , converse_launch.yaml for instance. At this point the launch file can be run using ros2 launch commands. ros2 launch ./tutorial_pkg/launch/converse_launch.yaml By adding launch resources to your package with CMakeList.txt configuration file, you make launch files easier to find. In CMakeList.txt file: # Install resource files. install(DIRECTORY launch DESTINATION share/${PROJECT_NAME}/ ) Then in the terminal: colcon build ros2 launch tutorial_pkg converse_launch.yaml To notice that other resources can be added to your package. Typically rviz for visualization configuration, world for simulation configuration, etc. # Install resource files. install(DIRECTORY launch rviz world DESTINATION share/${PROJECT_NAME}/ )","title":"ROS Package"},{"location":"tuto-kick-off/package/#ros-packages","text":"A ROS package regroups node definitions and other resources into a same location. It relies on colcon tool to build, install and manage the package dependancies. Documention on colcon.readthedocs.io .","title":"ROS Packages"},{"location":"tuto-kick-off/package/#colcon-cmds","text":"colcon is a meta command with several sub-command like ros2 or git . Its main sub-command is build . The colcon build command searches for all compliant packages included in a directory tree, at any detph. It should be processes at the root directory, so in your ros_space . cd ~/ros_space colcon build ls The build process generates tree directories: build with temporaly generated files by the build process ; install with generated ressources of the packages ; log for logs. At this time, there is no packages on your workspace, the build , install and log directories are almost empty. The results of colcon list (listing packages in the directory tree) or colcon graph (list with dependancies) are also empty. colcon list colcon graph","title":"colcon cmds"},{"location":"tuto-kick-off/package/#create-a-new-package","text":"A ROS package interpreted by colcon build is simplely a directory with configurations files. There is \\(2\\) files by using cmake build tool. package.xml : describing the package and its dependancies CMakeLists.txt : describing the build process (based on famous cmake tool) mkdir a-package touch a-package/package.xml touch a-package/CMakeLists.txt Naturally the package.xml and the CMakeLists.txt should include minimal informations in a correct way. rm -fr a-package colcon build ROS2 , with the pkg sub-command, provide a tool to generate the directory structure of a new package. Let create a tutorial_pkg package for instance. ros2 pkg create --build-type ament_cmake tutorial_pkg colcon list colcon buil","title":"Create a new package."},{"location":"tuto-kick-off/package/#configuration-files","text":"The package.xml provides metadata on the package (title, description, author licence). It should be manually completed. Its also give the build tool to use (a variation of cmake named ament-cmake in our exemple) and permit to specify dependancies with the mark <depend> . Typically, our new tutorial_pkg should depend from rclpy and std_msgs package and lines <depend>rclpy</depend> and <depend>std_msgs</depend> need to be added after the license information. To learn more about XML format: www.w3schools.com/xml Open the CMakeLists.txt file. At this step, it is almost empty. It only include information about the project to build, the C/C++ compiler to use, and some element relative to ament tool trigering action relative to ROS working environement. This file need to be update every time a new ressource (node or other) is added to the package project.","title":"Configuration files"},{"location":"tuto-kick-off/package/#python-based-nodes","text":"For convenient reason, our python scripts, coding new ros nodes, will be saved on a scripts directory. Typically, we can install there your talker and listerner from previous tutorial. mkdir tutorial_pkg/scripts cp playground/talker.py tutorial_pkg/scripts/talker cp playground/listener.py tutorial_pkg/scripts/listener Also, we need to inform whitch interpreter can process our script (ie python3). Add a #!/usr/bin/python3 at the first line in talker and listerner scripts. The #! states that the code interpreter follow. /usr/bin/python3 is simplely the result of the command whereis python3 . Next, we have to modify CMakeLists.txt to state that talker and listerner should be installed as program (ie. in the appropriate destination to make them reachable by ros2 command). So add a python scripts section to your CMakeLists.txt file: # Python scripts install( PROGRAMS scripts/talker DESTINATION lib/${PROJECT_NAME} ) install( PROGRAMS scripts/listener DESTINATION lib/${PROJECT_NAME} ) You can now build again your ros workspace. The install directory contain a tutorial_pkg with every think=gs inside. colcon build ls install/ ls install/tutorial_pkg/lib/tutorial_pkg/ To use your package with ros2 commands, you have to update your bash environment. Then it would be possible to start your node with ros2 run command. source ./install/local_setup.bash ros2 run tutorial_pkg talker #or listener To notice that, if your python node script relies on a local python package ( myLocalPkg for instance). This one should be install aside of your script with the following cmake instruction: install( DIRECTORY scripts/myLocalPkg DESTINATION lib/${PROJECT_NAME} )","title":"Python Based Nodes"},{"location":"tuto-kick-off/package/#launch-file","text":"ROS propose a launch mechanism to start in one command a configuration of several nodes. Launch file can be defined with markup language ( XML or YAML ) or python3 for more complex launch scenarios. Yaml purposes the simplest syntax to write launch files. The yaml.org give an example of a yaml resources. Similarly to Python , it relies on indentation to mark the ownership of elements. Another example: aMark: aMarkedAttribute: \"a string value\" aSecondAttribut: [\"a\", \"list\", \"of\", \"string\", \"element\"] 'a third attribute': - 1 - 2 - 12 again: anotherMark: aMarkedAttribute: 36.8 ... ROS yaml relies on predefinite marks, key works: 'launch' as the first element and composed by a list of node s. Minimal configuration for a node include pkg and exec attributes to identify the node to start. A simple launch file for talker/listener will be: launch: - node: pkg: \"tutorial_pkg\" exec: \"talker\" - node: pkg: \"tutorial_pkg\" exec: \"listener\" The file has to be set on a launch directory into your package and with a name ending by _launch.yaml , converse_launch.yaml for instance. At this point the launch file can be run using ros2 launch commands. ros2 launch ./tutorial_pkg/launch/converse_launch.yaml By adding launch resources to your package with CMakeList.txt configuration file, you make launch files easier to find. In CMakeList.txt file: # Install resource files. install(DIRECTORY launch DESTINATION share/${PROJECT_NAME}/ ) Then in the terminal: colcon build ros2 launch tutorial_pkg converse_launch.yaml To notice that other resources can be added to your package. Typically rviz for visualization configuration, world for simulation configuration, etc. # Install resource files. install(DIRECTORY launch rviz world DESTINATION share/${PROJECT_NAME}/ )","title":"Launch file."},{"location":"tuto-kick-off/ros-basics/","text":"Linux and ROS Basics The goal of this tutorial is to set up a workspace to control robots and to develop robotics programs. We choose to work with ROS (the most used open middleware for robotics) on a Linux-Ubuntu computer (which is the best supported configuration). This tutorial supposes that you have an Ubuntu-Like 22.04 computers with configured Python and C++ APIs. The tutorial relies on ROS2 Iron . Play with Linux Ubuntu is a distribution Linux (Operating System + Window Managment + Tools) derived from Debian distribution. It is a classical graphical operating system. You can explore the system and launch different program to test-it. In the variety of program, we are mostly interested in the terminal emulator, permitting to manipulate our system directly from command (moving in the directory tree, read and organize files, execute script or programs, administer...). gnome-terminal : on help.gnome.org Explore the following command (i.e. what for, how to use it). A good way to do this is to first Google the command ( example ), then play with the command in your terminal. man , ls , cp , mv , cat , rm source , apt , sudo To notice that tabulation allows for autocompletion. More commands: egrep , find , ps , ... ( Wikipedia is your friend ). Play with ROS The remainder of the tutorial relies directly on ROS documentation docs.ros.org ROS is mainly composed by Tools and Libraries . The Tools permit to configure and start a control architecture, to explore this architecture and to visualize data. The Libraries offer an API (Application Programming Interface) to developers who want to propose a new program compliant with ROS. Tools Start with Beginner CLI-Tools tutorials to get familiar with ROS tools. Some responses to the questions you could have : Yes we are in a classroom so we will work with : ROS_LOCALHOST_ONLY=1 . You can check your bash configuration by editing the hidden file at the user home directory: gedit ~/.bashrc bot user is sudoer, and the password is bot . Libraries Next, follow the Beginner Client-Libraries tutorials by focusing on Python language. Initially, we recommend using Python . While it is less efficient at runtime compared to C++, it is easier to use and debug, facilitating rapid development. If you are proficient in C++, you can use it instead. Otherwise, always prioritize Python commands. Although, you can use a basic text editor like gedit (available on Ubuntu), we encourage using code (Visual Studio Code) as your development environment. You can open Visual Studio Code by specifying a workspace in your terminal (and then manage multiple terminals directly within code ): cd code ros2_ws","title":"Linux and ROS Basics"},{"location":"tuto-kick-off/ros-basics/#linux-and-ros-basics","text":"The goal of this tutorial is to set up a workspace to control robots and to develop robotics programs. We choose to work with ROS (the most used open middleware for robotics) on a Linux-Ubuntu computer (which is the best supported configuration). This tutorial supposes that you have an Ubuntu-Like 22.04 computers with configured Python and C++ APIs. The tutorial relies on ROS2 Iron .","title":"Linux and ROS Basics"},{"location":"tuto-kick-off/ros-basics/#play-with-linux","text":"Ubuntu is a distribution Linux (Operating System + Window Managment + Tools) derived from Debian distribution. It is a classical graphical operating system. You can explore the system and launch different program to test-it. In the variety of program, we are mostly interested in the terminal emulator, permitting to manipulate our system directly from command (moving in the directory tree, read and organize files, execute script or programs, administer...). gnome-terminal : on help.gnome.org Explore the following command (i.e. what for, how to use it). A good way to do this is to first Google the command ( example ), then play with the command in your terminal. man , ls , cp , mv , cat , rm source , apt , sudo To notice that tabulation allows for autocompletion. More commands: egrep , find , ps , ... ( Wikipedia is your friend ).","title":"Play with Linux"},{"location":"tuto-kick-off/ros-basics/#play-with-ros","text":"The remainder of the tutorial relies directly on ROS documentation docs.ros.org ROS is mainly composed by Tools and Libraries . The Tools permit to configure and start a control architecture, to explore this architecture and to visualize data. The Libraries offer an API (Application Programming Interface) to developers who want to propose a new program compliant with ROS.","title":"Play with ROS"},{"location":"tuto-kick-off/ros-basics/#tools","text":"Start with Beginner CLI-Tools tutorials to get familiar with ROS tools. Some responses to the questions you could have : Yes we are in a classroom so we will work with : ROS_LOCALHOST_ONLY=1 . You can check your bash configuration by editing the hidden file at the user home directory: gedit ~/.bashrc bot user is sudoer, and the password is bot .","title":"Tools"},{"location":"tuto-kick-off/ros-basics/#libraries","text":"Next, follow the Beginner Client-Libraries tutorials by focusing on Python language. Initially, we recommend using Python . While it is less efficient at runtime compared to C++, it is easier to use and debug, facilitating rapid development. If you are proficient in C++, you can use it instead. Otherwise, always prioritize Python commands. Although, you can use a basic text editor like gedit (available on Ubuntu), we encourage using code (Visual Studio Code) as your development environment. You can open Visual Studio Code by specifying a workspace in your terminal (and then manage multiple terminals directly within code ): cd code ros2_ws","title":"Libraries"},{"location":"tuto-kick-off/simulation/","text":"Simulation in ROS Gazebo Simulator Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI systems using realistic scenarios. Gazebo is integrated with ROS and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available. Gazebo Installation Verify that Gazebo is installed using: dpkg -l | grep gazebo You should have at least the following packages: ii gazebo 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Binaries ii gazebo-common 11.10.2+dfsg-1 all Open Source Robotics Simulator - Shared files ii gazebo-plugin-base 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo-dev 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Development Files ii libgazebo11:amd64 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - shared library ii ros-iron-gazebo-dev 3.7.0-3jammy.20230622.191804 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.7.0-3jammy.20231117.090251 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.7.0-3jammy.20231117.111548 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.7.0-3jammy.20231117.104944 amd64 Utilities to interface with Gazebo through ROS. ii ros-iron-gazebo-ros-pkgs 3.7.0-3jammy.20231117.114324 amd64 Interface for using ROS with the Gazebo simulator. ii ros-iron-turtlebot3-gazebo 2.2.5-4jammy.20231117.114359 amd64 Gazebo simulation package for the TurtleBot3 Install missing packages using: sudo apt install <pakage_name> Launch your first Gazebo Simulation We propose some configuration into a pkg-tsim project, including a tbot_sim ROS2 package. cd ~/ros_space git clone https://github.com/imt-mobisyst/pkg-tsim colcon build source ./install/setup.bash Then, you can launch a preconfigured simulation: ros2 launch tbot_sim challenge-1.launch.py Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera (kinect). The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipment. Quiz on challenge 1 While the challenge 1 simulation is running: Question: which topics are available i.e published by Gazebo? Hint: an infinite loop safely` is a versatile and powerful tool to display data published in topics. Launch it: rviz2 Question: How to configure rviz2 to visualize the laser scans? Be careful, ensure that Global Option / Fixed frame is correctly set to base_link . Question: why is this important? (hint: check your tf using ros2 run tf2_tools view_frames ) You can also display the tf in rviz2 directly. Question: How to visualize camera images using rqt ? Controlling the Simulated Robot Launch a simple node to control the simulated robot using keyboard: ros2 run teleop_twist_keyboard teleop_twist_keyboard tuto_sim Create a launch file that starts the appropriate configuration: the challenge-1 , a configured rviz2 displaying laser scans and the teleop . We will prefer YAML format for launch file. All the information you need are in the tutorials : - Launch file documentation - Launch examples Create $ROS_WORKSPACE/pkg-tsim/tbot_sim/launch/tutosim_launch.yaml Add this code into this file to: launch: - include: file: \"$(find-pkg-share tbot_sim)/launch/challenge-1.launch.py\" - node: pkg: \"rviz2\" exec: \"rviz2\" name: \"rviz2\" - executable: cmd: gnome-terminal --tab -e 'ros2 run teleop_twist_keyboard teleop_twist_keyboard' Exercise: modify this launch file so that rviz loads a saved configuration file when it starts. This configuration file should add the laser data, ...","title":"Simulation"},{"location":"tuto-kick-off/simulation/#simulation-in-ros","text":"","title":"Simulation in ROS"},{"location":"tuto-kick-off/simulation/#gazebo-simulator","text":"Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI systems using realistic scenarios. Gazebo is integrated with ROS and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available.","title":"Gazebo Simulator"},{"location":"tuto-kick-off/simulation/#gazebo-installation","text":"Verify that Gazebo is installed using: dpkg -l | grep gazebo You should have at least the following packages: ii gazebo 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Binaries ii gazebo-common 11.10.2+dfsg-1 all Open Source Robotics Simulator - Shared files ii gazebo-plugin-base 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo-dev 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Development Files ii libgazebo11:amd64 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - shared library ii ros-iron-gazebo-dev 3.7.0-3jammy.20230622.191804 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.7.0-3jammy.20231117.090251 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.7.0-3jammy.20231117.111548 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.7.0-3jammy.20231117.104944 amd64 Utilities to interface with Gazebo through ROS. ii ros-iron-gazebo-ros-pkgs 3.7.0-3jammy.20231117.114324 amd64 Interface for using ROS with the Gazebo simulator. ii ros-iron-turtlebot3-gazebo 2.2.5-4jammy.20231117.114359 amd64 Gazebo simulation package for the TurtleBot3 Install missing packages using: sudo apt install <pakage_name>","title":"Gazebo Installation"},{"location":"tuto-kick-off/simulation/#launch-your-first-gazebo-simulation","text":"We propose some configuration into a pkg-tsim project, including a tbot_sim ROS2 package. cd ~/ros_space git clone https://github.com/imt-mobisyst/pkg-tsim colcon build source ./install/setup.bash Then, you can launch a preconfigured simulation: ros2 launch tbot_sim challenge-1.launch.py Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera (kinect). The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipment.","title":"Launch your first Gazebo Simulation"},{"location":"tuto-kick-off/simulation/#quiz-on-challenge-1","text":"While the challenge 1 simulation is running: Question: which topics are available i.e published by Gazebo? Hint: an infinite loop safely` is a versatile and powerful tool to display data published in topics. Launch it: rviz2 Question: How to configure rviz2 to visualize the laser scans? Be careful, ensure that Global Option / Fixed frame is correctly set to base_link . Question: why is this important? (hint: check your tf using ros2 run tf2_tools view_frames ) You can also display the tf in rviz2 directly. Question: How to visualize camera images using rqt ?","title":"Quiz on challenge 1"},{"location":"tuto-kick-off/simulation/#controlling-the-simulated-robot","text":"Launch a simple node to control the simulated robot using keyboard: ros2 run teleop_twist_keyboard teleop_twist_keyboard","title":"Controlling the Simulated Robot"},{"location":"tuto-kick-off/simulation/#tuto_sim","text":"Create a launch file that starts the appropriate configuration: the challenge-1 , a configured rviz2 displaying laser scans and the teleop . We will prefer YAML format for launch file. All the information you need are in the tutorials : - Launch file documentation - Launch examples Create $ROS_WORKSPACE/pkg-tsim/tbot_sim/launch/tutosim_launch.yaml Add this code into this file to: launch: - include: file: \"$(find-pkg-share tbot_sim)/launch/challenge-1.launch.py\" - node: pkg: \"rviz2\" exec: \"rviz2\" name: \"rviz2\" - executable: cmd: gnome-terminal --tab -e 'ros2 run teleop_twist_keyboard teleop_twist_keyboard' Exercise: modify this launch file so that rviz loads a saved configuration file when it starts. This configuration file should add the laser data, ...","title":"tuto_sim"},{"location":"tuto-level-up/camera-driver/","text":"Camera Driver This tutorial cover the basis of the integration of a new sensor in ROS 2 environment. In our case we will integrate Realsense D400 RGBDi camera. Drivers First, the integration of a sensor require to identify the driver (the piece of code permiting to communicate with a devices - hardware level) and the API (Application Programming interface). Concretly, we mostly seek for the appropriate librairies correctly integrated to our system. Id\u00e9aly the community already support the desired librairies (like for libsdl2 for instance, simple C lib \"to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware\"). By searching for libsdl2 with Ubuntu-Aptitude we will find several packages ready to be installed: apt search libsdl2 libsdl2-x.x runing librairies (installed if programs use SDL2 ) libsdl2-dev development file (to install if you plan to develop a program based on SDL2 ) and some extra libs. Other wise, we have to build/compile the driver from source code. In case of realsense, librealsense recommand to use vcpkg to build and install it. Normally, after installation, you can run a small script to request the cam (more on dev.intelrealsense.com ): #!/usr/bin/env python3 ############################################### ## Simple Request ## ############################################### import pyrealsense2 as rs # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) Copy the code on a test-camera.py file and process it ( python3 test-camera.py ). Try this script with severals cameras, not all the Realsense provide for IMU (accelerations) information. OpenCV2 - the queen of the vision librairie. Next we can try to visualise the image flux in a windows. For that we will use OpenCV2 librairy (an open source computer vision library). The next script, adapted from the oficial documentation, connect the camera, and display both the image and distance image in an infinite loop ( while True ). Based on librealsense, the script activates the expected data flux, with the wanted configuration (848x480 imagein a given format at 60 Hertz): config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) Start the aquisition process ( pipeline.start(config) ) Still with librealsense, the script wait for incomming data and get them: frames = pipeline.wait_for_frames() depth_frame = frames.first(rs.stream.depth) color_frame = frames.first(rs.stream.color) Then, the reminder of the script consists in converting and displaying the data based on Numpy and OpenCV #!/usr/bin/env python3 ## Doc: https://dev.intelrealsense.com/docs/python2 ############################################### ## Open CV and Numpy integration ## ############################################### import pyrealsense2 as rs import signal, time, numpy as np import sys, cv2, rclpy # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) found_rgb = True for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) if s.get_info(rs.camera_info.name) == 'RGB Camera': found_rgb = True if not (found_rgb): print(\"Depth camera equired !!!\") exit(0) config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) # Capture ctrl-c event isOk= True def signalInteruption(signum, frame): global isOk print( \"\\nCtrl-c pressed\" ) isOk= False signal.signal(signal.SIGINT, signalInteruption) # Start streaming pipeline.start(config) count= 1 refTime= time.process_time() freq= 60 sys.stdout.write(\"-\") while isOk: # Wait for a coherent tuple of frames: depth, color and accel frames = pipeline.wait_for_frames() color_frame = frames.first(rs.stream.color) depth_frame = frames.first(rs.stream.depth) if not (depth_frame and color_frame): continue # Convert images to numpy arrays depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # Apply colormap on depth image (image must be converted to 8-bit per pixel first) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape sys.stdout.write( f\"\\r- {color_colormap_dim} - {depth_colormap_dim} - ({round(freq)} fps)\" ) # Show images images = np.hstack((color_image, depth_colormap)) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 # Stop streaming print(\"\\nEnding...\") pipeline.stop() To notice the use of signal python librairy that permit to catch and process interuption signal ( ctrl-c ). Publish sensor-data Stating from the previous script, the goal is to encapsulated the connection to the camera into a ROS2 Node in a tuto_vision package (only the camera image flux). Considering previous developed ROS2 Node : We want to keep the control on the infinite loop. We will publish sensor_msgs/image . The ros2 documentation is verry poor, but ROS1 wiki remains valuable. Node structure: To control the infinite loop we will prefer spin_once to spin . To notice that spin_once process once the ROS2 instructions. It blocks until an event occurs. It is possible to overpass that by specifying a timeout ( spin_once(myNode, timeout_sec=0.01) ). At the end we want a ROS2 Node that connect the camera and publish continously the images (color and depth images). The python function of the Node will look like: # Node processes: def process_img(args=None): rclpy.init(args=args) rsNode= Realsense() while isOk: rsNode.read_imgs() rsNode.publish_imgs() rclpy.spin_once(rsNode, timeout_sec=0.001) # Stop streaming print(\"Ending...\") rsNode.pipeline.stop() # Clean end rsNode.destroy_node() rclpy.shutdown() You ca start from a blanc class Realsence and fill the differents methods step by steps (testing your code at each steps): # Realsense Node: class Realsense(Node): def __init__(self, fps= 60): super().__init__('realsense') def read_imgs(self): pass def publish_imgs(self): pass Publish Images: sensor_msgs include header to state for spacio-temporal information. Mainly the reference frame (ie. cam for instance) and time. For time stamp, get_clock() permits to get a clock of a Node instance ( node= Node() or self in case of ineritance) then now() and to_msg() methods respectivelly provide curent time() and convert it into a msg compliant format. msg.header.stamp = node.get_clock().now().to_msg() Then it is possible to feed sensor_msgs/image attributs (starting with msg.encoding= \"bgr8\" seems a good idea.) However, a librairy provides some tool to work both with ROS and OpenCV ( cv_bridge ). The code for image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() msg_image = self.bridge.cv2_to_imgmsg(color_image,\"bgr8\") msg_image.header.stamp = self.get_clock().now().to_msg() msg_image.header.frame_id = \"image\" self.image_publisher.publish(msg_image) The code for depth image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() # Utilisation de colormap sur l'image depth de la Realsense (image convertie en 8-bit par pixel) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,\"bgr8\") msg_depth.header.stamp = msg_image.header.stamp msg_depth.header.frame_id = \"depth\" self.depth_publisher.publish(msg_depth) Some test: At this point it is not relevant any more to show the images inside a CV2 window or to compute and print a frequency. The frequency can be conputed with ROS2 tool: ros2 topic hz \\img . The images are displayable into rviz2 program. Going futher: Play with the other streams provided with the RealSens sensor. Code to add both infrared channels self.config.enable_stream(rs.stream.infrared, 1, 848, 480, rs.format.y8, 60) self.config.enable_stream(rs.stream.infrared, 2, 848, 480, rs.format.y8, 60) self.infra_publisher_1 = self.create_publisher(Image, 'infrared_1',10) self.infra_publisher_2 = self.create_publisher(Image, 'infrared_2',10) infra_image_1 = np.asanyarray(infra_frame_1.get_data()) infra_image_2 = np.asanyarray(infra_frame_2.get_data()) in the loop : infra_frame_1 = frames.get_infrared_frame(1) infra_frame_2 = frames.get_infrared_frame(2) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_1 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_1, alpha=0.03), cv2.COLORMAP_JET) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_2 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_2, alpha=0.03), cv2.COLORMAP_JET) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_1,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_1\" self.infra_publisher_1.publish(msg_infra) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_2,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_2\" self.infra_publisher_2.publish(msg_infra)","title":"Camera Driver"},{"location":"tuto-level-up/camera-driver/#camera-driver","text":"This tutorial cover the basis of the integration of a new sensor in ROS 2 environment. In our case we will integrate Realsense D400 RGBDi camera.","title":"Camera Driver"},{"location":"tuto-level-up/camera-driver/#drivers","text":"First, the integration of a sensor require to identify the driver (the piece of code permiting to communicate with a devices - hardware level) and the API (Application Programming interface). Concretly, we mostly seek for the appropriate librairies correctly integrated to our system. Id\u00e9aly the community already support the desired librairies (like for libsdl2 for instance, simple C lib \"to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware\"). By searching for libsdl2 with Ubuntu-Aptitude we will find several packages ready to be installed: apt search libsdl2 libsdl2-x.x runing librairies (installed if programs use SDL2 ) libsdl2-dev development file (to install if you plan to develop a program based on SDL2 ) and some extra libs. Other wise, we have to build/compile the driver from source code. In case of realsense, librealsense recommand to use vcpkg to build and install it. Normally, after installation, you can run a small script to request the cam (more on dev.intelrealsense.com ): #!/usr/bin/env python3 ############################################### ## Simple Request ## ############################################### import pyrealsense2 as rs # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) Copy the code on a test-camera.py file and process it ( python3 test-camera.py ). Try this script with severals cameras, not all the Realsense provide for IMU (accelerations) information.","title":"Drivers"},{"location":"tuto-level-up/camera-driver/#opencv2-the-queen-of-the-vision-librairie","text":"Next we can try to visualise the image flux in a windows. For that we will use OpenCV2 librairy (an open source computer vision library). The next script, adapted from the oficial documentation, connect the camera, and display both the image and distance image in an infinite loop ( while True ). Based on librealsense, the script activates the expected data flux, with the wanted configuration (848x480 imagein a given format at 60 Hertz): config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) Start the aquisition process ( pipeline.start(config) ) Still with librealsense, the script wait for incomming data and get them: frames = pipeline.wait_for_frames() depth_frame = frames.first(rs.stream.depth) color_frame = frames.first(rs.stream.color) Then, the reminder of the script consists in converting and displaying the data based on Numpy and OpenCV #!/usr/bin/env python3 ## Doc: https://dev.intelrealsense.com/docs/python2 ############################################### ## Open CV and Numpy integration ## ############################################### import pyrealsense2 as rs import signal, time, numpy as np import sys, cv2, rclpy # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) found_rgb = True for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) if s.get_info(rs.camera_info.name) == 'RGB Camera': found_rgb = True if not (found_rgb): print(\"Depth camera equired !!!\") exit(0) config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) # Capture ctrl-c event isOk= True def signalInteruption(signum, frame): global isOk print( \"\\nCtrl-c pressed\" ) isOk= False signal.signal(signal.SIGINT, signalInteruption) # Start streaming pipeline.start(config) count= 1 refTime= time.process_time() freq= 60 sys.stdout.write(\"-\") while isOk: # Wait for a coherent tuple of frames: depth, color and accel frames = pipeline.wait_for_frames() color_frame = frames.first(rs.stream.color) depth_frame = frames.first(rs.stream.depth) if not (depth_frame and color_frame): continue # Convert images to numpy arrays depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # Apply colormap on depth image (image must be converted to 8-bit per pixel first) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape sys.stdout.write( f\"\\r- {color_colormap_dim} - {depth_colormap_dim} - ({round(freq)} fps)\" ) # Show images images = np.hstack((color_image, depth_colormap)) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 # Stop streaming print(\"\\nEnding...\") pipeline.stop() To notice the use of signal python librairy that permit to catch and process interuption signal ( ctrl-c ).","title":"OpenCV2 - the queen of the vision librairie."},{"location":"tuto-level-up/camera-driver/#publish-sensor-data","text":"Stating from the previous script, the goal is to encapsulated the connection to the camera into a ROS2 Node in a tuto_vision package (only the camera image flux). Considering previous developed ROS2 Node : We want to keep the control on the infinite loop. We will publish sensor_msgs/image . The ros2 documentation is verry poor, but ROS1 wiki remains valuable.","title":"Publish sensor-data"},{"location":"tuto-level-up/camera-driver/#node-structure","text":"To control the infinite loop we will prefer spin_once to spin . To notice that spin_once process once the ROS2 instructions. It blocks until an event occurs. It is possible to overpass that by specifying a timeout ( spin_once(myNode, timeout_sec=0.01) ). At the end we want a ROS2 Node that connect the camera and publish continously the images (color and depth images). The python function of the Node will look like: # Node processes: def process_img(args=None): rclpy.init(args=args) rsNode= Realsense() while isOk: rsNode.read_imgs() rsNode.publish_imgs() rclpy.spin_once(rsNode, timeout_sec=0.001) # Stop streaming print(\"Ending...\") rsNode.pipeline.stop() # Clean end rsNode.destroy_node() rclpy.shutdown() You ca start from a blanc class Realsence and fill the differents methods step by steps (testing your code at each steps): # Realsense Node: class Realsense(Node): def __init__(self, fps= 60): super().__init__('realsense') def read_imgs(self): pass def publish_imgs(self): pass","title":"Node structure:"},{"location":"tuto-level-up/camera-driver/#publish-images","text":"sensor_msgs include header to state for spacio-temporal information. Mainly the reference frame (ie. cam for instance) and time. For time stamp, get_clock() permits to get a clock of a Node instance ( node= Node() or self in case of ineritance) then now() and to_msg() methods respectivelly provide curent time() and convert it into a msg compliant format. msg.header.stamp = node.get_clock().now().to_msg() Then it is possible to feed sensor_msgs/image attributs (starting with msg.encoding= \"bgr8\" seems a good idea.) However, a librairy provides some tool to work both with ROS and OpenCV ( cv_bridge ). The code for image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() msg_image = self.bridge.cv2_to_imgmsg(color_image,\"bgr8\") msg_image.header.stamp = self.get_clock().now().to_msg() msg_image.header.frame_id = \"image\" self.image_publisher.publish(msg_image) The code for depth image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() # Utilisation de colormap sur l'image depth de la Realsense (image convertie en 8-bit par pixel) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,\"bgr8\") msg_depth.header.stamp = msg_image.header.stamp msg_depth.header.frame_id = \"depth\" self.depth_publisher.publish(msg_depth)","title":"Publish Images:"},{"location":"tuto-level-up/camera-driver/#some-test","text":"At this point it is not relevant any more to show the images inside a CV2 window or to compute and print a frequency. The frequency can be conputed with ROS2 tool: ros2 topic hz \\img . The images are displayable into rviz2 program.","title":"Some test:"},{"location":"tuto-level-up/camera-driver/#going-futher","text":"Play with the other streams provided with the RealSens sensor. Code to add both infrared channels self.config.enable_stream(rs.stream.infrared, 1, 848, 480, rs.format.y8, 60) self.config.enable_stream(rs.stream.infrared, 2, 848, 480, rs.format.y8, 60) self.infra_publisher_1 = self.create_publisher(Image, 'infrared_1',10) self.infra_publisher_2 = self.create_publisher(Image, 'infrared_2',10) infra_image_1 = np.asanyarray(infra_frame_1.get_data()) infra_image_2 = np.asanyarray(infra_frame_2.get_data()) in the loop : infra_frame_1 = frames.get_infrared_frame(1) infra_frame_2 = frames.get_infrared_frame(2) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_1 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_1, alpha=0.03), cv2.COLORMAP_JET) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_2 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_2, alpha=0.03), cv2.COLORMAP_JET) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_1,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_1\" self.infra_publisher_1.publish(msg_infra) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_2,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_2\" self.infra_publisher_2.publish(msg_infra)","title":"Going futher:"},{"location":"tuto-level-up/class/","text":"Create Node with Class The official ROS tutorial invites to use inheritance of ROS node. We propose here, to explore another way. Motivation ROS-node philosophy relies on events that trigger call-back function. Typically, the subscription to a topic trigger a function to process (a call-back) each time a new message is available. It is coded as : # Define a callback function def aCallback( aMsg ): ... # Connect a callback to events. aRosNode= Node('node_name') aRosNode.create_subscription( MsgType, 'scan', aCallback, 10) However, we generally need a context of execution for a given callback. We wan to access certain information in the callback definition (parameters, recorded previous data, et.). Or, in a more technical way, we need something like: # Define a callback function def aCallback( aContext, aMsg ): ... And Object-Oriented Programming allows such a thing. The trick consists of defining a class to model the context, and use a method of the class as call-back function. In python: # Define a callback function class Context : def aCallback( self, aMsg ): ... # Connect a callback to events. aRosNode= Node('node_name') myContext= Context() aRosNode.create_subscription( MsgType, 'scan', myContext.aCallback, 10) ROS solution The ROS tutorial propose a solution based on inheritence . The Context class inherits from ros Node class. Example with the subscriber: #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String class MinimalSubscriber(Node): def __init__(self): super().__init__('minimal_subscriber') self.subscription= self.create_subscription( String, 'topic', self.listener_callback, 10) def listener_callback(self, msg): self.get_logger().info('I heard: \"%s\"' % msg.data) def main(args=None): rclpy.init(args=args) minimal_subscriber = MinimalSubscriber() rclpy.spin(minimal_subscriber) minimal_subscriber.destroy_node() rclpy.shutdown() if __name__ == '__main__': main() As a result, Node method can be performed on self as for create_subscription , but the inheritance needs to be set correctly (call to super().__init__ ) and the node instance need to be created after the initialization to rclpy . Alternative Solution An alternative solution aims to minimize the effort developers should do when they use our solution. Typically, the number of lines in the main function, with a main for instance as : def main(): minimal_subscriber= MSContext() minimal_subscriber.process() The management of ros client ( rclpy ) and ros node is hidden to the user. For this purpose, one of the solutions, is to define initialize ros client only in the process method and to use ROS node as an attribute of the class. The solution could be, for instance: class MSContext(): def listener_callback(self, msg): self._node.get_logger().info('I heard: \"%s\"' % msg.data) def process(self) rclpy.init(args=args) self._node = Node() self._node.create_subscription( String, 'topic', self.listener_callback, 10) # Infinite loop: rclpy.spin(minimal_subscriber) # Clean stop: minimal_subscriber.destroy_node() rclpy.shutdown() Event-driven programming To notice that Event-Driven Programming is not something unique for ROS. Typically, most of the IHM API or web technology are based on EDP. Technically in ROS, it is managed by the ros client, with the spin_once or spin functions. These 2 functions connect events (the reception of a message for instance) with the appropriate callback. It is also those functions that effectively push messages into the topics after a publishe action. spin_once activates once the ros client and should be call in an infinite loop. spin handle the infinite loop directly (and should be preferred). To notice that it is possible to generate events with timers: aNode.create_timer( timer_period, timer_callback ) For instance, with a simple publisher: #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String class MPContext: def __init__(self): self.i = 0 def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self.i self._node.publisher_.publish(msg) self._node.get_logger().info('Publishing: \"%s\"' % msg.data) self.i += 1 def process(self): rclpy.init() self._node= Node() # Create a publisher self._publisher= self._node.create_publisher(String, 'topic', 10) # Create a timer at 2 hertz, with a callback self._timer = self._node.create_timer(0.5, self.timer_callback) # Go rclpy.spin(self._node) # Clean stop self._node.minimal_publisher.destroy_node() rclpy.shutdown() if __name__ == '__main__': rosContext= MPContext() rosContext.process() Infinit Safe Move Create a new node reactive_move in your package that will command the robot velocities in a way that the robot will avoid the obstacles. The node subscribes to scan data and publish velocities. Determine a rectangle in front of the robot and get the point cloud obstacles in this rectangle. If an obstacle is present in the right part of the rectangle, turn left. If an obstacle in present in the left part of the rectangle, turn right. Otherwise move in a straight line. Calibrate the rectangle configuration and the speeds to get a coherent and safe control. Add rules to better control the robot in a dead end scenario When it is working in the simulation, test your solution on a real robot. IMPORTANT - For a better security, implement the control function independently from the scan callback and activate your control at a desired frequency by using a timer. OPTIONAL - Stop the robot if no scan was arrived the last second and test it by stopping the urg_node.","title":"Create Node with Class"},{"location":"tuto-level-up/class/#create-node-with-class","text":"The official ROS tutorial invites to use inheritance of ROS node. We propose here, to explore another way.","title":"Create Node with Class"},{"location":"tuto-level-up/class/#motivation","text":"ROS-node philosophy relies on events that trigger call-back function. Typically, the subscription to a topic trigger a function to process (a call-back) each time a new message is available. It is coded as : # Define a callback function def aCallback( aMsg ): ... # Connect a callback to events. aRosNode= Node('node_name') aRosNode.create_subscription( MsgType, 'scan', aCallback, 10) However, we generally need a context of execution for a given callback. We wan to access certain information in the callback definition (parameters, recorded previous data, et.). Or, in a more technical way, we need something like: # Define a callback function def aCallback( aContext, aMsg ): ... And Object-Oriented Programming allows such a thing. The trick consists of defining a class to model the context, and use a method of the class as call-back function. In python: # Define a callback function class Context : def aCallback( self, aMsg ): ... # Connect a callback to events. aRosNode= Node('node_name') myContext= Context() aRosNode.create_subscription( MsgType, 'scan', myContext.aCallback, 10)","title":"Motivation"},{"location":"tuto-level-up/class/#ros-solution","text":"The ROS tutorial propose a solution based on inheritence . The Context class inherits from ros Node class. Example with the subscriber: #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String class MinimalSubscriber(Node): def __init__(self): super().__init__('minimal_subscriber') self.subscription= self.create_subscription( String, 'topic', self.listener_callback, 10) def listener_callback(self, msg): self.get_logger().info('I heard: \"%s\"' % msg.data) def main(args=None): rclpy.init(args=args) minimal_subscriber = MinimalSubscriber() rclpy.spin(minimal_subscriber) minimal_subscriber.destroy_node() rclpy.shutdown() if __name__ == '__main__': main() As a result, Node method can be performed on self as for create_subscription , but the inheritance needs to be set correctly (call to super().__init__ ) and the node instance need to be created after the initialization to rclpy .","title":"ROS solution"},{"location":"tuto-level-up/class/#alternative-solution","text":"An alternative solution aims to minimize the effort developers should do when they use our solution. Typically, the number of lines in the main function, with a main for instance as : def main(): minimal_subscriber= MSContext() minimal_subscriber.process() The management of ros client ( rclpy ) and ros node is hidden to the user. For this purpose, one of the solutions, is to define initialize ros client only in the process method and to use ROS node as an attribute of the class. The solution could be, for instance: class MSContext(): def listener_callback(self, msg): self._node.get_logger().info('I heard: \"%s\"' % msg.data) def process(self) rclpy.init(args=args) self._node = Node() self._node.create_subscription( String, 'topic', self.listener_callback, 10) # Infinite loop: rclpy.spin(minimal_subscriber) # Clean stop: minimal_subscriber.destroy_node() rclpy.shutdown()","title":"Alternative Solution"},{"location":"tuto-level-up/class/#event-driven-programming","text":"To notice that Event-Driven Programming is not something unique for ROS. Typically, most of the IHM API or web technology are based on EDP. Technically in ROS, it is managed by the ros client, with the spin_once or spin functions. These 2 functions connect events (the reception of a message for instance) with the appropriate callback. It is also those functions that effectively push messages into the topics after a publishe action. spin_once activates once the ros client and should be call in an infinite loop. spin handle the infinite loop directly (and should be preferred). To notice that it is possible to generate events with timers: aNode.create_timer( timer_period, timer_callback ) For instance, with a simple publisher: #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String class MPContext: def __init__(self): self.i = 0 def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self.i self._node.publisher_.publish(msg) self._node.get_logger().info('Publishing: \"%s\"' % msg.data) self.i += 1 def process(self): rclpy.init() self._node= Node() # Create a publisher self._publisher= self._node.create_publisher(String, 'topic', 10) # Create a timer at 2 hertz, with a callback self._timer = self._node.create_timer(0.5, self.timer_callback) # Go rclpy.spin(self._node) # Clean stop self._node.minimal_publisher.destroy_node() rclpy.shutdown() if __name__ == '__main__': rosContext= MPContext() rosContext.process()","title":"Event-driven programming"},{"location":"tuto-level-up/class/#infinit-safe-move","text":"Create a new node reactive_move in your package that will command the robot velocities in a way that the robot will avoid the obstacles. The node subscribes to scan data and publish velocities. Determine a rectangle in front of the robot and get the point cloud obstacles in this rectangle. If an obstacle is present in the right part of the rectangle, turn left. If an obstacle in present in the left part of the rectangle, turn right. Otherwise move in a straight line. Calibrate the rectangle configuration and the speeds to get a coherent and safe control. Add rules to better control the robot in a dead end scenario When it is working in the simulation, test your solution on a real robot. IMPORTANT - For a better security, implement the control function independently from the scan callback and activate your control at a desired frequency by using a timer. OPTIONAL - Stop the robot if no scan was arrived the last second and test it by stopping the urg_node.","title":"Infinit Safe Move"},{"location":"tuto-level-up/navigation/","text":"Autonomous Navigation ROS2 https://navigation.ros.org/ ros2 launch nav2_bringup navigation_launch.py Then, send goal points into /goal_pose ROS1 <-> ROS2 https://navigation.ros.org/about/ros1_comparison.html#ros1-comparison move_base => nav2_bt_navigator","title":"Autonomous Navigation"},{"location":"tuto-level-up/navigation/#autonomous-navigation","text":"","title":"Autonomous Navigation"},{"location":"tuto-level-up/navigation/#ros2","text":"https://navigation.ros.org/ ros2 launch nav2_bringup navigation_launch.py Then, send goal points into /goal_pose","title":"ROS2"},{"location":"tuto-level-up/navigation/#ros1-ros2","text":"https://navigation.ros.org/about/ros1_comparison.html#ros1-comparison move_base => nav2_bt_navigator","title":"ROS1 &lt;-&gt; ROS2"},{"location":"tuto-level-up/package/","text":"03 - Create a ROS compatible package This tutorial is based on the official documentation: docs.ros.org , with some tuning considering our preferences... Typically we prefer cmake method, whatever the targeted language ( C++ or python ). Your first package in ROS2 You can build your first package using the ros tool, with the build-type ament_cmake , then compile and install the package. Typically a tuto_kickoff package: ros2 pkg create --build-type ament_cmake tuto_kickoff colcon build --packages-select tuto_kickoff Package creation generate files identifying tuto_kickoff directory as a ROS package source. The file package.xml setup the ROS configuration, the name of the packages and its dependencies in ROS ecosystem. The file CMakeLists.txt is required to use CMake tool, a very famous cross-platform tool for automatizing building processes. The colcon build command produces the build and the install directories with respectively, temporary files generated by the build process and the resulting built executable and resources (yes, none at this point...). To inform your terminal that some ROS resources are available here, you have to source the ROS setup file. ls source ./install/setup.bash The package.xml file As we said, package.xml is the entrance point of the package for ROS tools. It is a text file in eXtensible Markup Language format. Typically, it regroups all the dependencies, for instance rclcpp and rclpy the ROS client libraries for Cpp and Python or std_msgs . std_msgs and geometry_msgs are a ROS package defining the format of the simple and common messages allowing the communication between ROS processes (standard and geometry messages for our use). In the package.xml file : <package format=\"3\"> ... <depend>rclcpp</depend> <depend>rclpy</depend> <depend>std_msgs</depend> <depend>geometry_msgs</depend> ... </package> Those lines inform ROS to make the targeted resources available at build and execution time. For information about package.xml you can refer to the official specifications on www.ros.org . The CMakeList.txt The CMakeList.txt provide instructions on how to build libraries and programs. Generally, the file starts with a project name and the importation of dependencies. The dependencies must be installed, reachable on the machine and with the appropriate version number. Then its define how to build new resources (typically, libraries and programs/executable). And finally the element to install and where. For instance, all launch file included in the launch s directory can be installed with the cmake command: # Install launch files. install(DIRECTORY launch DESTINATION share/${PROJECT_NAME}/ ) Most of the primitive ( find_package , add_executable , install ) and macros ( PROJECT_NAME , REQUIRED , ... ) are CMake primitives and macros. The ament tools provides some of primitive dedicated to ROS build automation . Python scripts with Cmake Scripts are short executable code file, defining a ROS Node (in our context). The simplest way to include Python-based ROS node depending on a specific Python Package is to use install instruction in CMakeLists.txt . Considering that your work is in a scripts directory and knowing that the ROS destination to make the resources available for ros2 run is lib (...), the install instructions would look-like: # Python scripts install( PROGRAMS scripts/myNode DESTINATION lib/${PROJECT_NAME}) Naturally, it supposes that you have myNode script file aside. A minimal myNode can look-like : #!/usr/bin/python3 import rclpy # core ROS2 client python library from rclpy.node import Node # To manipulate ROS Nodes def main(): rclpy.init() # Initialize ROS2 client myNode= Node('blanc_node') # Create a Node, with a name # Start the ros infinite loop with myNode. rclpy.spin( myNode ) # At the end, destroy the node explicitly. myNode.destroy_node() # and shut the light down. rclpy.shutdown() print(\"tuto_move :: STOP.\") # If the file is executed as a script (ie. not imported). if __name__ == '__main__': # call main() function main() ATTENTION , The #!/usr/bin/python3 in the first line is mandatory. It indicate the interpreter processing the script. In fact, you have a executable program named python3 in the location /usr/bin . Run your node: Build your packages, from your workspace directory, and update your shell environment with source . colcon build source install/setup.bash Normally, at this point a new package tuto_kickoff with myNode is reachable by ros2 . ros2 run tuto_kickoff myNode The executed myNode script is the one in install/tuto_kickoff/lib . Any modification in your script files would require a new colcon build for propagation in ros ecosystem. Exercice: Add the node developed into the tutorial \" move the robot \" into your package tuto_kickoff as a move_tbot s node. At the end, you will be capable of starting it with the command: ros2 run tuto_kickoff move_tbot . Add a launch file starting a gazebo simulation with the move_tbot node. At the end, you will be capable of launching gazebo, a robot, your control node with the command: ros2 launch tuto_kickoff sim_move_launch.yaml .","title":"03 - Create a ROS compatible package"},{"location":"tuto-level-up/package/#03-create-a-ros-compatible-package","text":"This tutorial is based on the official documentation: docs.ros.org , with some tuning considering our preferences... Typically we prefer cmake method, whatever the targeted language ( C++ or python ).","title":"03 - Create a ROS compatible package"},{"location":"tuto-level-up/package/#your-first-package-in-ros2","text":"You can build your first package using the ros tool, with the build-type ament_cmake , then compile and install the package. Typically a tuto_kickoff package: ros2 pkg create --build-type ament_cmake tuto_kickoff colcon build --packages-select tuto_kickoff Package creation generate files identifying tuto_kickoff directory as a ROS package source. The file package.xml setup the ROS configuration, the name of the packages and its dependencies in ROS ecosystem. The file CMakeLists.txt is required to use CMake tool, a very famous cross-platform tool for automatizing building processes. The colcon build command produces the build and the install directories with respectively, temporary files generated by the build process and the resulting built executable and resources (yes, none at this point...). To inform your terminal that some ROS resources are available here, you have to source the ROS setup file. ls source ./install/setup.bash","title":"Your first package in ROS2"},{"location":"tuto-level-up/package/#the-packagexml-file","text":"As we said, package.xml is the entrance point of the package for ROS tools. It is a text file in eXtensible Markup Language format. Typically, it regroups all the dependencies, for instance rclcpp and rclpy the ROS client libraries for Cpp and Python or std_msgs . std_msgs and geometry_msgs are a ROS package defining the format of the simple and common messages allowing the communication between ROS processes (standard and geometry messages for our use). In the package.xml file : <package format=\"3\"> ... <depend>rclcpp</depend> <depend>rclpy</depend> <depend>std_msgs</depend> <depend>geometry_msgs</depend> ... </package> Those lines inform ROS to make the targeted resources available at build and execution time. For information about package.xml you can refer to the official specifications on www.ros.org .","title":"The package.xml file"},{"location":"tuto-level-up/package/#the-cmakelisttxt","text":"The CMakeList.txt provide instructions on how to build libraries and programs. Generally, the file starts with a project name and the importation of dependencies. The dependencies must be installed, reachable on the machine and with the appropriate version number. Then its define how to build new resources (typically, libraries and programs/executable). And finally the element to install and where. For instance, all launch file included in the launch s directory can be installed with the cmake command: # Install launch files. install(DIRECTORY launch DESTINATION share/${PROJECT_NAME}/ ) Most of the primitive ( find_package , add_executable , install ) and macros ( PROJECT_NAME , REQUIRED , ... ) are CMake primitives and macros. The ament tools provides some of primitive dedicated to ROS build automation .","title":"The CMakeList.txt"},{"location":"tuto-level-up/package/#python-scripts-with-cmake","text":"Scripts are short executable code file, defining a ROS Node (in our context). The simplest way to include Python-based ROS node depending on a specific Python Package is to use install instruction in CMakeLists.txt . Considering that your work is in a scripts directory and knowing that the ROS destination to make the resources available for ros2 run is lib (...), the install instructions would look-like: # Python scripts install( PROGRAMS scripts/myNode DESTINATION lib/${PROJECT_NAME}) Naturally, it supposes that you have myNode script file aside. A minimal myNode can look-like : #!/usr/bin/python3 import rclpy # core ROS2 client python library from rclpy.node import Node # To manipulate ROS Nodes def main(): rclpy.init() # Initialize ROS2 client myNode= Node('blanc_node') # Create a Node, with a name # Start the ros infinite loop with myNode. rclpy.spin( myNode ) # At the end, destroy the node explicitly. myNode.destroy_node() # and shut the light down. rclpy.shutdown() print(\"tuto_move :: STOP.\") # If the file is executed as a script (ie. not imported). if __name__ == '__main__': # call main() function main() ATTENTION , The #!/usr/bin/python3 in the first line is mandatory. It indicate the interpreter processing the script. In fact, you have a executable program named python3 in the location /usr/bin .","title":"Python scripts with Cmake"},{"location":"tuto-level-up/package/#run-your-node","text":"Build your packages, from your workspace directory, and update your shell environment with source . colcon build source install/setup.bash Normally, at this point a new package tuto_kickoff with myNode is reachable by ros2 . ros2 run tuto_kickoff myNode The executed myNode script is the one in install/tuto_kickoff/lib . Any modification in your script files would require a new colcon build for propagation in ros ecosystem.","title":"Run your node:"},{"location":"tuto-level-up/package/#exercice","text":"Add the node developed into the tutorial \" move the robot \" into your package tuto_kickoff as a move_tbot s node. At the end, you will be capable of starting it with the command: ros2 run tuto_kickoff move_tbot . Add a launch file starting a gazebo simulation with the move_tbot node. At the end, you will be capable of launching gazebo, a robot, your control node with the command: ros2 launch tuto_kickoff sim_move_launch.yaml .","title":"Exercice:"},{"location":"tuto-level-up/parameters/","text":"Handle ROS parameters Get parameters in node code and use them for node configuration. Manadge parameters on launch files.","title":"Parameters"},{"location":"tuto-level-up/parameters/#handle-ros-parameters","text":"Get parameters in node code and use them for node configuration. Manadge parameters on launch files.","title":"Handle ROS parameters"},{"location":"tuto-level-up/range-sensor/","text":"Range sensor Range sensors are robot sensor permitting to detect obstacles and determine a distance to it. Basic range sensors (infrared, ultrasonic, laser) produce a unique measure considering a given direction at a time. By making the sensor rotating, it is possible to get measurements on a plan around the sensor. Hokuhyo, equipping the Tbot , is typically a kind of rotating lidar sensor ( l aser i maging or li ght d etection a nd r anging). The goal here is to integrate an almost 360 obstacle detection to generate safe robot movement. More complex lidar permits 3D measurements (i.e. in several plans at a time). Get Scan Data Well, let\u2019s visualize the laser scan in rviz2 . For that, verify that the user has the right to read data on the device. By connecting the laser sensor, a access file appears in Linux /dev directory named ttyACM0 . Verify the owner of the file: ls -l /dev | egrep ttyACM Normally /dev/ttyACM0 is owned by user root and group dialout with crw-rw---- right, mining that owner and all members of the group can read ( r ) and write ( w ) and all the other users have no access to the resource. Verify that bot is a member of the group dialout cat /etc/group | egrep dialout Cool. Let run a driver to convert device I/O to ros messages. ROS driver for hokuyo laser range is embedded in the urg_node package ros2 run urg_node urg_node_driver --ros-args -p serial_port:=/dev/ttyACM0 Specifying the serial_port file requires to activate arguments with --ros-args and pass a file path using -p parameter command line flag. All of this is specific to ROS parameters . From that point data are streamed in /scan topic. It is possible to check it with ros2 topic list and ros2 topic echo scan . Now you can visualize it on rviz2 program. Start rviz2 in a terminal, add a flux laserScan and configure it in /scan topic. Nothing appears and it is normal. Rviz2 global option is configured on map frame, and nothing permits to set the position of the laser sensor in the map. The laser-scan frame is named laser . Change this information into global options and set the laser-scan size to 0,1 for a better display. Stop everything. Perform the same exercise to visualize simulated LaserScan from Gazebo simulator: ros2 launch tbot_sim challenge-1.launch.py Warning: the scan topic and/or the laser-scan frame can have different names. A first node logging the scan First, we will initialize a node scan_echo . Edit a new file scan_echo.py with a very simple code : def main(): print('Move move move !') if __name__ == '_main__' : main() Test your scan_echo node: python3 scan_echo.py In a first version, you should: Initialize the rosclient (rclpy) Connect sensor_msgs LaserScan . Log continuously the received data #!python3 import rclpy from rclpy.node import Node from sensor_msgs.msg import LaserScan rosNode= None def scan_callback(scanMsg): global rosNode rosNode.get_logger().info( f\"scan:\\n{scanMsg}\" ) rclpy.init() rosNode= Node('scan_interpreter') rosNode.create_subscription( LaserScan, 'scan', scan_callback, 10) while True : rclpy.spin_once( rosNode ) scanInterpret.destroy_node() rclpy.shutdown() Test your scan_echo.py node, with the hokuyo laser and on simulation. Modify the logger to print only the information into the header and the number of ranges. From LaserScan to PointCloud LaserScan provides both the recorded bean distances (ranges) and the meta information permitting converting the distances on points in a regular Cartesian frame (i.e. the angle between beans). In a python, the conversion would look like this: obstacles= [] angle= scanMsg.angle_min for aDistance in scanMsg.ranges : if 0.1 < aDistance and aDistance < 5.0 : aPoint= [ math.cos(angle) * aDistance, math.sin(angle) * aDistance ] obstacles.append(aPoint) angle+= scanMsg.angle_increment The exercise consists in modifying the scan callback function to generate the point cloud list. To log a sample of the point cloud: sample= [ [ round(p[0], 2), round(p[1], 2) ] for p in obstacles[10:20] ] self.get_logger().info( f\" obs({len(obstacles)}) ...{sample}...\" ) Finally, it is possible to publish this result in a PointCloud message and to visualize it on rviz2 in a superposition of the LaserScan. PointCloud is based on geometry_msgs.Point32 with float coordinate. The creation of Point32 will require explicite cast. aPoint= Point32() aPoint.x= (float)(math.cos(angle) * aDistance) aPoint.y= (float)(math.sin( angle ) * aDistance) aPoint.z= (float)(0)","title":"Range Sensor"},{"location":"tuto-level-up/range-sensor/#range-sensor","text":"Range sensors are robot sensor permitting to detect obstacles and determine a distance to it. Basic range sensors (infrared, ultrasonic, laser) produce a unique measure considering a given direction at a time. By making the sensor rotating, it is possible to get measurements on a plan around the sensor. Hokuhyo, equipping the Tbot , is typically a kind of rotating lidar sensor ( l aser i maging or li ght d etection a nd r anging). The goal here is to integrate an almost 360 obstacle detection to generate safe robot movement. More complex lidar permits 3D measurements (i.e. in several plans at a time).","title":"Range sensor"},{"location":"tuto-level-up/range-sensor/#get-scan-data","text":"Well, let\u2019s visualize the laser scan in rviz2 . For that, verify that the user has the right to read data on the device. By connecting the laser sensor, a access file appears in Linux /dev directory named ttyACM0 . Verify the owner of the file: ls -l /dev | egrep ttyACM Normally /dev/ttyACM0 is owned by user root and group dialout with crw-rw---- right, mining that owner and all members of the group can read ( r ) and write ( w ) and all the other users have no access to the resource. Verify that bot is a member of the group dialout cat /etc/group | egrep dialout Cool. Let run a driver to convert device I/O to ros messages. ROS driver for hokuyo laser range is embedded in the urg_node package ros2 run urg_node urg_node_driver --ros-args -p serial_port:=/dev/ttyACM0 Specifying the serial_port file requires to activate arguments with --ros-args and pass a file path using -p parameter command line flag. All of this is specific to ROS parameters . From that point data are streamed in /scan topic. It is possible to check it with ros2 topic list and ros2 topic echo scan . Now you can visualize it on rviz2 program. Start rviz2 in a terminal, add a flux laserScan and configure it in /scan topic. Nothing appears and it is normal. Rviz2 global option is configured on map frame, and nothing permits to set the position of the laser sensor in the map. The laser-scan frame is named laser . Change this information into global options and set the laser-scan size to 0,1 for a better display. Stop everything. Perform the same exercise to visualize simulated LaserScan from Gazebo simulator: ros2 launch tbot_sim challenge-1.launch.py Warning: the scan topic and/or the laser-scan frame can have different names.","title":"Get Scan Data"},{"location":"tuto-level-up/range-sensor/#a-first-node-logging-the-scan","text":"First, we will initialize a node scan_echo . Edit a new file scan_echo.py with a very simple code : def main(): print('Move move move !') if __name__ == '_main__' : main() Test your scan_echo node: python3 scan_echo.py In a first version, you should: Initialize the rosclient (rclpy) Connect sensor_msgs LaserScan . Log continuously the received data #!python3 import rclpy from rclpy.node import Node from sensor_msgs.msg import LaserScan rosNode= None def scan_callback(scanMsg): global rosNode rosNode.get_logger().info( f\"scan:\\n{scanMsg}\" ) rclpy.init() rosNode= Node('scan_interpreter') rosNode.create_subscription( LaserScan, 'scan', scan_callback, 10) while True : rclpy.spin_once( rosNode ) scanInterpret.destroy_node() rclpy.shutdown() Test your scan_echo.py node, with the hokuyo laser and on simulation. Modify the logger to print only the information into the header and the number of ranges.","title":"A first node logging the scan"},{"location":"tuto-level-up/range-sensor/#from-laserscan-to-pointcloud","text":"LaserScan provides both the recorded bean distances (ranges) and the meta information permitting converting the distances on points in a regular Cartesian frame (i.e. the angle between beans). In a python, the conversion would look like this: obstacles= [] angle= scanMsg.angle_min for aDistance in scanMsg.ranges : if 0.1 < aDistance and aDistance < 5.0 : aPoint= [ math.cos(angle) * aDistance, math.sin(angle) * aDistance ] obstacles.append(aPoint) angle+= scanMsg.angle_increment The exercise consists in modifying the scan callback function to generate the point cloud list. To log a sample of the point cloud: sample= [ [ round(p[0], 2), round(p[1], 2) ] for p in obstacles[10:20] ] self.get_logger().info( f\" obs({len(obstacles)}) ...{sample}...\" ) Finally, it is possible to publish this result in a PointCloud message and to visualize it on rviz2 in a superposition of the LaserScan. PointCloud is based on geometry_msgs.Point32 with float coordinate. The creation of Point32 will require explicite cast. aPoint= Point32() aPoint.x= (float)(math.cos(angle) * aDistance) aPoint.y= (float)(math.sin( angle ) * aDistance) aPoint.z= (float)(0)","title":"From LaserScan to PointCloud"},{"location":"tuto-level-up/transforms/","text":"","title":"Transforms"},{"location":"tuto-mastering/slam/","text":"What is SLAM in a Nutshell? Mobile robots rely heavily on accurate representations of the environment (i.e maps ) to fulfill their tasks (autonomous navigation, exploration, ...). Inside buildings, GPS signals are too weak to be used to localize robots. Hence we face a so-called Chicken-and-Egg-Problem, as localization requires a map, and map building (i.e. mapping ) requires the current location. One solution consists in doing Simultaneous Localization and Mapping (a.k.a. SLAM) using a SLAM algorithm that typically reaches centimetric precision. There are many different flavors of SLAM especially regarding the map format. The dominating 2D map format is the occupancy grid, also called grid map. A grid map is a matrix whose cells represents a defined region of the real world; this is the resolution of the grid map (typically a square of 5cm). A cell holds the estimated probability that the space it represents is traversable (free space) or not (obstacle). The simplest format is the 3-state occupancy grid in which a cell has 3 different possible values: 0 (free space), 0.5 (unknown) and 1 (obstacle). Transformations doc Map building using slam_toolbox There are a lot of different SLAM algorithms and some implementations are open source and available on OpenSLAM . We will use here the slam_toolbox ROS implementation (documentation is here ). Launch slam_toolbox ros2 launch tbot_sim challenge-1.launch ros2 launch slam_toolbox online_sync_launch.py use_sim_time:=False rviz2 Question: using all the tools you already know ( rviz2 , rqt_graph , tf, ...), what are the input and output data of slam_toolbox ? Manual Mapping Launch a teleop: # keyboard ros2 run teleop_twist_keyboard teleop_twist_keyboard # or xbox ros2 launch teleop_twist_joy teleop-launch.py joy_config:=xbox Now, while moving the robot around the simulated environment, you should see the result of slam_toolbox (both the map and robot pose) updated in rviz2 . Autonomous Navigation sending goal points Facultative part ros2 launch nav2_bringup navigation_launch.py Be carreful, navigation should publish in the right topic so that the robot receive command velocities. Then, send goal points into /goal_pose (use rviz2) Save the Map ros2 run nav2_map_server map_saver_cli -f /home/bot/map Sometimes this command produces a timeout. This is because it listens to the map topic no map is received during a certain amount of time and we cannot extend this delay... Another solution to save the map is to use the following service call: ros2 service call /slam_toolbox/save_map slam_toolbox/srv/SaveMap \"name: {data: '/home/bot/map2'}\"","title":"S.L.A.M"},{"location":"tuto-mastering/slam/#what-is-slam-in-a-nutshell","text":"Mobile robots rely heavily on accurate representations of the environment (i.e maps ) to fulfill their tasks (autonomous navigation, exploration, ...). Inside buildings, GPS signals are too weak to be used to localize robots. Hence we face a so-called Chicken-and-Egg-Problem, as localization requires a map, and map building (i.e. mapping ) requires the current location. One solution consists in doing Simultaneous Localization and Mapping (a.k.a. SLAM) using a SLAM algorithm that typically reaches centimetric precision. There are many different flavors of SLAM especially regarding the map format. The dominating 2D map format is the occupancy grid, also called grid map. A grid map is a matrix whose cells represents a defined region of the real world; this is the resolution of the grid map (typically a square of 5cm). A cell holds the estimated probability that the space it represents is traversable (free space) or not (obstacle). The simplest format is the 3-state occupancy grid in which a cell has 3 different possible values: 0 (free space), 0.5 (unknown) and 1 (obstacle). Transformations doc","title":"What is SLAM in a Nutshell?"},{"location":"tuto-mastering/slam/#map-building-using-slam_toolbox","text":"There are a lot of different SLAM algorithms and some implementations are open source and available on OpenSLAM . We will use here the slam_toolbox ROS implementation (documentation is here ).","title":"Map building using slam_toolbox"},{"location":"tuto-mastering/slam/#launch-slam_toolbox","text":"ros2 launch tbot_sim challenge-1.launch ros2 launch slam_toolbox online_sync_launch.py use_sim_time:=False rviz2 Question: using all the tools you already know ( rviz2 , rqt_graph , tf, ...), what are the input and output data of slam_toolbox ?","title":"Launch slam_toolbox"},{"location":"tuto-mastering/slam/#manual-mapping","text":"Launch a teleop: # keyboard ros2 run teleop_twist_keyboard teleop_twist_keyboard # or xbox ros2 launch teleop_twist_joy teleop-launch.py joy_config:=xbox Now, while moving the robot around the simulated environment, you should see the result of slam_toolbox (both the map and robot pose) updated in rviz2 .","title":"Manual Mapping"},{"location":"tuto-mastering/slam/#autonomous-navigation-sending-goal-points","text":"Facultative part ros2 launch nav2_bringup navigation_launch.py Be carreful, navigation should publish in the right topic so that the robot receive command velocities. Then, send goal points into /goal_pose (use rviz2)","title":"Autonomous Navigation sending goal points"},{"location":"tuto-mastering/slam/#save-the-map","text":"ros2 run nav2_map_server map_saver_cli -f /home/bot/map Sometimes this command produces a timeout. This is because it listens to the map topic no map is received during a certain amount of time and we cannot extend this delay... Another solution to save the map is to use the following service call: ros2 service call /slam_toolbox/save_map slam_toolbox/srv/SaveMap \"name: {data: '/home/bot/map2'}\"","title":"Save the Map"},{"location":"tuto-mastering/transform/","text":"Move To The idea here is to develop a move strategy to permits a robot to reach positions successively, in a cluttered environment. To do that, the node subscribes to a topic goals to get in a position to reach. The main difficulty here, consists in following positioning kwoledge of the goals while the robot is moving. Record a goal position It supposes that you play with at least 2 frames. A local frame is attached to the robot and is moving with it. A global frame permits to localize the goal at a fixed position in the environement and the robot (i.e. the local frame). It supposes that your global frame is fixed in the environment. Classically, we use the map frame for global referring system, but without map it is possible to use the odom (from robot odometer). The robot is defined with different frame: base_link at the gravity center of the robot. base_footprint as a projection of base_link on the floor. Understand frame and transformations By starting any sensor as the laser, the publishing data is in its own frame. It would be impossible for rviz2 to display the laser information into map frame ( fixed frame ). The map and laser frames are independent. Start the laser and rviz2: # First console ros2 run urg_node urg_node_driver --ros-args -p serial_port:=/dev/ttyACM0 # Second console rviz2 In rviz, connect to scan topic, but nothing appears. Try by modifying the global frame with the frame of the laser. Transform tools The package tf2_tools provides with a process that generates a graph of the connection between the frames. #third console ros2 run tf2_tools view_frames.py evince frames.pdf In ROS tf stand for transformation. It is a central tool permitting getting space-temporal information from a frame to another. It supposes that specific messages are published into dedicated topic tf . At this step, no transform are published... It is possible to generate a static transformation (it supposes that the laser is fixed in the environment at a specific position) ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 0 1 \"map\" \"laser\" You can validate with view_frame that the 2 frames ares connected and that laser scan are displayed in rviz . The first 3 numbers fix the translation. It is the potion of the laser center into map . The next 4 numbers give the rotation. In fact, the publisher generates a TransfromStamped mesage and the rotation is based on quaternion definition (cf. wikipedia for details...) Display the frames in rviz2 (add > axes > set reference frame) and play with different configurations (kill and restart the static_transform_publisher ). tbot configuration For a simple robot, it can be dozens of frames and it grows with robot parts (legs, arms). ROS provide a tool (state publisher) to publish transform regarding how the frames are interconnected. The tbot launch file of the tbot_start package already starts state publisher based on a description of the tbot (kobuki robot in IMT Nord Europe configuration). Spot every thing but rviz and start the tbot launch file: tbot_start minimal.launch.py . Generate the frame graph ( view_frame ). In basic configuration, the robot provides a first pose estimation in global odom frame (ie. transformation between odom and base_link ). So set the fixed frame in rviz on odom , the laser scans appear. Connect to tf topic and all the frame axis appear too. Oficial documentation about tf2: docs.ros.org . Bonus: it is possible to visualize the robot in rviz2 : add > robot description (select the appropriate topic). Publish a pose in a specific frame. Naturally, ROS also provide C++ and Python library to manipulate transformation and permits developers to get pose coordinate from a frame to another. The idea is to a declare a tf2 listener, an object that will subscribe to transform topics and maintain transformation data. Then it is possible to recompute pose coordinates in any connected frames. More on : wiki.ros.org . The idea in our context is to develop a node localGoal that will remember a pose in a global frame and publish at a given frequence the pose in another local frame. For our new node, we have to declare the elements permitting the node to listen and keep the transformation available, a listerner and a buffer . # Transform tool: self.tf_buffer = tf2_ros.Buffer() self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self) ... Do not forget to import tf2_ros into your script and to add the reference into the package dependencies ( package.xml ). The node is also defined with it onw attrituts: for instance a target local frame, a goal pose (position orientation). ... # Node Attribute: self.local_frame= 'base_link' self.global_goal= Pose() self.global_goal.position.x= (float)(1) self.global_goal.position.y= (float)(2) ... And finally we require a timer with a callback function to publish continuously the goal pose. ... node.create_timer(0.1, self.publish_goal) ... We can now address the interesting question: How to transform a position defined into a frame in another frame ? It consists in building a Transform object from the reference and target frames. While a listener was declared on a transform buffer, it is possible to create this object from that tool (if exist). The Transform object is generated with the method lookup_transform(target_frame, reference_frame, time) . This method gets a target_frame (the frame id in which we want the pose) a reference_frame (the frame id in which the pose is currently defined) and a time. In fact, the transformations are dynamically. The position and orientation of elements (the robot(s), robot parts, ...) change continuously and it is possible to get transformation in the present or in the past. To get the current transformation a node.time.Time() permits to get the current time. The lookup is not guaranteed to achieve. It can fail in case of a gap in the transforms or obsolete transforms. In case of fail, an exception is thrown accordingly the python exception manager (more on w3schools ). Finally, inside our publish_goal call back, getting a transform will look like: def publish_goal(self): currentTime= rclpy.time.Time() # Get Transformation try: stampedTransform= self.tf_buffer.lookup_transform( self.local_frame, 'odom', currentTime) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):TransformException as tex: self._node.get_logger().info( f'Could not transform the goal into {self.local_frame}: {tex}') return ... The transform is a stamped transform (ie. defined in a given time) defined by geometry_msgs package. The pose transformation is already defined in a ros method of tf2_geometry_msgs package and it require the installation of python3-tf2-geometry-msgs (things are never simple in ros...): sudo apt update sudo apt install python3-tf2-geometry-msgs def publish_goal(self): ... # Compute goal into local coordinates localGoal = tf2_geometry_msgs.do_transform_pose( self.global_goal, stampedTransform ) ... Permit Autonomous Navigation The goal poses itself is not interesting. The objective now is to include this code into the reactive move node in order to permits the robot to reach a decided destination, by avoiding obstacles. Get the goal pose from a topic pose (a pose can be published with rviz). Convert the received goal pose into a global frame ( odom ). Control the robot to reach the goal (the control is in the local frame) Stop the robot if it is close enough to the position. Going Further - Path following Rather than a unique pose, it could be interesting to define a succession of pose to follow (a path). That for the reactive move has to manage a list of pose and switch from a pose to the next one each time it is expected.","title":"Move To"},{"location":"tuto-mastering/transform/#move-to","text":"The idea here is to develop a move strategy to permits a robot to reach positions successively, in a cluttered environment. To do that, the node subscribes to a topic goals to get in a position to reach. The main difficulty here, consists in following positioning kwoledge of the goals while the robot is moving.","title":"Move To"},{"location":"tuto-mastering/transform/#record-a-goal-position","text":"It supposes that you play with at least 2 frames. A local frame is attached to the robot and is moving with it. A global frame permits to localize the goal at a fixed position in the environement and the robot (i.e. the local frame). It supposes that your global frame is fixed in the environment. Classically, we use the map frame for global referring system, but without map it is possible to use the odom (from robot odometer). The robot is defined with different frame: base_link at the gravity center of the robot. base_footprint as a projection of base_link on the floor.","title":"Record a goal position"},{"location":"tuto-mastering/transform/#understand-frame-and-transformations","text":"By starting any sensor as the laser, the publishing data is in its own frame. It would be impossible for rviz2 to display the laser information into map frame ( fixed frame ). The map and laser frames are independent. Start the laser and rviz2: # First console ros2 run urg_node urg_node_driver --ros-args -p serial_port:=/dev/ttyACM0 # Second console rviz2 In rviz, connect to scan topic, but nothing appears. Try by modifying the global frame with the frame of the laser.","title":"Understand frame and transformations"},{"location":"tuto-mastering/transform/#transform-tools","text":"The package tf2_tools provides with a process that generates a graph of the connection between the frames. #third console ros2 run tf2_tools view_frames.py evince frames.pdf In ROS tf stand for transformation. It is a central tool permitting getting space-temporal information from a frame to another. It supposes that specific messages are published into dedicated topic tf . At this step, no transform are published... It is possible to generate a static transformation (it supposes that the laser is fixed in the environment at a specific position) ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 0 1 \"map\" \"laser\" You can validate with view_frame that the 2 frames ares connected and that laser scan are displayed in rviz . The first 3 numbers fix the translation. It is the potion of the laser center into map . The next 4 numbers give the rotation. In fact, the publisher generates a TransfromStamped mesage and the rotation is based on quaternion definition (cf. wikipedia for details...) Display the frames in rviz2 (add > axes > set reference frame) and play with different configurations (kill and restart the static_transform_publisher ).","title":"Transform tools"},{"location":"tuto-mastering/transform/#tbot-configuration","text":"For a simple robot, it can be dozens of frames and it grows with robot parts (legs, arms). ROS provide a tool (state publisher) to publish transform regarding how the frames are interconnected. The tbot launch file of the tbot_start package already starts state publisher based on a description of the tbot (kobuki robot in IMT Nord Europe configuration). Spot every thing but rviz and start the tbot launch file: tbot_start minimal.launch.py . Generate the frame graph ( view_frame ). In basic configuration, the robot provides a first pose estimation in global odom frame (ie. transformation between odom and base_link ). So set the fixed frame in rviz on odom , the laser scans appear. Connect to tf topic and all the frame axis appear too. Oficial documentation about tf2: docs.ros.org . Bonus: it is possible to visualize the robot in rviz2 : add > robot description (select the appropriate topic).","title":"tbot configuration"},{"location":"tuto-mastering/transform/#publish-a-pose-in-a-specific-frame","text":"Naturally, ROS also provide C++ and Python library to manipulate transformation and permits developers to get pose coordinate from a frame to another. The idea is to a declare a tf2 listener, an object that will subscribe to transform topics and maintain transformation data. Then it is possible to recompute pose coordinates in any connected frames. More on : wiki.ros.org . The idea in our context is to develop a node localGoal that will remember a pose in a global frame and publish at a given frequence the pose in another local frame. For our new node, we have to declare the elements permitting the node to listen and keep the transformation available, a listerner and a buffer . # Transform tool: self.tf_buffer = tf2_ros.Buffer() self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self) ... Do not forget to import tf2_ros into your script and to add the reference into the package dependencies ( package.xml ). The node is also defined with it onw attrituts: for instance a target local frame, a goal pose (position orientation). ... # Node Attribute: self.local_frame= 'base_link' self.global_goal= Pose() self.global_goal.position.x= (float)(1) self.global_goal.position.y= (float)(2) ... And finally we require a timer with a callback function to publish continuously the goal pose. ... node.create_timer(0.1, self.publish_goal) ... We can now address the interesting question: How to transform a position defined into a frame in another frame ? It consists in building a Transform object from the reference and target frames. While a listener was declared on a transform buffer, it is possible to create this object from that tool (if exist). The Transform object is generated with the method lookup_transform(target_frame, reference_frame, time) . This method gets a target_frame (the frame id in which we want the pose) a reference_frame (the frame id in which the pose is currently defined) and a time. In fact, the transformations are dynamically. The position and orientation of elements (the robot(s), robot parts, ...) change continuously and it is possible to get transformation in the present or in the past. To get the current transformation a node.time.Time() permits to get the current time. The lookup is not guaranteed to achieve. It can fail in case of a gap in the transforms or obsolete transforms. In case of fail, an exception is thrown accordingly the python exception manager (more on w3schools ). Finally, inside our publish_goal call back, getting a transform will look like: def publish_goal(self): currentTime= rclpy.time.Time() # Get Transformation try: stampedTransform= self.tf_buffer.lookup_transform( self.local_frame, 'odom', currentTime) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):TransformException as tex: self._node.get_logger().info( f'Could not transform the goal into {self.local_frame}: {tex}') return ... The transform is a stamped transform (ie. defined in a given time) defined by geometry_msgs package. The pose transformation is already defined in a ros method of tf2_geometry_msgs package and it require the installation of python3-tf2-geometry-msgs (things are never simple in ros...): sudo apt update sudo apt install python3-tf2-geometry-msgs def publish_goal(self): ... # Compute goal into local coordinates localGoal = tf2_geometry_msgs.do_transform_pose( self.global_goal, stampedTransform ) ...","title":"Publish a pose in a specific frame."},{"location":"tuto-mastering/transform/#permit-autonomous-navigation","text":"The goal poses itself is not interesting. The objective now is to include this code into the reactive move node in order to permits the robot to reach a decided destination, by avoiding obstacles. Get the goal pose from a topic pose (a pose can be published with rviz). Convert the received goal pose into a global frame ( odom ). Control the robot to reach the goal (the control is in the local frame) Stop the robot if it is close enough to the position.","title":"Permit Autonomous Navigation"},{"location":"tuto-mastering/transform/#going-further-path-following","text":"Rather than a unique pose, it could be interesting to define a succession of pose to follow (a path). That for the reactive move has to manage a list of pose and switch from a pose to the next one each time it is expected.","title":"Going Further - Path following"},{"location":"tuto-mastering/vision/","text":"Introduction to image processing. Vision provides a rich information about the immediate environment around the robot. However it requires to process the images to extract pertinent information... The goal here is to detect a specific object in the scene. The first proposed approach is to detect the object regarding its main color before to move toward approaches handle textures. Setup our programming environment This tutorial focus on OpenCV librairy, addressed in Python development language on a Linux machine. First we want to install Python3 and the libraries: Numpy, Matplot, Sklearn, Scipy and OpenCV. The command whereis permit to localize a command (like python3 interpreter). If python3 is correctly installed, its execution would not be empty: whereis python3 Python uses its own package managers pip to install libraries. So just to be sure you can: sudo apt install python3 pip Then we can use pip to install modules: pip3 install numpy tensorflow opencv-python opencv-contrib-python scikit-learn scipy matplotlib psutil scikit-image Now normally you can load the different modules, for instance: import cv2 import numpy as np import os from sklearn.svm import LinearSVC from scipy.cluster.vq import * from sklearn.preprocessing import StandardScaler from sklearn import preprocessing Image from simulation Gazebo is capable of simulating robot vision (classical and 3D sensor). Launch a simulation : '''bash roslaunch larm chalenge-2.launch ''' And observe the topic and mainly the one published the images ( camera/rgb/image_raw ) with rostopic list , rostopic info and rostopic hz . Image are published as 'sensor_msgs/Image' in camera/rgb/image_raw ROS doc So the pixel value is stored in img.data array but several tool to convert ROS image to OpenCV images already exist (for instance cv_bridge ) Segmentation d'images couleur par seuillage des composantes et Gestion de la souris Voici quelques lignes de codes pour extraire une r\u00e9gion d'int\u00e9r\u00eat \u00e0 la souris. Gr\u00e2ce \u00e0 ces quelques lignes il vous sera possible de calculer la valeur moyenne et la variance de chaque composante de l'image, utile pour proc\u00e9der ensuite \u00e0 une \u00e9tape de segmentation. Dans cet exemple, nous proc\u00e9dons tout d'abord \u00e0 l'acquisition d'une image de la webcam du portable. Puis, nous d\u00e9finissons un crop de l'image acquise gr\u00e2ce \u00e0 la souris. Il vous est alors facile de calculer les m\u00e9triques statistiques que vous souhaitez sur ce crop. La moyenne et la variance d\u00e9finissent un mod\u00e8le gaussien sur chaque composante du crop. Dans la suite, vous pourrez soit utiliser le flux d'images provenant de votre webcam ou celui provenant de la cam\u00e9ra Realsense (cf. tuto pr\u00e9c\u00e9dent) import cv2 import numpy as np # connect to a sensor (0: webcam) cap=cv2.VideoCapture(0) # capture an image ret, frame=cap.read() # Select ROI r = cv2.selectROI(frame) # Crop image imCrop = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])] average_h = np.mean(imCrop[:,:,0]) average_s = np.mean(imCrop[:,:,1]) average_v = np.mean(imCrop[:,:,2]) print(average_h,average_s,average_v) # Display cropped image cv2.imshow(\"Image\", imCrop) cv2.waitKey(0) Dans cet exemple, il s'agit de produire un masque des pixels dont les composantes HSV sont comprises entre les variables lo et hi. Dans cet exemple, en agissant sur le click gauche ou droit de la souris vous diminuez ou augmentez la teinte h des deux variables lo et hi. Par ailleurs, dans notre exemple, vous constaterez que lo et hi se diff\u00e9rentient non seulement par leur teinte mais \u00e9galement par leur saturation. Vous pourrez tester ce script sur une image de votre visage. Ces quelques lignes de codes illustrent \u00e9galement comment g\u00e9rer des actions sur la souris. Elles g\u00e8res les \u00e9v\u00e9nements souris tels que le mouvement de la souris (cv2.EVENT_MOUSEMOVE), le double click milieu (EVENT_MBUTTONDBLCLK), le click droit (EVENT_RBUTTONDOWN) et le click gauche (EVENT_LBUTTONDOWN). import cv2 import numpy as np def souris(event, x, y, flags, param): global lo, hi, color, hsv_px if event == cv2.EVENT_MOUSEMOVE: # Conversion des trois couleurs RGB sous la souris en HSV px = frame[y,x] px_array = np.uint8([[px]]) hsv_px = cv2.cvtColor(px_array,cv2.COLOR_BGR2HSV) if event==cv2.EVENT_MBUTTONDBLCLK: color=image[y, x][0] if event==cv2.EVENT_LBUTTONDOWN: if color>5: color-=1 if event==cv2.EVENT_RBUTTONDOWN: if color<250: color+=1 lo[0]=color-10 hi[0]=color+10 color=100 lo=np.array([color-5, 100, 50]) hi=np.array([color+5, 255,255]) color_info=(0, 0, 255) cap=cv2.VideoCapture(0) cv2.namedWindow('Camera') cv2.setMouseCallback('Camera', souris) hsv_px = [0,0,0] # Creating morphological kernel kernel = np.ones((3, 3), np.uint8) while True: ret, frame=cap.read() image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) mask=cv2.inRange(image, lo, hi) mask=cv2.erode(mask, kernel, iterations=1) mask=cv2.dilate(mask, kernel, iterations=1) image2=cv2.bitwise_and(frame, frame, mask= mask) cv2.putText(frame, \"Couleur: {:d}\".format(color), (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Affichage des composantes HSV sous la souris sur l'image pixel_hsv = \" \".join(str(values) for values in hsv_px) font = cv2.FONT_HERSHEY_SIMPLEX cv2.putText(frame, \"px HSV: \"+pixel_hsv, (10, 260), font, 1, (255, 255, 255), 1, cv2.LINE_AA) cv2.imshow('Camera', frame) cv2.imshow('image2', image2) cv2.imshow('Mask', mask) if cv2.waitKey(1)&0xFF==ord('q'): break cap.release() cv2.destroyAllWindows() G\u00e9n\u00e9ralement il est tr\u00e8s int\u00e9ressante de changer d'espace colorim\u00e9trique afin de mieux cibler l'espace dans lequel l'objet d'int\u00e9r\u00eat est discriminable : image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) Apr\u00e8s avoir produite le mask avec mask=cv2.inRange(image, lo, hi) il est parfois pertinant de d\u00e9bruiter l'image r\u00e9sultats en la flouttant ou par quelques op\u00e9rations motrphologiques. Cela permet de fermer et remplir les formes : # Flouttage de l'image image=cv2.blur(image, (7, 7)) # Erosion d'un mask mask=cv2.erode(mask, None, iterations=4) # dilatation d'un mask mask=cv2.dilate(mask, None, iterations=4) Dans le code de segmentation d'une image couleur fourni pr\u00e9c\u00e9demment, vous jouerez avec la taille du kernel (3x3 dans notre exemple), vous ajouterez une \u00e9tape de flouttage de chaque canal en jouant sur la taille du voisinage (7 x 7 dans notre exemple). Pour finir vous jouerez avec les \u00e9tapes d'\u00e9rosion et de dilatation en modifiant le nombre de fois o\u00f9 chaque op\u00e9rateur morphologique est appliqu\u00e9 (4 fois dans notre exemple). Le code de segmentation d'une image couleur fourni pr\u00e9c\u00e9demment permet de d\u00e9finir un mask binaire des pixels dont les composantes HSV sont l'intervalle [lo,hi]. Il est alors possible de d\u00e9tecter les \u00e9l\u00e9ments connexes dans le mask afin d'en extraire certaines informations telles qu'ici le minEnclosingCircle. D'autres features peuvent \u00eatre utiles. Vous les trouverez ici : https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html Sous l'hypoth\u00e8se qu'un objet d'inter\u00eat est repr\u00e9sent\u00e9 par un ensemble de pixels connexes dont la couleur est contenu dans l'intervalle [lo,hi], il est alors possible de d\u00e9finir des contraintes sur un ensemble de features qui permettent de classer les objets ainsi d\u00e9tect\u00e9s. Vous ajouterez les lignes suivantes dans le code de segmentation pr\u00e9c\u00e9dent. elements=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] if len(elements) > 0: c=max(elements, key=cv2.contourArea) ((x, y), rayon)=cv2.minEnclosingCircle(c) if rayon>30: cv2.circle(image2, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(frame, (int(x), int(y)), 5, color_info, 10) cv2.line(frame, (int(x), int(y)), (int(x)+150, int(y)), color_info, 2) cv2.putText(frame, \"Objet !!!\", (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) ``` Reste ensuite \u00e0 visualiser les images. ## Extraction de r\u00e9gions dans une image binaris\u00e9e Voici quelques lignes en python pour extraire des r\u00e9gion de pixels connexes dans une image binaris\u00e9e ``` label()```. De ces r\u00e9gions sont extraites quelques propri\u00e9t\u00e9s ``` regionprops()``` Ce code agit strat\u00e9giquement de la m\u00eame mani\u00e8re que le script de segmentation pr\u00e9c\u00e9dent mais en utilisant la librairie skimage. ```python import cv2 import numpy as np import matplotlib.pyplot as plt from skimage.measure import label, regionprops import math image = cv2.imread('./imageasegmenter.jpg') # passage en niveau de gris gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ###### extration des r\u00e9gions avec la lib skimage # Binarisation de l'image ret, thresh = cv2.threshold(gray, 127, 255, 1) cv2.imshow(\"image seuill\u00e9e\",thresh) cv2.waitKey(0) # extraction des r\u00e9gions et des propri\u00e9t\u00e9s des r\u00e9gions label_img = label(thresh) regions = regionprops(label_img) print(regions) cv2.waitKey(0) # affichage des r\u00e9gions et des boites englobantes fig, ax = plt.subplots() ax.imshow(thresh, cmap=plt.cm.gray) for props in regions: y0, x0 = props.centroid orientation = props.orientation x1 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length y1 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5) ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5) ax.plot(x0, y0, '.g', markersize=15) minr, minc, maxr, maxc = props.bbox bx = (minc, maxc, maxc, minc, minc) by = (minr, minr, maxr, maxr, minr) ax.plot(bx, by, '-b', linewidth=2.5) ax.axis((0, 600, 600, 0)) plt.show() cv2.waitKey(0) D\u00e9tection d'objets par template matching Il est possible de d\u00e9tecter un ou plusieurs objets dans une image en appliquant une proc\u00e9dure de matching d'un template de chaque objet \u00e0 d\u00e9tecter. Un template est une image du ou des objets en question. La fonction \u00e0 utiliser est cv.matchTemplate(img_gray,template,parametre) . Plusieurs parametre de matching sont possibles correspondant chacun \u00e0 une m\u00e9trique de corr\u00e9lation. Voici les lignes de codes que vous testerez. Vous testerez les parametres suivants afin de d\u00e9finir celui qui fournit les meilleurs r\u00e9sultats. Par ailleurs, vous adapterez le code afin de prendre un charge le flux des images de la Realsense et une image template de l'objet que vous voulez d\u00e9tect\u00e9. import cv2 as cv import numpy as np from matplotlib import pyplot as plt # charger l'image dans laquelle on cherche l'objet img_rgb = cv.imread('car.png') img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY) # charger le template de l'objet \u00e0 rechercher template = cv.imread('template.png',0) # R\u00e9cup\u00e9ration des dimensions de l'image w, h = template.shape[::-1] # Application du template atching res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED) # S\u00e9lection des meilleurs matched objects threshold = 0.8 loc = np.where( res >= threshold) # Affichage de la boite englobante de chaque objet d\u00e9tect\u00e9 for pt in zip(*loc[::-1]): cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2) Segmentation des images par la m\u00e9thodes des k-moyennes (kmeans) Kmeans est un algorithme de clustering, dont l'objectif est de partitionner n points de donn\u00e9es en k grappes. Chacun des n points de donn\u00e9es sera assign\u00e9 \u00e0 un cluster avec la moyenne la plus proche. La moyenne de chaque groupe s'appelle \u00abcentro\u00efde\u00bb ou \u00abcentre\u00bb. Globalement, l'application de k-means donne k grappes distinctes des n points de donn\u00e9es d'origine. Les points de donn\u00e9es \u00e0 l'int\u00e9rieur d'un cluster particulier sont consid\u00e9r\u00e9s comme \u00abplus similaires\u00bb les uns aux autres que les points de donn\u00e9es appartenant \u00e0 d'autres groupes. Cet algorithme peut \u00eatre appliquer sur des points d\u2019origine g\u00e9om\u00e9trique, colorim\u00e9triques et autres. Nous allons appliquer cette m\u00e9thode afin d'assurer une segmentation couleur d'une image i.e. cela revient \u00e0 trouver les couleur domainantes dans l'image. from sklearn.cluster import KMeans import matplotlib.pyplot as plt import cv2 import numpy as np #Ensuite charger une image et la convertir de BGR \u00e0 RGB si n\u00e9cessaire et l\u2019afficher : image = cv2.imread('lena.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.figure() plt.axis(\"off\") plt.imshow(image) Afin de traiter l\u2019image en tant que point de donn\u00e9es, il faut la convertir d\u2019une forme matricielle \u00e0 une forme vectorielle (liste de couleur rgb) avant d'appliquer la fonction de clustering : n_clusters=5 image = image.reshape((image.shape[0] * image.shape[1], 3)) clt = KMeans(n_clusters = n_clusters ) clt.fit(image) Pour afficher les couleurs les plus dominantes dans l'image, il faut d\u00e9finir deux fonctions : centroid_histogram() pour r\u00e9cup\u00e9rer le nombre de clusters diff\u00e9rents et cr\u00e9er un histogramme bas\u00e9 sur le nombre de pixels affect\u00e9s \u00e0 chaque cluster ; et plot_colors() pour initialiser le graphique \u00e0 barres repr\u00e9sentant la fr\u00e9quence relative de chacune des couleurs def centroid_histogram(clt): numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)>>> (hist, _) = np.histogram(clt.labels_, bins=numLabels) # normalize the histogram, such that it sums to one hist = hist.astype(\"float\") hist /= hist.sum() return hist def plot_colors(hist, centroids): bar = np.zeros((50, 300, 3), dtype=\"uint8\") startX = 0 # loop over the percentage of each cluster and the color of # each cluster for (percent, color) in zip(hist, centroids): # plot the relative percentage of each cluster endX = startX + (percent * 300) cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(\"uint8\").tolist(), -1) startX = endX return bar Il suffit maintenant de construire un histogramme de clusters puis cr\u00e9er une figure repr\u00e9sentant le nombre de pixels \u00e9tiquet\u00e9s pour chaque couleur. hist = centroid_histogram(clt) bar = plot_colors(hist, clt.cluster_centers_) plt.figure() plt.axis(\"off\") plt.imshow(bar) plt.show() Classification d'images par la mathode des K plus proches voisins (k-NN ou KNN) Cet exercice permettra d'apprendre un mod\u00e8le \u00e0 partir des images de la bases CIFAR-10 t\u00e9l\u00e9chargeable ici: http://www.cs.toronto.edu/~kriz/cifar.html D\u00e9compresser les fichier dans un dossier que vous utiliserez dans le script suivant. Ici, le dossier est ./data import numpy as np import cv2 basedir_data = \"./data/\" rel_path = basedir_data + \"cifar-10-batches-py/\" #D\u00e9s\u00e9rialiser les fichiers image afin de permettre l\u2019acc\u00e8s aux donn\u00e9es et aux labels: def unpickle(file): import pickle with open(file, 'rb') as fo: dict = pickle.load(fo,encoding='bytes') return dict X = unpickle(rel_path + 'data_batch_1') img_data = X[b'data'] img_label_orig = img_label = X[b'labels'] img_label = np.array(img_label).reshape(-1, 1) afin de v\u00e9rifier que tout s'est bien pass\u00e9 utilis\u00e9 : print(img_data) print('shape', img_data.shape) Vous devriez trouver un tableau numpy de 10000x3072 d'uint8s (le 3072 vient du 3 x 1024). Chaque ligne du tableau stocke une image couleur 32x32 en RGB. L'image est stock\u00e9e dans l'ordre des lignes principales, de sorte que les 32 premi\u00e8res entr\u00e9es du tableau correspondent aux valeurs des canaux rouges de la premi\u00e8re ligne de l'image. Pour v\u00e9rifier les labels : print(img_label) print('shape', img_label.shape) Nous avons les \u00e9tiquettes comme dane matrice 10000 x 1 Pour charger les donn\u00e9es de test, utiliser la m\u00eame proc\u00e9dure que pr\u00e9c\u00e9dement car la forme des donn\u00e9es de test est identique \u00e0 la forme des donn\u00e9es d\u2019apprentissage: test_X = unpickle(rel_path + 'test_batch'); test_data = test_X[b'data'] test_label = test_X[b'labels'] test_label = np.array(test_label).reshape(-1, 1) print(sample_img_data) print('shape', sample_img_data.shape) print('shape', sample_img_data[1,:].shape) Attention, les composantes RGB des images sont arrang\u00e9es sous la forme d'une vecteur \u00e0 1 dimension. Pour afficher chaque image, il faut donc remettre sous la forme d'une image 2D RGB. Pour cela, nous op\u00e9rons de la mani\u00e8re suivante en consid\u00e9rant que les images sont de r\u00e9solution 32x32 one_img=sample_img_data[0,:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) image = cv2.imread('./vector-handwritten-numbers-on-white-background-brusk-stroke.jpg') cv2.imshow('Image CIFAR',rgb) cv2.waitKey(0) cv2.destroyAllWindows() D\u00e9sormais, nous allons appliquer l'algorithmes des k-NN sur toutes les images de la base de training img_data et leurs labels img_label_orig from sklearn.neighbors import KNeighborsClassifier #def pred_label_fn(i, original): # return original + '::' + meta[YPred[i]].decode('utf-8') nbrs = KNeighborsClassifier(n_neighbors=3, algorithm='brute').fit(img_data, img_label_orig) # test sur les 10 premi\u00e8res images data_point_no = 10 sample_test_data = test_data[:data_point_no, :] YPred = nbrs.predict(sample_test_data) for i in range(0, len(YPred)): #show_im(sample_test_data, test_label, meta, i, label_fn=pred_label_fn) r = sample_test_data[i][:1024].reshape(32, 32) g = sample_test_data[i][1024:2048].reshape(32, 32) b = sample_test_data[i][2048:].reshape(32, 32) print(YPred[i]) cv2.imshow('image test',np.dstack([r, g, b])) neigh_dist,neigh_ind = nbrs.kneighbors([sample_test_data[i]]) print(neigh_ind) for j in range(0, len(neigh_ind[0])): one_img=img_data[neigh_ind[0][j],:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) cv2.imshow('K plus proche image',np.dstack([r, g, b])) cv2.waitKey(0) D\u00e9tection d'objets par ondelettes de Haar La d\u00e9tection d'objets \u00e0 l'aide de classificateurs en cascade bas\u00e9s sur la d\u00e9composition en ondelettes de Haar est une m\u00e9thode efficace de d\u00e9tection d'objets propos\u00e9e par Paul Viola et Michael Jones dans leur article, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" en 2001. Il s'agit d'une approche bas\u00e9e sur l'apprentissage automatique o\u00f9 un la fonction en cascade est form\u00e9e \u00e0 partir d'un grand nombre d'images positives et n\u00e9gatives. Cette m\u00e9thode a \u00e9t\u00e9 initialement mise en au point pour d\u00e9tecter des visages et a \u00e9t\u00e9 \u00e9tendu \u00e0 d'autres objets tels quels les voitures. En python, vous pouvez faire appel \u00e0 cette m\u00e9thode via object_cascade=cv2.CascadeClassifier() . Cette classe est instanci\u00e9e en lui passant un param\u00e8tre qui repr\u00e9sente le \"mod\u00e8le\" adapt\u00e9 \u00e0 l'objet \u00e0 d\u00e9tecter. Vous pouvez t\u00e9l\u00e9charger les mod\u00e8les relatifs \u00e0 des humains ici : https://github.com/opencv/opencv/tree/master/data/haarcascades Pour tester le d\u00e9tecteur sur des v\u00e9hicules, le mod\u00e8le propos\u00e9 par Andrews Sobral est t\u00e9l\u00e9chrgeable ici : https://github.com/andrewssobral/vehicle_detection_haarcascades/blob/master/cars.xml Pour appliquer le d\u00e9tecteur \u00e0 une image il suffit d'appeler la m\u00e9thode object=object_cascade.detectMultiScale(gray, scaleFactor=1.10, minNeighbors=3) en passant en param\u00e8tre le nom de la variable image (gray) qu'il faut pr\u00e9alablement transform\u00e9e en niveau de gris. Il fauat \u00e9galement renseigner le facteur d'\u00e9chelle (scaleFactor) utilis\u00e9 pour r\u00e9duire l'image \u00e0 chaque \u00e9tage et le nombre de voisins (minNeighbors) que chaque objet d\u00e9tect\u00e9 doit avoir pour le valider comme \"effectivement\" l'objet recherch\u00e9. Cette m\u00e9thode fournit une liste de boites englobantes (x, y, w et h) que vous afficherez sur chaque image couleur trait\u00e9e afin de visualiser les r\u00e9sultats de la d\u00e9tection. for x, y, w, h in object: cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) Ecrire un script permettant de mettre en musique cette classe et cette m\u00e9thode sur la vid\u00e9o cars.mp4 fournies. Vous validerez votre script en utilisant les mod\u00e8les relatifs au corps humains et en utilisant le flux d'une cam\u00e9ra. Model training Cette m\u00e9thode pourrait \u00eatre tr\u00e8s int\u00e9ressante pour d\u00e9tecter des objets lors du \"challenge\". Pour cela, je vous invite \u00e0 lire et utiliser ce qui est propos\u00e9 sur les 2 liens suivants. Ces liens d\u00e9crivent comment il est possible d'apprendre un mod\u00e8le sp\u00e9cifique \u00e0 un objet donn\u00e9. http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html https://github.com/mrnugget/opencv-haar-classifier-training Model training for an other feature Vous trouverez dans le lien suivant, l'apprentissage d'un mod\u00e8le sur la base d'un autre type de caract\u00e9reristique : les Local Binary Pattern (LBP). https://medium.com/@rithikachowta/object-detection-lbp-cascade-classifier-generation-a1d1a1c2d0b Gestion de la depth map et estimation de la distance Ce code permet de calculer la distance des parties de la sc\u00e8ne qui se projette en chaque pixel de la cam\u00e9ra. Il utilise les deux nouvelles parties de code suivantes. La premi\u00e8re permet de recaler les deux flux (depth map et rgb image) afin de garantir une correspondance pixel \u00e0 pixel. Ces deux parties de codes sont \u00e0 replac\u00e9es dans le code complet pr\u00e9sent\u00e9 ensuite. #Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() LA seconde permet de calculer la distance en un pixel (x,y) donn\u00e9. #Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) Le code complet integrant la partie acquisition et visualisation est la suivante. Un cercle est positionn\u00e9 dans les deux flux autour du pixel en lequel la distance est calcul\u00e9e. import pyrealsense2 as rs import numpy as np import math import cv2,time,sys pipeline = rs.pipeline() config = rs.config() colorizer = rs.colorizer() # fps plus bas (30) config.enable_stream(rs.stream.depth, 840, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 840, 480, rs.format.bgr8, 30) pipeline.start(config) align_to = rs.stream.depth align = rs.align(align_to) color_info=(0, 0, 255) rayon=10 count=1 refTime= time.process_time() freq= 60 try: while True: # This call waits until a new coherent set of frames is available on a device frames = pipeline.wait_for_frames() #Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() if not depth_frame or not aligned_color_frame: continue # Two ways to colorized the depth map # first : using colorizer of pyrealsense colorized_depth = colorizer.colorize(depth_frame) depth_colormap = np.asanyarray(colorized_depth.get_data()) # second : using opencv by applying colormap on depth image (image must be converted to 8-bit per pixel first) #depth_image = np.asanyarray(depth_frame.get_data()) #depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) # Get the intrinsic parameters color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics color_image = np.asanyarray(aligned_color_frame.get_data()) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape #Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) #print(\"Distance from camera to pixel:\", distance) #print(\"Z-depth from camera surface to pixel surface:\", depth) # Show images images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (640x480) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA) cv2.circle(images, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(images, (int(x+color_colormap_dim[1]), int(y)), int(rayon), color_info, 2) # Affichage distance au pixel (x,y) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x+color_colormap_dim[1])+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL) # Resize the Window cv2.resizeWindow('RealSense', 960, 720) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 except Exception as e: print(e) pass finally: pipeline.stop()","title":"Vision"},{"location":"tuto-mastering/vision/#introduction-to-image-processing","text":"Vision provides a rich information about the immediate environment around the robot. However it requires to process the images to extract pertinent information... The goal here is to detect a specific object in the scene. The first proposed approach is to detect the object regarding its main color before to move toward approaches handle textures.","title":"Introduction to image processing."},{"location":"tuto-mastering/vision/#setup-our-programming-environment","text":"This tutorial focus on OpenCV librairy, addressed in Python development language on a Linux machine. First we want to install Python3 and the libraries: Numpy, Matplot, Sklearn, Scipy and OpenCV. The command whereis permit to localize a command (like python3 interpreter). If python3 is correctly installed, its execution would not be empty: whereis python3 Python uses its own package managers pip to install libraries. So just to be sure you can: sudo apt install python3 pip Then we can use pip to install modules: pip3 install numpy tensorflow opencv-python opencv-contrib-python scikit-learn scipy matplotlib psutil scikit-image Now normally you can load the different modules, for instance: import cv2 import numpy as np import os from sklearn.svm import LinearSVC from scipy.cluster.vq import * from sklearn.preprocessing import StandardScaler from sklearn import preprocessing","title":"Setup our programming environment"},{"location":"tuto-mastering/vision/#image-from-simulation","text":"Gazebo is capable of simulating robot vision (classical and 3D sensor). Launch a simulation : '''bash roslaunch larm chalenge-2.launch ''' And observe the topic and mainly the one published the images ( camera/rgb/image_raw ) with rostopic list , rostopic info and rostopic hz . Image are published as 'sensor_msgs/Image' in camera/rgb/image_raw ROS doc So the pixel value is stored in img.data array but several tool to convert ROS image to OpenCV images already exist (for instance cv_bridge )","title":"Image from simulation"},{"location":"tuto-mastering/vision/#segmentation-dimages-couleur-par-seuillage-des-composantes-et-gestion-de-la-souris","text":"Voici quelques lignes de codes pour extraire une r\u00e9gion d'int\u00e9r\u00eat \u00e0 la souris. Gr\u00e2ce \u00e0 ces quelques lignes il vous sera possible de calculer la valeur moyenne et la variance de chaque composante de l'image, utile pour proc\u00e9der ensuite \u00e0 une \u00e9tape de segmentation. Dans cet exemple, nous proc\u00e9dons tout d'abord \u00e0 l'acquisition d'une image de la webcam du portable. Puis, nous d\u00e9finissons un crop de l'image acquise gr\u00e2ce \u00e0 la souris. Il vous est alors facile de calculer les m\u00e9triques statistiques que vous souhaitez sur ce crop. La moyenne et la variance d\u00e9finissent un mod\u00e8le gaussien sur chaque composante du crop. Dans la suite, vous pourrez soit utiliser le flux d'images provenant de votre webcam ou celui provenant de la cam\u00e9ra Realsense (cf. tuto pr\u00e9c\u00e9dent) import cv2 import numpy as np # connect to a sensor (0: webcam) cap=cv2.VideoCapture(0) # capture an image ret, frame=cap.read() # Select ROI r = cv2.selectROI(frame) # Crop image imCrop = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])] average_h = np.mean(imCrop[:,:,0]) average_s = np.mean(imCrop[:,:,1]) average_v = np.mean(imCrop[:,:,2]) print(average_h,average_s,average_v) # Display cropped image cv2.imshow(\"Image\", imCrop) cv2.waitKey(0) Dans cet exemple, il s'agit de produire un masque des pixels dont les composantes HSV sont comprises entre les variables lo et hi. Dans cet exemple, en agissant sur le click gauche ou droit de la souris vous diminuez ou augmentez la teinte h des deux variables lo et hi. Par ailleurs, dans notre exemple, vous constaterez que lo et hi se diff\u00e9rentient non seulement par leur teinte mais \u00e9galement par leur saturation. Vous pourrez tester ce script sur une image de votre visage. Ces quelques lignes de codes illustrent \u00e9galement comment g\u00e9rer des actions sur la souris. Elles g\u00e8res les \u00e9v\u00e9nements souris tels que le mouvement de la souris (cv2.EVENT_MOUSEMOVE), le double click milieu (EVENT_MBUTTONDBLCLK), le click droit (EVENT_RBUTTONDOWN) et le click gauche (EVENT_LBUTTONDOWN). import cv2 import numpy as np def souris(event, x, y, flags, param): global lo, hi, color, hsv_px if event == cv2.EVENT_MOUSEMOVE: # Conversion des trois couleurs RGB sous la souris en HSV px = frame[y,x] px_array = np.uint8([[px]]) hsv_px = cv2.cvtColor(px_array,cv2.COLOR_BGR2HSV) if event==cv2.EVENT_MBUTTONDBLCLK: color=image[y, x][0] if event==cv2.EVENT_LBUTTONDOWN: if color>5: color-=1 if event==cv2.EVENT_RBUTTONDOWN: if color<250: color+=1 lo[0]=color-10 hi[0]=color+10 color=100 lo=np.array([color-5, 100, 50]) hi=np.array([color+5, 255,255]) color_info=(0, 0, 255) cap=cv2.VideoCapture(0) cv2.namedWindow('Camera') cv2.setMouseCallback('Camera', souris) hsv_px = [0,0,0] # Creating morphological kernel kernel = np.ones((3, 3), np.uint8) while True: ret, frame=cap.read() image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) mask=cv2.inRange(image, lo, hi) mask=cv2.erode(mask, kernel, iterations=1) mask=cv2.dilate(mask, kernel, iterations=1) image2=cv2.bitwise_and(frame, frame, mask= mask) cv2.putText(frame, \"Couleur: {:d}\".format(color), (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Affichage des composantes HSV sous la souris sur l'image pixel_hsv = \" \".join(str(values) for values in hsv_px) font = cv2.FONT_HERSHEY_SIMPLEX cv2.putText(frame, \"px HSV: \"+pixel_hsv, (10, 260), font, 1, (255, 255, 255), 1, cv2.LINE_AA) cv2.imshow('Camera', frame) cv2.imshow('image2', image2) cv2.imshow('Mask', mask) if cv2.waitKey(1)&0xFF==ord('q'): break cap.release() cv2.destroyAllWindows() G\u00e9n\u00e9ralement il est tr\u00e8s int\u00e9ressante de changer d'espace colorim\u00e9trique afin de mieux cibler l'espace dans lequel l'objet d'int\u00e9r\u00eat est discriminable : image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) Apr\u00e8s avoir produite le mask avec mask=cv2.inRange(image, lo, hi) il est parfois pertinant de d\u00e9bruiter l'image r\u00e9sultats en la flouttant ou par quelques op\u00e9rations motrphologiques. Cela permet de fermer et remplir les formes : # Flouttage de l'image image=cv2.blur(image, (7, 7)) # Erosion d'un mask mask=cv2.erode(mask, None, iterations=4) # dilatation d'un mask mask=cv2.dilate(mask, None, iterations=4) Dans le code de segmentation d'une image couleur fourni pr\u00e9c\u00e9demment, vous jouerez avec la taille du kernel (3x3 dans notre exemple), vous ajouterez une \u00e9tape de flouttage de chaque canal en jouant sur la taille du voisinage (7 x 7 dans notre exemple). Pour finir vous jouerez avec les \u00e9tapes d'\u00e9rosion et de dilatation en modifiant le nombre de fois o\u00f9 chaque op\u00e9rateur morphologique est appliqu\u00e9 (4 fois dans notre exemple). Le code de segmentation d'une image couleur fourni pr\u00e9c\u00e9demment permet de d\u00e9finir un mask binaire des pixels dont les composantes HSV sont l'intervalle [lo,hi]. Il est alors possible de d\u00e9tecter les \u00e9l\u00e9ments connexes dans le mask afin d'en extraire certaines informations telles qu'ici le minEnclosingCircle. D'autres features peuvent \u00eatre utiles. Vous les trouverez ici : https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html Sous l'hypoth\u00e8se qu'un objet d'inter\u00eat est repr\u00e9sent\u00e9 par un ensemble de pixels connexes dont la couleur est contenu dans l'intervalle [lo,hi], il est alors possible de d\u00e9finir des contraintes sur un ensemble de features qui permettent de classer les objets ainsi d\u00e9tect\u00e9s. Vous ajouterez les lignes suivantes dans le code de segmentation pr\u00e9c\u00e9dent. elements=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] if len(elements) > 0: c=max(elements, key=cv2.contourArea) ((x, y), rayon)=cv2.minEnclosingCircle(c) if rayon>30: cv2.circle(image2, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(frame, (int(x), int(y)), 5, color_info, 10) cv2.line(frame, (int(x), int(y)), (int(x)+150, int(y)), color_info, 2) cv2.putText(frame, \"Objet !!!\", (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) ``` Reste ensuite \u00e0 visualiser les images. ## Extraction de r\u00e9gions dans une image binaris\u00e9e Voici quelques lignes en python pour extraire des r\u00e9gion de pixels connexes dans une image binaris\u00e9e ``` label()```. De ces r\u00e9gions sont extraites quelques propri\u00e9t\u00e9s ``` regionprops()``` Ce code agit strat\u00e9giquement de la m\u00eame mani\u00e8re que le script de segmentation pr\u00e9c\u00e9dent mais en utilisant la librairie skimage. ```python import cv2 import numpy as np import matplotlib.pyplot as plt from skimage.measure import label, regionprops import math image = cv2.imread('./imageasegmenter.jpg') # passage en niveau de gris gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ###### extration des r\u00e9gions avec la lib skimage # Binarisation de l'image ret, thresh = cv2.threshold(gray, 127, 255, 1) cv2.imshow(\"image seuill\u00e9e\",thresh) cv2.waitKey(0) # extraction des r\u00e9gions et des propri\u00e9t\u00e9s des r\u00e9gions label_img = label(thresh) regions = regionprops(label_img) print(regions) cv2.waitKey(0) # affichage des r\u00e9gions et des boites englobantes fig, ax = plt.subplots() ax.imshow(thresh, cmap=plt.cm.gray) for props in regions: y0, x0 = props.centroid orientation = props.orientation x1 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length y1 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5) ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5) ax.plot(x0, y0, '.g', markersize=15) minr, minc, maxr, maxc = props.bbox bx = (minc, maxc, maxc, minc, minc) by = (minr, minr, maxr, maxr, minr) ax.plot(bx, by, '-b', linewidth=2.5) ax.axis((0, 600, 600, 0)) plt.show() cv2.waitKey(0)","title":"Segmentation d'images couleur par seuillage des composantes et Gestion de la souris"},{"location":"tuto-mastering/vision/#detection-dobjets-par-template-matching","text":"Il est possible de d\u00e9tecter un ou plusieurs objets dans une image en appliquant une proc\u00e9dure de matching d'un template de chaque objet \u00e0 d\u00e9tecter. Un template est une image du ou des objets en question. La fonction \u00e0 utiliser est cv.matchTemplate(img_gray,template,parametre) . Plusieurs parametre de matching sont possibles correspondant chacun \u00e0 une m\u00e9trique de corr\u00e9lation. Voici les lignes de codes que vous testerez. Vous testerez les parametres suivants afin de d\u00e9finir celui qui fournit les meilleurs r\u00e9sultats. Par ailleurs, vous adapterez le code afin de prendre un charge le flux des images de la Realsense et une image template de l'objet que vous voulez d\u00e9tect\u00e9. import cv2 as cv import numpy as np from matplotlib import pyplot as plt # charger l'image dans laquelle on cherche l'objet img_rgb = cv.imread('car.png') img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY) # charger le template de l'objet \u00e0 rechercher template = cv.imread('template.png',0) # R\u00e9cup\u00e9ration des dimensions de l'image w, h = template.shape[::-1] # Application du template atching res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED) # S\u00e9lection des meilleurs matched objects threshold = 0.8 loc = np.where( res >= threshold) # Affichage de la boite englobante de chaque objet d\u00e9tect\u00e9 for pt in zip(*loc[::-1]): cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)","title":"D\u00e9tection d'objets par template matching"},{"location":"tuto-mastering/vision/#segmentation-des-images-par-la-methodes-des-k-moyennes-kmeans","text":"Kmeans est un algorithme de clustering, dont l'objectif est de partitionner n points de donn\u00e9es en k grappes. Chacun des n points de donn\u00e9es sera assign\u00e9 \u00e0 un cluster avec la moyenne la plus proche. La moyenne de chaque groupe s'appelle \u00abcentro\u00efde\u00bb ou \u00abcentre\u00bb. Globalement, l'application de k-means donne k grappes distinctes des n points de donn\u00e9es d'origine. Les points de donn\u00e9es \u00e0 l'int\u00e9rieur d'un cluster particulier sont consid\u00e9r\u00e9s comme \u00abplus similaires\u00bb les uns aux autres que les points de donn\u00e9es appartenant \u00e0 d'autres groupes. Cet algorithme peut \u00eatre appliquer sur des points d\u2019origine g\u00e9om\u00e9trique, colorim\u00e9triques et autres. Nous allons appliquer cette m\u00e9thode afin d'assurer une segmentation couleur d'une image i.e. cela revient \u00e0 trouver les couleur domainantes dans l'image. from sklearn.cluster import KMeans import matplotlib.pyplot as plt import cv2 import numpy as np #Ensuite charger une image et la convertir de BGR \u00e0 RGB si n\u00e9cessaire et l\u2019afficher : image = cv2.imread('lena.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.figure() plt.axis(\"off\") plt.imshow(image) Afin de traiter l\u2019image en tant que point de donn\u00e9es, il faut la convertir d\u2019une forme matricielle \u00e0 une forme vectorielle (liste de couleur rgb) avant d'appliquer la fonction de clustering : n_clusters=5 image = image.reshape((image.shape[0] * image.shape[1], 3)) clt = KMeans(n_clusters = n_clusters ) clt.fit(image) Pour afficher les couleurs les plus dominantes dans l'image, il faut d\u00e9finir deux fonctions : centroid_histogram() pour r\u00e9cup\u00e9rer le nombre de clusters diff\u00e9rents et cr\u00e9er un histogramme bas\u00e9 sur le nombre de pixels affect\u00e9s \u00e0 chaque cluster ; et plot_colors() pour initialiser le graphique \u00e0 barres repr\u00e9sentant la fr\u00e9quence relative de chacune des couleurs def centroid_histogram(clt): numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)>>> (hist, _) = np.histogram(clt.labels_, bins=numLabels) # normalize the histogram, such that it sums to one hist = hist.astype(\"float\") hist /= hist.sum() return hist def plot_colors(hist, centroids): bar = np.zeros((50, 300, 3), dtype=\"uint8\") startX = 0 # loop over the percentage of each cluster and the color of # each cluster for (percent, color) in zip(hist, centroids): # plot the relative percentage of each cluster endX = startX + (percent * 300) cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(\"uint8\").tolist(), -1) startX = endX return bar Il suffit maintenant de construire un histogramme de clusters puis cr\u00e9er une figure repr\u00e9sentant le nombre de pixels \u00e9tiquet\u00e9s pour chaque couleur. hist = centroid_histogram(clt) bar = plot_colors(hist, clt.cluster_centers_) plt.figure() plt.axis(\"off\") plt.imshow(bar) plt.show()","title":"Segmentation des images par la m\u00e9thodes des k-moyennes (kmeans)"},{"location":"tuto-mastering/vision/#classification-dimages-par-la-mathode-des-k-plus-proches-voisins-k-nn-ou-knn","text":"Cet exercice permettra d'apprendre un mod\u00e8le \u00e0 partir des images de la bases CIFAR-10 t\u00e9l\u00e9chargeable ici: http://www.cs.toronto.edu/~kriz/cifar.html D\u00e9compresser les fichier dans un dossier que vous utiliserez dans le script suivant. Ici, le dossier est ./data import numpy as np import cv2 basedir_data = \"./data/\" rel_path = basedir_data + \"cifar-10-batches-py/\" #D\u00e9s\u00e9rialiser les fichiers image afin de permettre l\u2019acc\u00e8s aux donn\u00e9es et aux labels: def unpickle(file): import pickle with open(file, 'rb') as fo: dict = pickle.load(fo,encoding='bytes') return dict X = unpickle(rel_path + 'data_batch_1') img_data = X[b'data'] img_label_orig = img_label = X[b'labels'] img_label = np.array(img_label).reshape(-1, 1) afin de v\u00e9rifier que tout s'est bien pass\u00e9 utilis\u00e9 : print(img_data) print('shape', img_data.shape) Vous devriez trouver un tableau numpy de 10000x3072 d'uint8s (le 3072 vient du 3 x 1024). Chaque ligne du tableau stocke une image couleur 32x32 en RGB. L'image est stock\u00e9e dans l'ordre des lignes principales, de sorte que les 32 premi\u00e8res entr\u00e9es du tableau correspondent aux valeurs des canaux rouges de la premi\u00e8re ligne de l'image. Pour v\u00e9rifier les labels : print(img_label) print('shape', img_label.shape) Nous avons les \u00e9tiquettes comme dane matrice 10000 x 1 Pour charger les donn\u00e9es de test, utiliser la m\u00eame proc\u00e9dure que pr\u00e9c\u00e9dement car la forme des donn\u00e9es de test est identique \u00e0 la forme des donn\u00e9es d\u2019apprentissage: test_X = unpickle(rel_path + 'test_batch'); test_data = test_X[b'data'] test_label = test_X[b'labels'] test_label = np.array(test_label).reshape(-1, 1) print(sample_img_data) print('shape', sample_img_data.shape) print('shape', sample_img_data[1,:].shape) Attention, les composantes RGB des images sont arrang\u00e9es sous la forme d'une vecteur \u00e0 1 dimension. Pour afficher chaque image, il faut donc remettre sous la forme d'une image 2D RGB. Pour cela, nous op\u00e9rons de la mani\u00e8re suivante en consid\u00e9rant que les images sont de r\u00e9solution 32x32 one_img=sample_img_data[0,:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) image = cv2.imread('./vector-handwritten-numbers-on-white-background-brusk-stroke.jpg') cv2.imshow('Image CIFAR',rgb) cv2.waitKey(0) cv2.destroyAllWindows() D\u00e9sormais, nous allons appliquer l'algorithmes des k-NN sur toutes les images de la base de training img_data et leurs labels img_label_orig from sklearn.neighbors import KNeighborsClassifier #def pred_label_fn(i, original): # return original + '::' + meta[YPred[i]].decode('utf-8') nbrs = KNeighborsClassifier(n_neighbors=3, algorithm='brute').fit(img_data, img_label_orig) # test sur les 10 premi\u00e8res images data_point_no = 10 sample_test_data = test_data[:data_point_no, :] YPred = nbrs.predict(sample_test_data) for i in range(0, len(YPred)): #show_im(sample_test_data, test_label, meta, i, label_fn=pred_label_fn) r = sample_test_data[i][:1024].reshape(32, 32) g = sample_test_data[i][1024:2048].reshape(32, 32) b = sample_test_data[i][2048:].reshape(32, 32) print(YPred[i]) cv2.imshow('image test',np.dstack([r, g, b])) neigh_dist,neigh_ind = nbrs.kneighbors([sample_test_data[i]]) print(neigh_ind) for j in range(0, len(neigh_ind[0])): one_img=img_data[neigh_ind[0][j],:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) cv2.imshow('K plus proche image',np.dstack([r, g, b])) cv2.waitKey(0)","title":"Classification d'images par la mathode des K plus proches voisins (k-NN ou KNN)"},{"location":"tuto-mastering/vision/#detection-dobjets-par-ondelettes-de-haar","text":"La d\u00e9tection d'objets \u00e0 l'aide de classificateurs en cascade bas\u00e9s sur la d\u00e9composition en ondelettes de Haar est une m\u00e9thode efficace de d\u00e9tection d'objets propos\u00e9e par Paul Viola et Michael Jones dans leur article, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" en 2001. Il s'agit d'une approche bas\u00e9e sur l'apprentissage automatique o\u00f9 un la fonction en cascade est form\u00e9e \u00e0 partir d'un grand nombre d'images positives et n\u00e9gatives. Cette m\u00e9thode a \u00e9t\u00e9 initialement mise en au point pour d\u00e9tecter des visages et a \u00e9t\u00e9 \u00e9tendu \u00e0 d'autres objets tels quels les voitures. En python, vous pouvez faire appel \u00e0 cette m\u00e9thode via object_cascade=cv2.CascadeClassifier() . Cette classe est instanci\u00e9e en lui passant un param\u00e8tre qui repr\u00e9sente le \"mod\u00e8le\" adapt\u00e9 \u00e0 l'objet \u00e0 d\u00e9tecter. Vous pouvez t\u00e9l\u00e9charger les mod\u00e8les relatifs \u00e0 des humains ici : https://github.com/opencv/opencv/tree/master/data/haarcascades Pour tester le d\u00e9tecteur sur des v\u00e9hicules, le mod\u00e8le propos\u00e9 par Andrews Sobral est t\u00e9l\u00e9chrgeable ici : https://github.com/andrewssobral/vehicle_detection_haarcascades/blob/master/cars.xml Pour appliquer le d\u00e9tecteur \u00e0 une image il suffit d'appeler la m\u00e9thode object=object_cascade.detectMultiScale(gray, scaleFactor=1.10, minNeighbors=3) en passant en param\u00e8tre le nom de la variable image (gray) qu'il faut pr\u00e9alablement transform\u00e9e en niveau de gris. Il fauat \u00e9galement renseigner le facteur d'\u00e9chelle (scaleFactor) utilis\u00e9 pour r\u00e9duire l'image \u00e0 chaque \u00e9tage et le nombre de voisins (minNeighbors) que chaque objet d\u00e9tect\u00e9 doit avoir pour le valider comme \"effectivement\" l'objet recherch\u00e9. Cette m\u00e9thode fournit une liste de boites englobantes (x, y, w et h) que vous afficherez sur chaque image couleur trait\u00e9e afin de visualiser les r\u00e9sultats de la d\u00e9tection. for x, y, w, h in object: cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) Ecrire un script permettant de mettre en musique cette classe et cette m\u00e9thode sur la vid\u00e9o cars.mp4 fournies. Vous validerez votre script en utilisant les mod\u00e8les relatifs au corps humains et en utilisant le flux d'une cam\u00e9ra.","title":"D\u00e9tection d'objets par ondelettes de Haar"},{"location":"tuto-mastering/vision/#model-training","text":"Cette m\u00e9thode pourrait \u00eatre tr\u00e8s int\u00e9ressante pour d\u00e9tecter des objets lors du \"challenge\". Pour cela, je vous invite \u00e0 lire et utiliser ce qui est propos\u00e9 sur les 2 liens suivants. Ces liens d\u00e9crivent comment il est possible d'apprendre un mod\u00e8le sp\u00e9cifique \u00e0 un objet donn\u00e9. http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html https://github.com/mrnugget/opencv-haar-classifier-training","title":"Model training"},{"location":"tuto-mastering/vision/#model-training-for-an-other-feature","text":"Vous trouverez dans le lien suivant, l'apprentissage d'un mod\u00e8le sur la base d'un autre type de caract\u00e9reristique : les Local Binary Pattern (LBP). https://medium.com/@rithikachowta/object-detection-lbp-cascade-classifier-generation-a1d1a1c2d0b","title":"Model training for an other feature"},{"location":"tuto-mastering/vision/#gestion-de-la-depth-map-et-estimation-de-la-distance","text":"Ce code permet de calculer la distance des parties de la sc\u00e8ne qui se projette en chaque pixel de la cam\u00e9ra. Il utilise les deux nouvelles parties de code suivantes. La premi\u00e8re permet de recaler les deux flux (depth map et rgb image) afin de garantir une correspondance pixel \u00e0 pixel. Ces deux parties de codes sont \u00e0 replac\u00e9es dans le code complet pr\u00e9sent\u00e9 ensuite. #Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() LA seconde permet de calculer la distance en un pixel (x,y) donn\u00e9. #Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) Le code complet integrant la partie acquisition et visualisation est la suivante. Un cercle est positionn\u00e9 dans les deux flux autour du pixel en lequel la distance est calcul\u00e9e. import pyrealsense2 as rs import numpy as np import math import cv2,time,sys pipeline = rs.pipeline() config = rs.config() colorizer = rs.colorizer() # fps plus bas (30) config.enable_stream(rs.stream.depth, 840, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 840, 480, rs.format.bgr8, 30) pipeline.start(config) align_to = rs.stream.depth align = rs.align(align_to) color_info=(0, 0, 255) rayon=10 count=1 refTime= time.process_time() freq= 60 try: while True: # This call waits until a new coherent set of frames is available on a device frames = pipeline.wait_for_frames() #Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() if not depth_frame or not aligned_color_frame: continue # Two ways to colorized the depth map # first : using colorizer of pyrealsense colorized_depth = colorizer.colorize(depth_frame) depth_colormap = np.asanyarray(colorized_depth.get_data()) # second : using opencv by applying colormap on depth image (image must be converted to 8-bit per pixel first) #depth_image = np.asanyarray(depth_frame.get_data()) #depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) # Get the intrinsic parameters color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics color_image = np.asanyarray(aligned_color_frame.get_data()) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape #Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) #print(\"Distance from camera to pixel:\", distance) #print(\"Z-depth from camera surface to pixel surface:\", depth) # Show images images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (640x480) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA) cv2.circle(images, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(images, (int(x+color_colormap_dim[1]), int(y)), int(rayon), color_info, 2) # Affichage distance au pixel (x,y) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x+color_colormap_dim[1])+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL) # Resize the Window cv2.resizeWindow('RealSense', 960, 720) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 except Exception as e: print(e) pass finally: pipeline.stop()","title":"Gestion de la depth map et estimation de la distance"}]}