{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lecture - Software and Architecture for Mobile-Robotics Here is a lecture on software and architecture for mobile robots at the IMT-Nord-Europe engineering school. This lecture is an introduction on how to develop a modular control and supervision software for a mobile platform. The notions are illustrated with tutorials based on Linux/ ROS and Kobuki/Turtlebot2 robot. This support is shared on github and published thanks to GitHub pages . Furthermore, the purpose of this lecture and its tutorials is not to cover all ROS elements. The tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. Notice that other solutions are provided by ROS2 with potentially more efficient results. Tutorials Tutorials are organized on several levels of complexity: Kick-Off focus on basic concept 'terminal commands', interconnected software developments on simulations and with a real mobile platform. Level-up deep into __ROS2_ to develop a complete packaged solution of reactive control. Mastering , with a good understanding of ROS2 tools and API, the goal now is to achieve complex missions including mapping and localization. Challenge The evaluation mainly consists in the realization of an application involving specific challenges: Autonomous Control of an AGV (Automated Guided Vehicle) Mapping and Localization Research and recognition of an object Going further Most of the content and supports for learning robotics architecture are already shared on the internet. We try to guide the students through project realizations rather than to provide an exhaustive definition of concepts and implementations. This course relies on the ROS middleware for practical sessions, the ROS doc tutorials and ros-packages' descriptions: docs.ros.org . An alternative to this course can be found on roboticsbackend.com . You also can find an excellent virtual working environment and resources on TheConstruct . Contact For comments, questions, corrections, feel free to contact: Guillaume Lozenguez (coordinator, but not the only author here).","title":"Intro"},{"location":"#lecture-software-and-architecture-for-mobile-robotics","text":"Here is a lecture on software and architecture for mobile robots at the IMT-Nord-Europe engineering school. This lecture is an introduction on how to develop a modular control and supervision software for a mobile platform. The notions are illustrated with tutorials based on Linux/ ROS and Kobuki/Turtlebot2 robot. This support is shared on github and published thanks to GitHub pages . Furthermore, the purpose of this lecture and its tutorials is not to cover all ROS elements. The tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. Notice that other solutions are provided by ROS2 with potentially more efficient results.","title":"Lecture - Software and Architecture for Mobile-Robotics"},{"location":"#tutorials","text":"Tutorials are organized on several levels of complexity: Kick-Off focus on basic concept 'terminal commands', interconnected software developments on simulations and with a real mobile platform. Level-up deep into __ROS2_ to develop a complete packaged solution of reactive control. Mastering , with a good understanding of ROS2 tools and API, the goal now is to achieve complex missions including mapping and localization.","title":"Tutorials"},{"location":"#challenge","text":"The evaluation mainly consists in the realization of an application involving specific challenges: Autonomous Control of an AGV (Automated Guided Vehicle) Mapping and Localization Research and recognition of an object","title":"Challenge"},{"location":"#going-further","text":"Most of the content and supports for learning robotics architecture are already shared on the internet. We try to guide the students through project realizations rather than to provide an exhaustive definition of concepts and implementations. This course relies on the ROS middleware for practical sessions, the ROS doc tutorials and ros-packages' descriptions: docs.ros.org . An alternative to this course can be found on roboticsbackend.com . You also can find an excellent virtual working environment and resources on TheConstruct .","title":"Going further"},{"location":"#contact","text":"For comments, questions, corrections, feel free to contact: Guillaume Lozenguez (coordinator, but not the only author here).","title":"Contact"},{"location":"appendix/faq/","text":"Frequent Asked Question... Multi-computer configuration Configure a ROS_DOMAIN_ID betwwen 1 and 100 in your bashrc file to limit the topics area, and set ROS_LOCALHOST_ONLY to zero to permits several computer to share ROS ressources. gedit ~/.bashrc add at the end of the file (with 42 for instance): export ROS_LOCALHOST_ONLY= 0 export ROS_DOMAIN_ID= 42 ROS2 tbot custom topic types I cannot echo topics provided by the tbot such /events/button or /events/bumper because the type isn't recognized. To install those custom types: cd $ROS_WORKSPACE git clone https://github.com/imt-mobisyst/pkg-interfaces.git colcon build --base-path pkg-interfaces source ./install/setup.bash ROS2 tbot driver How to get ROS2 tbot driver on my computer (to use it without a Pi) Install: cd $ROS_WORKSPACE git clone https://github.com/imt-mobisyst/pkg-interfaces.git git clone https://github.com/imt-mobisyst/pkg-tbot.git ./script/install-kobuki_ros.sh colcon build Launch: # base only ros2 launch tbot_start base.launch.py # base + laser ros2 launch tbot_start minimal.launch.py # base + with laser + camera ros2 launch tbot_start full.launch.py ros2 topic list Fix malformed packets Info there: https://github.com/kobuki-base/kobuki_core/commit/2bc11a1bf0ff364b37eb812a404f124dae9c0699 sudo cp /home/bot/ros2_ws/pkg-kobuki/kobuki_core/60-kobuki.rules /lib/udev/rules.d/ Then unplug / replug the robot. To ensure it worked, the followwing command should display 1: cat /sys/bus/usb-serial/devices/ttyUSB0/tty/ttyUSB0/device/latency_timer --> There is no Wifi on my dell-xps13 ? Connect with cable Get the appropriate drivers: killer driver sudo add-apt-repository ppa:canonical-hwe-team/ backport-iwlwifi sudo apt-get update sudo apt-get install backport-iwlwifi-dkms reboot Remove password asking for docker commands sudo echo \"\\n%sudo ALL=(ALL) NOPASSWD: /usr/bin/docker\\n\" >> /etc/sudoers Catkin_create_pkg - invalid email ? you can use the -m option to force an author name. catin_create_pkg -m AuthorName package_name [dependencies...] opencv_createsamples The latest OpenCV does not include opencv_createsamples . Let's compile an older version (~5 min on labs PC). git clone https://github.com/opencv/opencv.git cd opencv git checkout 3.4.17 mkdir build cd build cmake -D'CMAKE_BUILD_TYPE=RELEASE' .. make -j8 ls -l bin/opencv_createsamples How to get an aligned depth image to the color image ? you can use the align_depth:=true ROS parameter. The aligned image is streamed in a specific topic. (tks Orange group) roslaunch realsense2_camera rs_camera.launch align_depth:=true ROS1 vs ROS2 commands cheatsheet ROS1 ROS2 rostopic list ros2 topic list rqt_graph rqt_graph rviz rviz2 rosrun tf view_frames ros2 run tf2_tools view_frames colcon build --packages-select my_package colcon build --symlink-install colcon build --paths path/to/some/package other/path/to/other/packages/* colcon build --event-handlers console_direct+ --cmake-args -DCMAKE_VERBOSE_MAKEFILE=ON --packages-select my_package .bashrc ROS additions # ROS export ROS_LOCALHOST_ONLY=1 export PS1=\"\\${ROS_VERSION:+(ros\\$ROS_VERSION) }$PS1\" alias rosify1=\"source /opt/ros/noetic/setup.bash && source $HOME/ros1_ws/devel/setup.bash\" alias rosify2=\"source /opt/ros/iron/setup.bash && source $HOME/ros2_ws/install/setup.bash\" Flash kobuki https://kobuki.readthedocs.io/en/devel/firmware.html#linux ROS2 Package with custom Python Library suppose we are in packge my-package Inside the package folder my-package create a folder for my library. For example libs/my_lib Inside myLib folder add __init__.py file that imports resources from other python files. Add the CMakeList.txt install( DIRECTORY libs/my_lib DESTINATION lib/${PROJECT_NAME}) build the package colcon build More on ros2 tutorial by readthedocs :","title":"F.A.Q"},{"location":"appendix/faq/#frequent-asked-question","text":"","title":"Frequent Asked Question..."},{"location":"appendix/faq/#multi-computer-configuration","text":"Configure a ROS_DOMAIN_ID betwwen 1 and 100 in your bashrc file to limit the topics area, and set ROS_LOCALHOST_ONLY to zero to permits several computer to share ROS ressources. gedit ~/.bashrc add at the end of the file (with 42 for instance): export ROS_LOCALHOST_ONLY= 0 export ROS_DOMAIN_ID= 42","title":"Multi-computer configuration"},{"location":"appendix/faq/#ros2-tbot-custom-topic-types","text":"I cannot echo topics provided by the tbot such /events/button or /events/bumper because the type isn't recognized. To install those custom types: cd $ROS_WORKSPACE git clone https://github.com/imt-mobisyst/pkg-interfaces.git colcon build --base-path pkg-interfaces source ./install/setup.bash","title":"ROS2 tbot custom topic types"},{"location":"appendix/faq/#ros2-tbot-driver","text":"How to get ROS2 tbot driver on my computer (to use it without a Pi) Install: cd $ROS_WORKSPACE git clone https://github.com/imt-mobisyst/pkg-interfaces.git git clone https://github.com/imt-mobisyst/pkg-tbot.git ./script/install-kobuki_ros.sh colcon build Launch: # base only ros2 launch tbot_start base.launch.py # base + laser ros2 launch tbot_start minimal.launch.py # base + with laser + camera ros2 launch tbot_start full.launch.py ros2 topic list","title":"ROS2 tbot driver"},{"location":"appendix/faq/#fix-malformed-packets","text":"Info there: https://github.com/kobuki-base/kobuki_core/commit/2bc11a1bf0ff364b37eb812a404f124dae9c0699 sudo cp /home/bot/ros2_ws/pkg-kobuki/kobuki_core/60-kobuki.rules /lib/udev/rules.d/ Then unplug / replug the robot. To ensure it worked, the followwing command should display 1: cat /sys/bus/usb-serial/devices/ttyUSB0/tty/ttyUSB0/device/latency_timer -->","title":"Fix malformed packets"},{"location":"appendix/faq/#there-is-no-wifi-on-my-dell-xps13","text":"Connect with cable Get the appropriate drivers: killer driver sudo add-apt-repository ppa:canonical-hwe-team/ backport-iwlwifi sudo apt-get update sudo apt-get install backport-iwlwifi-dkms reboot","title":"There is no Wifi on my dell-xps13 ?"},{"location":"appendix/faq/#remove-password-asking-for-docker-commands","text":"sudo echo \"\\n%sudo ALL=(ALL) NOPASSWD: /usr/bin/docker\\n\" >> /etc/sudoers","title":"Remove password asking for docker commands"},{"location":"appendix/faq/#catkin_create_pkg-invalid-email","text":"you can use the -m option to force an author name. catin_create_pkg -m AuthorName package_name [dependencies...]","title":"Catkin_create_pkg - invalid email ?"},{"location":"appendix/faq/#opencv_createsamples","text":"The latest OpenCV does not include opencv_createsamples . Let's compile an older version (~5 min on labs PC). git clone https://github.com/opencv/opencv.git cd opencv git checkout 3.4.17 mkdir build cd build cmake -D'CMAKE_BUILD_TYPE=RELEASE' .. make -j8 ls -l bin/opencv_createsamples","title":"opencv_createsamples"},{"location":"appendix/faq/#how-to-get-an-aligned-depth-image-to-the-color-image","text":"you can use the align_depth:=true ROS parameter. The aligned image is streamed in a specific topic. (tks Orange group) roslaunch realsense2_camera rs_camera.launch align_depth:=true","title":"How to get an aligned depth image to the color image ?"},{"location":"appendix/faq/#ros1-vs-ros2-commands-cheatsheet","text":"ROS1 ROS2 rostopic list ros2 topic list rqt_graph rqt_graph rviz rviz2 rosrun tf view_frames ros2 run tf2_tools view_frames colcon build --packages-select my_package colcon build --symlink-install colcon build --paths path/to/some/package other/path/to/other/packages/* colcon build --event-handlers console_direct+ --cmake-args -DCMAKE_VERBOSE_MAKEFILE=ON --packages-select my_package","title":"ROS1 vs ROS2 commands cheatsheet"},{"location":"appendix/faq/#bashrc-ros-additions","text":"# ROS export ROS_LOCALHOST_ONLY=1 export PS1=\"\\${ROS_VERSION:+(ros\\$ROS_VERSION) }$PS1\" alias rosify1=\"source /opt/ros/noetic/setup.bash && source $HOME/ros1_ws/devel/setup.bash\" alias rosify2=\"source /opt/ros/iron/setup.bash && source $HOME/ros2_ws/install/setup.bash\"","title":".bashrc ROS additions"},{"location":"appendix/faq/#flash-kobuki","text":"https://kobuki.readthedocs.io/en/devel/firmware.html#linux","title":"Flash kobuki"},{"location":"appendix/faq/#ros2-package-with-custom-python-library","text":"suppose we are in packge my-package Inside the package folder my-package create a folder for my library. For example libs/my_lib Inside myLib folder add __init__.py file that imports resources from other python files. Add the CMakeList.txt install( DIRECTORY libs/my_lib DESTINATION lib/${PROJECT_NAME}) build the package colcon build More on ros2 tutorial by readthedocs :","title":"ROS2 Package with custom Python Library"},{"location":"appendix/installation/","text":"Installation How to install Ubuntu? Ubuntu is a fork of the Debian project, a Linux-based desktop operating system. Official website : https://ubuntu.com/ French community : https://www.ubuntu-fr.org/ Notice that, ubuntu can be installed in double boot mode in parallel to a Microsoft OS on your personal computer. It is not recommended to use Ubuntu+ROS in a virtual machine (the performances would be poor). To-do: Install Ubuntu 20.04 LTS (Long Term Supported version) from live USB-Key Ideally, use all the hard disk (you can split the disk in advance for double-boot install ) Configure \"bot\" username and \"bot\" password. Configure network Login and upgrade your installation sudo apt update sudo apt upgrade How to install ROS2? The official documentation is here: https://docs.ros.org/en/iron/Installation.html However, we have our own installation scripts here: https://imt-mobisyst.github.io/mb6-space/","title":"Installation"},{"location":"appendix/installation/#installation","text":"","title":"Installation"},{"location":"appendix/installation/#how-to-install-ubuntu","text":"Ubuntu is a fork of the Debian project, a Linux-based desktop operating system. Official website : https://ubuntu.com/ French community : https://www.ubuntu-fr.org/ Notice that, ubuntu can be installed in double boot mode in parallel to a Microsoft OS on your personal computer. It is not recommended to use Ubuntu+ROS in a virtual machine (the performances would be poor).","title":"How to install Ubuntu?"},{"location":"appendix/installation/#to-do","text":"Install Ubuntu 20.04 LTS (Long Term Supported version) from live USB-Key Ideally, use all the hard disk (you can split the disk in advance for double-boot install ) Configure \"bot\" username and \"bot\" password. Configure network Login and upgrade your installation sudo apt update sudo apt upgrade","title":"To-do:"},{"location":"appendix/installation/#how-to-install-ros2","text":"The official documentation is here: https://docs.ros.org/en/iron/Installation.html However, we have our own installation scripts here: https://imt-mobisyst.github.io/mb6-space/","title":"How to install ROS2?"},{"location":"appendix/simulation_ros1/","text":"Simulation in ROS Gazebo Simulator Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI system using realistic scenarios. Gazebo is integrated with ROS (cf. Gazebo ROS ) and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available. Gazebo Installation Verify that Gazebo is installed. $ dpkg -l | grep gazebo ii gazebo11 11.12.0-1~focal amd64 Open Source Robotics Simulator ii gazebo11-common 11.12.0-1~focal all Open Source Robotics Simulator - Shared files ii gazebo11-plugin-base 11.12.0-1~focal amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo11:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - shared library ii libgazebo11-dev:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - Development Files ii ros-iron-gazebo-dev 3.5.3-1focal.20220829.174620 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.5.3-1focal.20221012.224922 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.5.3-1focal.20221021.150213 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.5.3-1focal.20221013.010602 amd64 Utilities to interface with Gazebo through ROS. Notice that you can install missing packages with the command line: sudo apt install <pakage_name> . Install larm_material ROS1 packages The better way to use Gazebo with ROS is to launch the simulator using ROS launch files. LARM Material is a git repository containing ROS1 resources for this lecture and therefore already done launch files. {% hint style=\"warning\" %} In this lecture, we will still a ROS1 environment to launch Gazebo because the tbot simulated model is not yet ready for ROS2. Cloning and installing the larm_material repository: #ensure ros1_ws catkin workspace is created mkdir -p ~/ros1_ws/src cd ~/ros1_ws/src # clone the LARM repo git clone https://bitbucket.org/imt-mobisyst/larm_material.git # compile cd ~/ros1_ws catkin_make # let your shell know about new ROS packages source devel/setup.bash Launch your first Gazebo Simulation Finally you can launch a preconfigured simulation: $ rosify1 # cf. FAQ to have the rosify1 shell command (ros1) $ roslaunch larm challenge-1.launch Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera. The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipments. Try to list and explore the different topics in a new terminal: (ros1) $ rostopic list (ros1) $ rostopic info <topic_name> ... Question: In which topic are laser scans published ? and camera images ? Connect to ROS2 and Visualize Topics, like the other tools of ROS, has evolved from ROS1 to ROS2 . This evolution makes them incompatible, but it is possble to start a bridge node to transfer information from ROS1 to ROS2 and vise-versa. The ROS1_bridge (a ros2 package) with its dynamic_bridge listen to connection to topics and transfers the data. Then, Rviz2 will be capable of reading and showing the data simulated by Gazebo. rviz2 is a very useful and versatile tool to visualize data that goes through topics. Start the dynamic bridge, Rviz2 and visualize the laser scan data, and the camera data. Frame and transformations Test the main ROS2 GUI tools Rviz and Rqt (in new terminals): Configure rviz2 to visualize the laser scans. Be carreful, ensure that Global Option / Fixed frame is correctly configured to base_link . Question: why is this important? (hint: check your tf using ros2 run tf_tools view_frames.py ) Use rqt to see the graph of ROS nodes and the topics they use to communicate (ref of ROS Qt: RQt ). Controlling the Simulated Robot Launch a simple node to control a robot using keyboard: rosify2 ros2 run teleop_twist_keyboard teleop_twist_keyboard Why can't you control the robot ? Use the tbot_pytool multiplexer and command the robot from the /multi/cmd_teleop topic. # First terminal: ros2 run tbot_pytool multiplexer # Second terminal: ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args --remap /cmd_vel:=/multi/cmd_teleop How many terminals are open ? tuto_sim Create a new package (python or cmake as you want) tuto_sim in your ROS2 workspace and create a launch file that starts with the apropriate configration: the dynamic_bridge , rviz2 , multiplexer and the teleop . All the information you need are in the tutorials on docs.ros.org .","title":"Simulation in ROS"},{"location":"appendix/simulation_ros1/#simulation-in-ros","text":"","title":"Simulation in ROS"},{"location":"appendix/simulation_ros1/#gazebo-simulator","text":"Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI system using realistic scenarios. Gazebo is integrated with ROS (cf. Gazebo ROS ) and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available.","title":"Gazebo Simulator"},{"location":"appendix/simulation_ros1/#gazebo-installation","text":"Verify that Gazebo is installed. $ dpkg -l | grep gazebo ii gazebo11 11.12.0-1~focal amd64 Open Source Robotics Simulator ii gazebo11-common 11.12.0-1~focal all Open Source Robotics Simulator - Shared files ii gazebo11-plugin-base 11.12.0-1~focal amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo11:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - shared library ii libgazebo11-dev:amd64 11.12.0-1~focal amd64 Open Source Robotics Simulator - Development Files ii ros-iron-gazebo-dev 3.5.3-1focal.20220829.174620 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.5.3-1focal.20221012.224922 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.5.3-1focal.20221021.150213 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.5.3-1focal.20221013.010602 amd64 Utilities to interface with Gazebo through ROS. Notice that you can install missing packages with the command line: sudo apt install <pakage_name> .","title":"Gazebo Installation"},{"location":"appendix/simulation_ros1/#install-larm_material-ros1-packages","text":"The better way to use Gazebo with ROS is to launch the simulator using ROS launch files. LARM Material is a git repository containing ROS1 resources for this lecture and therefore already done launch files. {% hint style=\"warning\" %} In this lecture, we will still a ROS1 environment to launch Gazebo because the tbot simulated model is not yet ready for ROS2. Cloning and installing the larm_material repository: #ensure ros1_ws catkin workspace is created mkdir -p ~/ros1_ws/src cd ~/ros1_ws/src # clone the LARM repo git clone https://bitbucket.org/imt-mobisyst/larm_material.git # compile cd ~/ros1_ws catkin_make # let your shell know about new ROS packages source devel/setup.bash","title":"Install larm_material ROS1 packages"},{"location":"appendix/simulation_ros1/#launch-your-first-gazebo-simulation","text":"Finally you can launch a preconfigured simulation: $ rosify1 # cf. FAQ to have the rosify1 shell command (ros1) $ roslaunch larm challenge-1.launch Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera. The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipments. Try to list and explore the different topics in a new terminal: (ros1) $ rostopic list (ros1) $ rostopic info <topic_name> ... Question: In which topic are laser scans published ? and camera images ?","title":"Launch your first Gazebo Simulation"},{"location":"appendix/simulation_ros1/#connect-to-ros2-and-visualize","text":"Topics, like the other tools of ROS, has evolved from ROS1 to ROS2 . This evolution makes them incompatible, but it is possble to start a bridge node to transfer information from ROS1 to ROS2 and vise-versa. The ROS1_bridge (a ros2 package) with its dynamic_bridge listen to connection to topics and transfers the data. Then, Rviz2 will be capable of reading and showing the data simulated by Gazebo. rviz2 is a very useful and versatile tool to visualize data that goes through topics. Start the dynamic bridge, Rviz2 and visualize the laser scan data, and the camera data.","title":"Connect to ROS2 and Visualize"},{"location":"appendix/simulation_ros1/#frame-and-transformations","text":"Test the main ROS2 GUI tools Rviz and Rqt (in new terminals): Configure rviz2 to visualize the laser scans. Be carreful, ensure that Global Option / Fixed frame is correctly configured to base_link . Question: why is this important? (hint: check your tf using ros2 run tf_tools view_frames.py ) Use rqt to see the graph of ROS nodes and the topics they use to communicate (ref of ROS Qt: RQt ).","title":"Frame and transformations"},{"location":"appendix/simulation_ros1/#controlling-the-simulated-robot","text":"Launch a simple node to control a robot using keyboard: rosify2 ros2 run teleop_twist_keyboard teleop_twist_keyboard Why can't you control the robot ? Use the tbot_pytool multiplexer and command the robot from the /multi/cmd_teleop topic. # First terminal: ros2 run tbot_pytool multiplexer # Second terminal: ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args --remap /cmd_vel:=/multi/cmd_teleop How many terminals are open ?","title":"Controlling the Simulated Robot"},{"location":"appendix/simulation_ros1/#tuto_sim","text":"Create a new package (python or cmake as you want) tuto_sim in your ROS2 workspace and create a launch file that starts with the apropriate configration: the dynamic_bridge , rviz2 , multiplexer and the teleop . All the information you need are in the tutorials on docs.ros.org .","title":"tuto_sim"},{"location":"appendix/tbot/","text":"Tbot In this lecture, we use a tbot. It is a turtlebot2 base equipped with: a kobuki base an hokuyo 2d lidar (potentially, a realsense RGBD camera D435I) Robot configuration : ROS2 driver is availlable on Tbot git repository","title":"Tbot"},{"location":"appendix/tbot/#tbot","text":"In this lecture, we use a tbot. It is a turtlebot2 base equipped with: a kobuki base an hokuyo 2d lidar (potentially, a realsense RGBD camera D435I)","title":"Tbot"},{"location":"appendix/tbot/#robot-configuration","text":"ROS2 driver is availlable on Tbot git repository","title":"Robot configuration :"},{"location":"appendix/turtlebot2/","text":"Simulate a turtlebot2 The first step in the challenge is to get control over a turtlebot2 robot equipped with a standard plan laser. Robot configuration Turtlebot is a simple robot very similar than small home-cleaning robot, but with a connection panel allowing hacking its sensors and actuators. More detail on the official web site . Basically, the turtlebot is equipped with sonars and a 3D camera. The robot version to use is also equipped with a scanning rangefinder like the one proposed by hokuyo . Those solutions are also well supported in ROS . On TheConstruct RDS TheConstruct RDS provides built in the gazebo simulation with turtlebot2 robots (on Melodic). You can start the simulation from your fresh gited ROSject (Melodic - No template) through the simulation button. Then select the turtlebot robot in an empty world. Finally, you will be capable of spawning some obstacles to avoid. Avoid obstacles The solution development for the challenge can begin. The first mission would be to permit the robot to move from its start position toward a goal position by avoiding the obstacles. A quick look at the available ROS topics permit to identifiate entrances for control and scan. rostopic list The goal position (in odom frame) is transmeted to the robot by using Rviz.","title":"Simulate a turtlebot2"},{"location":"appendix/turtlebot2/#simulate-a-turtlebot2","text":"The first step in the challenge is to get control over a turtlebot2 robot equipped with a standard plan laser.","title":"Simulate a turtlebot2"},{"location":"appendix/turtlebot2/#robot-configuration","text":"Turtlebot is a simple robot very similar than small home-cleaning robot, but with a connection panel allowing hacking its sensors and actuators. More detail on the official web site . Basically, the turtlebot is equipped with sonars and a 3D camera. The robot version to use is also equipped with a scanning rangefinder like the one proposed by hokuyo . Those solutions are also well supported in ROS .","title":"Robot configuration"},{"location":"appendix/turtlebot2/#on-theconstruct-rds","text":"TheConstruct RDS provides built in the gazebo simulation with turtlebot2 robots (on Melodic). You can start the simulation from your fresh gited ROSject (Melodic - No template) through the simulation button. Then select the turtlebot robot in an empty world. Finally, you will be capable of spawning some obstacles to avoid.","title":"On TheConstruct RDS"},{"location":"appendix/turtlebot2/#avoid-obstacles","text":"The solution development for the challenge can begin. The first mission would be to permit the robot to move from its start position toward a goal position by avoiding the obstacles. A quick look at the available ROS topics permit to identifiate entrances for control and scan. rostopic list The goal position (in odom frame) is transmeted to the robot by using Rviz.","title":"Avoid obstacles"},{"location":"challenge/challenge-1on2/","text":"Random Search in a Small Environment The goal of the challenge is to demonstrate the capability of a robot to move in a cluttered environment with a possibility to see what the robot see. Expected The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area while avoiding obstacles (i.e. area limits and some obstacles randomly set in the area). The sensor data (scan, vision) and potentially other information (considered obstacles, frames,...) is visible on rviz. The robot trajectory would permit the robot to explore the overall area. In other words, the robot will go everywhere (i.e. the probability that the robot will reach a specific reachable position in the area is equal to 1 at infinite time). One first approach can be to develop a ricochet robot that changes its direction randomly each time an obstacle prevent the robot to move forward. consigns Each group commit the minimal required files in a specific grp_pibotXX ros2 package inside their git repository. Release: Thusday afternoon of week-3 (Monday 14-01-2025) The required files: At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_pibotXX matching the ROS2 package where the elements will be found ( XX matches the number of the pibot ). Inside the grp_pibotXX package, a launch file simulation_v1_launch.yaml starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_v1_launch.yaml starting the appropriate nodes for demonstrating with a tbot. Criteria The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured. The robot moves everywhere in it environment. A String message is sent in a detection topic each time a bottle is found in front of the robot. Evaluation protocol (for evaluators...) Here the evaluation protocol applied. It is highly recommended to process it yourself before the submission... Clone the group\u2019s repository Take a look to what is inside the repository and read the README.md file. Build it ( colcon build and source from the workspace directory) accordingly to README.md instructions. Launch the simulation demonstration: ros2 launch grp_pibotXX simulation_v1_launch.yaml and appreciate the solution (at this point vision detection should not be activated) Stop everything. Start the pibotXX robot, and connect the camera. Launch the Turtlebot demonstration ( ros2 launch grp_pibotXX tbot_v1_launch.yaml ), and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Challenge 1"},{"location":"challenge/challenge-1on2/#random-search-in-a-small-environment","text":"The goal of the challenge is to demonstrate the capability of a robot to move in a cluttered environment with a possibility to see what the robot see.","title":"Random Search in a Small Environment"},{"location":"challenge/challenge-1on2/#expected","text":"The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area while avoiding obstacles (i.e. area limits and some obstacles randomly set in the area). The sensor data (scan, vision) and potentially other information (considered obstacles, frames,...) is visible on rviz. The robot trajectory would permit the robot to explore the overall area. In other words, the robot will go everywhere (i.e. the probability that the robot will reach a specific reachable position in the area is equal to 1 at infinite time). One first approach can be to develop a ricochet robot that changes its direction randomly each time an obstacle prevent the robot to move forward.","title":"Expected"},{"location":"challenge/challenge-1on2/#consigns","text":"Each group commit the minimal required files in a specific grp_pibotXX ros2 package inside their git repository. Release: Thusday afternoon of week-3 (Monday 14-01-2025)","title":"consigns"},{"location":"challenge/challenge-1on2/#the-required-files","text":"At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_pibotXX matching the ROS2 package where the elements will be found ( XX matches the number of the pibot ). Inside the grp_pibotXX package, a launch file simulation_v1_launch.yaml starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_v1_launch.yaml starting the appropriate nodes for demonstrating with a tbot.","title":"The required files:"},{"location":"challenge/challenge-1on2/#criteria","text":"The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured. The robot moves everywhere in it environment. A String message is sent in a detection topic each time a bottle is found in front of the robot.","title":"Criteria"},{"location":"challenge/challenge-1on2/#evaluation-protocol-for-evaluators","text":"Here the evaluation protocol applied. It is highly recommended to process it yourself before the submission... Clone the group\u2019s repository Take a look to what is inside the repository and read the README.md file. Build it ( colcon build and source from the workspace directory) accordingly to README.md instructions. Launch the simulation demonstration: ros2 launch grp_pibotXX simulation_v1_launch.yaml and appreciate the solution (at this point vision detection should not be activated) Stop everything. Start the pibotXX robot, and connect the camera. Launch the Turtlebot demonstration ( ros2 launch grp_pibotXX tbot_v1_launch.yaml ), and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Evaluation protocol (for evaluators...)"},{"location":"challenge/challenge-2on2/","text":"Efficient Exploration The goal of the challenge is to demonstrate the capability a robot has to navigate in a cluttered environment and locate specific objects. The localization requires to build a map of the environment. Expected The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area by avoiding obstacles. The robot knowledge is extended with new incoming data. In other words, the robot build a map and localizes itself in it. The robot detects NukaCola bottle in the vision flux. Messages are sent in topics one to state the detection and another one to mark the position in the map (cf. marker_msgs ) Experiments can be performed with 2 computers, one on the robot (Control PC) and a second for visualization and human control (Operator PC). consigns Each group commit the minimal required files in a specific grp_'machine' ros2 package inside their git repository. Release: Monday of week-4, Monday 22 of January The required files: At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_'machine' matching the ROS2 package where the elements will be found ( machine matches the name of the machine embedded in the robot). Inside the grp_'machine' package, a launch file simulation_launch starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_launch starting the appropriate nodes for demonstrating with a Turtlebot. Finally, a launch file operator_launch start a visualization + control solution. In simulations, we will work with the configuration set in challenge-2.launch.py . Criteria Minimal: The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured in a second PC and display the built map. It is possible to visualize a marker for detected bottles at the position of the bottle in the environment. The bottles are identified with a number and the robot is capable of recognizing a bottle on a second passage (it should be easy to count the bottle in the map or by reading the topic). The bottle detection detects all the bottles, in any position but with robustness to false positives. Optional (the order does not matter): The robot movement is oriented toward the unknown areas to speed up the exploration. Processes are clearly established (start and stop the robot, save the map, get the lists of bottles, set the xp in pause, ...) Developed nodes are based on ROS2 Parameters (for speed, obstacle detections, ...) The Kobuki features are integrated to the scenario (robot button, contact to the ground, void detection, bips,...) The list is not exhaustive, be inventive ! The challenge2 tbot.launch.py launch file may take an initial map. Evaluation protocol Here the evaluation protocol to apply. It is highly recommended to process it yourself before the submission... Clone/update the group\u2019s repository on both machines (Control and Operator) Take a look to what is inside the repository and read the README.md file (normally it states that the project depends on mb6-tbot , make sure that mb6-tbot project is already installed aside). Build it: colcon build and source from the workspace directory. Set the appropriate ROS configuration (domain ID, etc.). Launch the simulation demonstration: ros2 launch challenge1 simulation.launch.py and appreciate the solution. Stop everything. Configure the computers for working with the Tbot. Launch the Tbot demonstration, and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Challenge 2"},{"location":"challenge/challenge-2on2/#efficient-exploration","text":"The goal of the challenge is to demonstrate the capability a robot has to navigate in a cluttered environment and locate specific objects. The localization requires to build a map of the environment.","title":"Efficient Exploration"},{"location":"challenge/challenge-2on2/#expected","text":"The robot is positioned somewhere in a closed area (i.e. an area bounded with obstacles). The robot moves continuously in this area by avoiding obstacles. The robot knowledge is extended with new incoming data. In other words, the robot build a map and localizes itself in it. The robot detects NukaCola bottle in the vision flux. Messages are sent in topics one to state the detection and another one to mark the position in the map (cf. marker_msgs ) Experiments can be performed with 2 computers, one on the robot (Control PC) and a second for visualization and human control (Operator PC).","title":"Expected"},{"location":"challenge/challenge-2on2/#consigns","text":"Each group commit the minimal required files in a specific grp_'machine' ros2 package inside their git repository. Release: Monday of week-4, Monday 22 of January","title":"consigns"},{"location":"challenge/challenge-2on2/#the-required-files","text":"At the root repository, a README.md file in markdown syntax introducing the project. A directory grp_'machine' matching the ROS2 package where the elements will be found ( machine matches the name of the machine embedded in the robot). Inside the grp_'machine' package, a launch file simulation_launch starting the appropriate nodes for demonstrating in the simulation. Then, a launch file tbot_launch starting the appropriate nodes for demonstrating with a Turtlebot. Finally, a launch file operator_launch start a visualization + control solution. In simulations, we will work with the configuration set in challenge-2.launch.py .","title":"The required files:"},{"location":"challenge/challenge-2on2/#criteria","text":"Minimal: The group follows the consigns (i.e. the repository is presented as expected) The robot behavior is safe (no collision with any obstacles) rviz2 is started and well configured in a second PC and display the built map. It is possible to visualize a marker for detected bottles at the position of the bottle in the environment. The bottles are identified with a number and the robot is capable of recognizing a bottle on a second passage (it should be easy to count the bottle in the map or by reading the topic). The bottle detection detects all the bottles, in any position but with robustness to false positives. Optional (the order does not matter): The robot movement is oriented toward the unknown areas to speed up the exploration. Processes are clearly established (start and stop the robot, save the map, get the lists of bottles, set the xp in pause, ...) Developed nodes are based on ROS2 Parameters (for speed, obstacle detections, ...) The Kobuki features are integrated to the scenario (robot button, contact to the ground, void detection, bips,...) The list is not exhaustive, be inventive ! The challenge2 tbot.launch.py launch file may take an initial map.","title":"Criteria"},{"location":"challenge/challenge-2on2/#evaluation-protocol","text":"Here the evaluation protocol to apply. It is highly recommended to process it yourself before the submission... Clone/update the group\u2019s repository on both machines (Control and Operator) Take a look to what is inside the repository and read the README.md file (normally it states that the project depends on mb6-tbot , make sure that mb6-tbot project is already installed aside). Build it: colcon build and source from the workspace directory. Set the appropriate ROS configuration (domain ID, etc.). Launch the simulation demonstration: ros2 launch challenge1 simulation.launch.py and appreciate the solution. Stop everything. Configure the computers for working with the Tbot. Launch the Tbot demonstration, and appreciate the solution. Take a look to the code, by starting from the launch files.","title":"Evaluation protocol"},{"location":"challenge/kick-off/","text":"Challenge Kick-Off The challenge aims at making learners develop a first robotic project. In the end, both the execution of the proposed solution and the source code with documentation will be evaluated. The main objectives of the project consist of: Control a robot in a cluttered environment Map a static environment Detect all the Nuka-Cola bottles Estimate the position of all the Nuka-Cola in the map Optimize the exploration strategy Challenges are proposed to increase sequentially the complexity of the expected solution, but first the students have to structure their developping environment... Create a group As a first move, you have to constitute a group of \\(3\\) developers. Record the created group on a shared document: groups' doc . Create a new section for your group Record the name of the machine and the pibot you use and the names of each member of the group. The number of the pibot matches the number identifying a group and it must be used as ROS_DOMAIN_ID . Attributs a role to each of the group members: Navigation - Responcible for the good robot navigation. Localization - Responcible for the good localization of objects to detect. Integration - Responcible for the good integration of the overall architecture. Generate a working environment We ask each group to use git and share their work with professors through github (because it is the reference) (or gitlab at the student request, because it is open). Git is a versioning program working in a distributed way. \"Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development.\" Wikipedia-2021-12-6 . Official web site: git-scm.com git basics: in video Potentially the developers can share some of the versions there developed (or all) by installing an extra git on a shared server. github is certainly the most famous web git-solution (but you can install for instance a web application as gitlab on your own server for instance). Notice that gitlab is both an open solution you can deploy on your own server and a web-service ( gitlab.com ) you can use. For this lecture, you will use github or gitlab.com , no other solution would be accepted. (Each of the developers) Create a github account. (One of the developers) Create a new repository on github and invite the teammates. Choose a name referring the group (for instance uvlarm-machinename ), the idea is to propose a repository name clear but different from a group to another. (Each of the developers) Clone locally. In developpers terminals: cd your-ros-workspace git clone https://my.github.url/uvlarm-machinename.git (One of the developers) Invite the professor (or set the repository public) - github group: imt-mobisyst (One of the developers) Reccord the url in the shared document: (2023-2024 groups). You can then, work with visual studio code. By opening a the project uvlarm-machinename , VSCode will recognise the git repository (i.e. the presence of hiden .git directory). VSCode is also capable of managing a secure connection between your machine and github. code uvlarm-machinename Optionaly, you can configure ssh access: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/about-ssh Initialize: Your repository has to match a meta ROS-package (i.e. a directory composed by other directories each of them matching a ros-package). The repository would be cloned aside of the larm packages ( pkg-tbot ). So clone your repository in the your-ros-workspace directory and then create as many packages you want inside. cd ~/your-ros-workspace git clone github/uvlarm-machinename.git (One of the developers) Initialize a README.md file in Mardown at least with a simple tittle and refering the developers (cf. Markdown syntax ): echo \"# grp-`machinename` repository for the UV-LARM\" > README.md git add README.md It is possible then to push on the server your first commit. One commit refer to one version of your project, you can (must) generate has versions has you want. A commit do not have to serve a functional version of your works. It could be transitive commit. And most importantly git is capable to re-generate any of your commits, so do not hesitate to commit continuously... git commit -am \"Initialize README file\" git pull git push All the other developers can now pull the new version (typically, in the IMT computers).... New package: Then you can go inside your repository and create a new ros package: cd larm-machinename ros2 pkg create ... # cf. package tutorial A new directory my_amasing_pkg appears: The git status command informs you that this package is not followed by git . Let correct that. git add my_amasing_pkg git commit -am \"Initializing my_amasing_pkg\" Strange, nothing changes on my github repo. The git repo on github is a different entity than the one on your computer. You have to manually synchronize then when you want to share your work with the command pull and push . Try-it. Now you can commit , pull , push as often as possible (and add if you have some new files...).","title":"Kick-off"},{"location":"challenge/kick-off/#challenge-kick-off","text":"The challenge aims at making learners develop a first robotic project. In the end, both the execution of the proposed solution and the source code with documentation will be evaluated. The main objectives of the project consist of: Control a robot in a cluttered environment Map a static environment Detect all the Nuka-Cola bottles Estimate the position of all the Nuka-Cola in the map Optimize the exploration strategy Challenges are proposed to increase sequentially the complexity of the expected solution, but first the students have to structure their developping environment...","title":"Challenge Kick-Off"},{"location":"challenge/kick-off/#create-a-group","text":"As a first move, you have to constitute a group of \\(3\\) developers. Record the created group on a shared document: groups' doc . Create a new section for your group Record the name of the machine and the pibot you use and the names of each member of the group. The number of the pibot matches the number identifying a group and it must be used as ROS_DOMAIN_ID . Attributs a role to each of the group members: Navigation - Responcible for the good robot navigation. Localization - Responcible for the good localization of objects to detect. Integration - Responcible for the good integration of the overall architecture.","title":"Create a group"},{"location":"challenge/kick-off/#generate-a-working-environment","text":"We ask each group to use git and share their work with professors through github (because it is the reference) (or gitlab at the student request, because it is open). Git is a versioning program working in a distributed way. \"Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development.\" Wikipedia-2021-12-6 . Official web site: git-scm.com git basics: in video Potentially the developers can share some of the versions there developed (or all) by installing an extra git on a shared server. github is certainly the most famous web git-solution (but you can install for instance a web application as gitlab on your own server for instance). Notice that gitlab is both an open solution you can deploy on your own server and a web-service ( gitlab.com ) you can use. For this lecture, you will use github or gitlab.com , no other solution would be accepted. (Each of the developers) Create a github account. (One of the developers) Create a new repository on github and invite the teammates. Choose a name referring the group (for instance uvlarm-machinename ), the idea is to propose a repository name clear but different from a group to another. (Each of the developers) Clone locally. In developpers terminals: cd your-ros-workspace git clone https://my.github.url/uvlarm-machinename.git (One of the developers) Invite the professor (or set the repository public) - github group: imt-mobisyst (One of the developers) Reccord the url in the shared document: (2023-2024 groups). You can then, work with visual studio code. By opening a the project uvlarm-machinename , VSCode will recognise the git repository (i.e. the presence of hiden .git directory). VSCode is also capable of managing a secure connection between your machine and github. code uvlarm-machinename Optionaly, you can configure ssh access: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/about-ssh","title":"Generate a working environment"},{"location":"challenge/kick-off/#initialize","text":"Your repository has to match a meta ROS-package (i.e. a directory composed by other directories each of them matching a ros-package). The repository would be cloned aside of the larm packages ( pkg-tbot ). So clone your repository in the your-ros-workspace directory and then create as many packages you want inside. cd ~/your-ros-workspace git clone github/uvlarm-machinename.git (One of the developers) Initialize a README.md file in Mardown at least with a simple tittle and refering the developers (cf. Markdown syntax ): echo \"# grp-`machinename` repository for the UV-LARM\" > README.md git add README.md It is possible then to push on the server your first commit. One commit refer to one version of your project, you can (must) generate has versions has you want. A commit do not have to serve a functional version of your works. It could be transitive commit. And most importantly git is capable to re-generate any of your commits, so do not hesitate to commit continuously... git commit -am \"Initialize README file\" git pull git push All the other developers can now pull the new version (typically, in the IMT computers)....","title":"Initialize:"},{"location":"challenge/kick-off/#new-package","text":"Then you can go inside your repository and create a new ros package: cd larm-machinename ros2 pkg create ... # cf. package tutorial A new directory my_amasing_pkg appears: The git status command informs you that this package is not followed by git . Let correct that. git add my_amasing_pkg git commit -am \"Initializing my_amasing_pkg\" Strange, nothing changes on my github repo. The git repo on github is a different entity than the one on your computer. You have to manually synchronize then when you want to share your work with the command pull and push . Try-it. Now you can commit , pull , push as often as possible (and add if you have some new files...).","title":"New package:"},{"location":"pdf/","text":"PDF resources: An introdotion [ pdf ]","title":""},{"location":"pdf/#pdf-resources","text":"An introdotion [ pdf ]","title":"PDF resources:"},{"location":"tuto-kick-off/basics/","text":"Operating System: Linux/Ubuntu for ROS2 ROS stands for R obot O perating S ystem. However, it extends a classical O perating S ystem, specifically a Linux/Ubuntu OS. The goal of this tutorial is to revisit basic O perating S ystem concepts, focusing mainly on the Shell , which offers the simplest way to interact with your computer. For an introduction to operating systems, read the Wikipedia page . Play with Linux Ubuntu is a Linux distribution (Operating System + Window Management + Tools) derived from the Debian distribution, within the galaxy of Open Source solutions. Open Source means that every user can access the source code of the programs (open source is not necessarily free of charge). Linux is mostly an OS kernel. It exposes computer capabilities (computing, memory access, and other devices) to developers. The C/C++ programming languages were developed for the Linux kernel. Distribution refers to an OS kernel combined with software management (packages), a GUI (display server + desktop environment), and various software tools. Typically, ROS is designed to work on Ubuntu , which includes the following: a Linux kernel , GNOME desktop environment, the aptitude ( apt ) package manager. Ubuntu is a great example of an Open Source project: It is based on another Open Source project: Debian OS , leveraging the Debian community\u2019s work while offering a more user-friendly solution. It aims for profitability through Ubuntu Pro . It is used as a base for other Linux-based OSs, such as Pop!_OS and Zorin . Visit distrowatch.com to explore the interconnected galaxy of Linux OSs . The Shell or Terminal Among the variety of programs running on Linux, we are particularly interested in the terminal emulator, which allows direct manipulation of the system via commands (navigating the directory tree, reading and organizing files, executing scripts or programs, managing system resources such as the network, etc.). On Ubuntu, the default terminal emulator is gnome-terminal ( help.gnome.org ), configured with the bash ( on Wikipedia ) command interpreter. Commands are interpretable instructions that generally execute programs. The following commands are expected to be known before starting with ROS . Note: Tab enables autocompletion. Explore the following commands (i.e., their purpose and usage), then practice using them in your terminal. The File System ls : List directory elements. cd : Change directory. cat : Display the content of a text file. more : Read a text file page by page. touch : Create a file (if absent). nano : Edit a text file. rm : Permanently remove a file. mkdir : Create a new directory. rmdir : Remove a directory. mv : Move or rename a resource. cp : Copy a file. clear : Clear the shell screen. Path manipulation: / : Root directory in a Linux system - Example: du /bin/bash (du estimates file space usage). . : Current directory - Example: mkdir ./workspace . .. : Parent directory - Example: cd ../../media . ~ : User home directory - Example: cd ~/Documents . CApitAl Letters maTer - Example: cd ~/Documents \u2260 cd ~/documents . Mastering Commands The classical syntax is: command --option -o argument1 - Example: ls -a ~ . Commands exist to help master other commands: man : Command and library manual - Example: man cp . apropos : Search manuals - Example: apropos search . whereis : Locate a resource - Example: whereis python3 . alias : Create a custom command - Example: alias listall=\"ls -a\" . Sessions and Environment A session is an active connection to a system or program. who : List active sessions on a computer. ssh : Open a session on a remote computer using the ssh ( Secure Shell ) protocol. An environment is the system configuration in which a program runs. ( Wikipedia Sept. 2023 ). env : List environment variables. $ : Access a variable - Example: echo $PATH . export : Create a new variable - Example: export PATH=$PATH:~/bin . ~/.bashrc : User run-command configuration file. User, Group, and Rules Users are identified and grouped. Users and groups have names and IDs. Resources are owned by users and groups. Access permissions can be configured for resources: r : Read, w : Write, x : Execute/Open. Permissions apply to the user , group , and others . The ls -l command lists the contents of a directory with ownership and permissions - Example: ls -l /etc . To list all users: cat /etc/passwd (for groups, cat /etc/group ). Commands: chmod : Change file permissions. Example: chmod +x aFile (add execute permission to aFile ). Example: chmod 752 aFile (set permissions in binary style). chown : Change ownership. Managing Processes Processes are running instances of programs. Key commands: ps : Lists processes. Example: ps (list local processes started by the current shell). Example: ps -e (list all processes). gedit : launches a graphical text editor. You should use gedit & to get back to the prompt and send the gedit process to the background. top : Interactive process monitoring ( Q to quit). kill : Sends signals to processes. Example: kill 19482 (send TERM signal to process 19482). Example: kill -s KILL 19482 (send KILL signal to process 19482). Processes have attributes like a PID (Process Identifier) and a parent process. Some Bash Tools and Shortcuts Bash is one of the shell solutions available on Unix systems. Tab : Auto-complete the command or list possible options. !xx : Re-run the last command starting with xx . Ctrl-R : Search for a command in history. Q : Quit a running program. Ctrl-C : Terminate a running program. ~/.bashrc : User-specific shell configuration file. For more, see ( Wikipedia ).","title":"OS and Shell"},{"location":"tuto-kick-off/basics/#operating-system-linuxubuntu-for-ros2","text":"ROS stands for R obot O perating S ystem. However, it extends a classical O perating S ystem, specifically a Linux/Ubuntu OS. The goal of this tutorial is to revisit basic O perating S ystem concepts, focusing mainly on the Shell , which offers the simplest way to interact with your computer. For an introduction to operating systems, read the Wikipedia page .","title":"Operating System: Linux/Ubuntu for ROS2"},{"location":"tuto-kick-off/basics/#play-with-linux","text":"Ubuntu is a Linux distribution (Operating System + Window Management + Tools) derived from the Debian distribution, within the galaxy of Open Source solutions. Open Source means that every user can access the source code of the programs (open source is not necessarily free of charge). Linux is mostly an OS kernel. It exposes computer capabilities (computing, memory access, and other devices) to developers. The C/C++ programming languages were developed for the Linux kernel. Distribution refers to an OS kernel combined with software management (packages), a GUI (display server + desktop environment), and various software tools. Typically, ROS is designed to work on Ubuntu , which includes the following: a Linux kernel , GNOME desktop environment, the aptitude ( apt ) package manager. Ubuntu is a great example of an Open Source project: It is based on another Open Source project: Debian OS , leveraging the Debian community\u2019s work while offering a more user-friendly solution. It aims for profitability through Ubuntu Pro . It is used as a base for other Linux-based OSs, such as Pop!_OS and Zorin . Visit distrowatch.com to explore the interconnected galaxy of Linux OSs .","title":"Play with Linux"},{"location":"tuto-kick-off/basics/#the-shell-or-terminal","text":"Among the variety of programs running on Linux, we are particularly interested in the terminal emulator, which allows direct manipulation of the system via commands (navigating the directory tree, reading and organizing files, executing scripts or programs, managing system resources such as the network, etc.). On Ubuntu, the default terminal emulator is gnome-terminal ( help.gnome.org ), configured with the bash ( on Wikipedia ) command interpreter. Commands are interpretable instructions that generally execute programs. The following commands are expected to be known before starting with ROS . Note: Tab enables autocompletion. Explore the following commands (i.e., their purpose and usage), then practice using them in your terminal.","title":"The Shell or Terminal"},{"location":"tuto-kick-off/basics/#the-file-system","text":"ls : List directory elements. cd : Change directory. cat : Display the content of a text file. more : Read a text file page by page. touch : Create a file (if absent). nano : Edit a text file. rm : Permanently remove a file. mkdir : Create a new directory. rmdir : Remove a directory. mv : Move or rename a resource. cp : Copy a file. clear : Clear the shell screen. Path manipulation: / : Root directory in a Linux system - Example: du /bin/bash (du estimates file space usage). . : Current directory - Example: mkdir ./workspace . .. : Parent directory - Example: cd ../../media . ~ : User home directory - Example: cd ~/Documents . CApitAl Letters maTer - Example: cd ~/Documents \u2260 cd ~/documents .","title":"The File System"},{"location":"tuto-kick-off/basics/#mastering-commands","text":"The classical syntax is: command --option -o argument1 - Example: ls -a ~ . Commands exist to help master other commands: man : Command and library manual - Example: man cp . apropos : Search manuals - Example: apropos search . whereis : Locate a resource - Example: whereis python3 . alias : Create a custom command - Example: alias listall=\"ls -a\" .","title":"Mastering Commands"},{"location":"tuto-kick-off/basics/#sessions-and-environment","text":"A session is an active connection to a system or program. who : List active sessions on a computer. ssh : Open a session on a remote computer using the ssh ( Secure Shell ) protocol. An environment is the system configuration in which a program runs. ( Wikipedia Sept. 2023 ). env : List environment variables. $ : Access a variable - Example: echo $PATH . export : Create a new variable - Example: export PATH=$PATH:~/bin . ~/.bashrc : User run-command configuration file.","title":"Sessions and Environment"},{"location":"tuto-kick-off/basics/#user-group-and-rules","text":"Users are identified and grouped. Users and groups have names and IDs. Resources are owned by users and groups. Access permissions can be configured for resources: r : Read, w : Write, x : Execute/Open. Permissions apply to the user , group , and others . The ls -l command lists the contents of a directory with ownership and permissions - Example: ls -l /etc . To list all users: cat /etc/passwd (for groups, cat /etc/group ). Commands: chmod : Change file permissions. Example: chmod +x aFile (add execute permission to aFile ). Example: chmod 752 aFile (set permissions in binary style). chown : Change ownership.","title":"User, Group, and Rules"},{"location":"tuto-kick-off/basics/#managing-processes","text":"Processes are running instances of programs. Key commands: ps : Lists processes. Example: ps (list local processes started by the current shell). Example: ps -e (list all processes). gedit : launches a graphical text editor. You should use gedit & to get back to the prompt and send the gedit process to the background. top : Interactive process monitoring ( Q to quit). kill : Sends signals to processes. Example: kill 19482 (send TERM signal to process 19482). Example: kill -s KILL 19482 (send KILL signal to process 19482). Processes have attributes like a PID (Process Identifier) and a parent process.","title":"Managing Processes"},{"location":"tuto-kick-off/basics/#some-bash-tools-and-shortcuts","text":"Bash is one of the shell solutions available on Unix systems. Tab : Auto-complete the command or list possible options. !xx : Re-run the last command starting with xx . Ctrl-R : Search for a command in history. Q : Quit a running program. Ctrl-C : Terminate a running program. ~/.bashrc : User-specific shell configuration file. For more, see ( Wikipedia ).","title":"Some Bash Tools and Shortcuts"},{"location":"tuto-kick-off/first-contact/","text":"ROS Basics The goal of this tutorial is to set up a workspace for all the resources required to install and develop as part of our journey to control robots. We will work with ROS (the most widely used open middleware for robotics) on a Linux-Ubuntu computer, which is the best-supported operating system for ROS . This tutorial assumes you are using an Ubuntu-like 22.04 computer with configured Python and C++ APIs. It is based on ROS2 Iron . The wiki.ros.org website primarily refers to ROS1 . While ROS1 and ROS2 share similarities, there are significant differences between them. Be cautious when interpreting search results on the internet, as they might refer to ROS1 instead of ROS2 . Nodes and Topics Robots are complex cybernetic systems that require complex software to handle complex missions. That for, the main feature proposed by ROS is to develop programs in a modular way. Pieces of programs are dedicated to atomic functionalities and the overall control program is composed of several interconnected pieces. A piece of program is called node and nodes exchange data by communicating through topics . This tutorial part relies directly on ROS documentation docs.ros.org . Configuring environment : You should well understand how to configure your Linux environment First contact : Then, you can play with the hello world example on ROS2 . Notice that sudo commands suppose you have administrator privilege on the computer Understanding nodes Understanding topics At this point you should be comfortable with the core component of ROS , nodes and topics. You realized that you will interact a lot with your software architecture throuh your terminal. So a good practice consists of turning your terminal prompt to clearly state your ROS configuration at any time... Add the following 2 lines into your ~/.bashrc file. The bash run command file ( ~/.bashrc ) is a good tool to configure all your terminal with common environment variables and configurations. # Tunned prompt: PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:($ROS_AUTOMATIC_DISCOVERY_RANGE::$ROS_DOMAIN_ID)\\[\\033[01;34m\\]\\w\\[\\033[00m\\].\\n\\$ ' Python API ROS include more elements, components like services and actions , features like parameters or tools to debug ( rqt_console ), visualize ( rviz2 ), launch configurations of nodes (launch) or recording topics ( bag ). But let see the Python API ( Application Programming Interface ) on nodes and topics first, before to visit other ROS elements. Notice that the purpose of this lecture and its tutorials is not to cover all ROS elements. Furthermore, the tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. Other solutions are provided by ROS2 . Setup a Dev. Environment First, you should work on a dedicated directory to regroup the ROS things you need and you develop, let say your ros_space or ros_ws . The things would generally be packages but we will see that later. In this workspace, we will work for now on a playground directory and mode specifically on ROS talker and listener python script. To set up our environment in a terminal: cd # Go to the home directory of the current user. mkdir ros_space # Create a directory cd ros_space # Enter it mkdir playground # Create a directory touch playground/talker.py # Create a file (if it does not exist) touch playground/listener.py # Create a file (if it does not exist) We recommend working with a complete extensible IDE, typically Visual Studio Code . And open the entire ros_space as a project, plus a terminal inside the IDE. At this point, your IDE should have \\(3\\) areas: the explorer on the left side, a text editor on top of a terminal . The explorer should contain the playground directory with your \\(2\\) python scripts inside. Talker: a simple topic publisher The goal of our first program is to send a message on a topic. For that, we need the ROS client Python package ( rclpy ) for initializing ROS and creating nodes, and all the packages defining all the types of messages the program should manipulate (only simple string message on our example). At the end the first hello world program will look like: #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String def oneTalk(): # Initialize ROS client rclpy.init() # Create a node aNode= Node( \"simpleTalker\" ) # Attach a publisher to the node, with a specific type, the name of the topic, a history depth aPublisher= aNode.create_publisher( String, 'testTopic', 10 ) # Create a message to send msg = String() msg.data = 'Hello World' # Add the message to the list of messages to publish aPublisher.publish(msg) # Activate the ROS client with the node # (that will publish the message on testTopic topic) rclpy.spin_once(aNode, timeout_sec= 10.0) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() # Execute the function. if __name__ == \"__main__\": oneTalk() Run the solution on a first terminal. python3 playground/talker.py Listen to the topic in another terminal (you have to run again the talker to see a message...). ros2 topic echo testTopic The talker node need to be active when echo testTopic is started otherwise ros2 topic end because no node is connected to testTopic . Also, the String message description (ie. composed by a unique data attribute) come from index.ros.org , aside from the other standard message type of std_msgs . The main ROS packages defining message types are grouped on common_interfaces with, for example geometry_msgs for objects defined on the Cartesian coordinate system, visualization_msgs focused on message types for visualization or sensor_msgs , well, for sensor-based data. Continuous Talker In a second version, we aim to create a node publishing continuously messages. For that purpose, we use an Event-Driven Programming style. Technically in ROS, it is managed by the ros client, with the spin_once or spin functions. These \\(2\\) functions connect events (the reception of a message for instance) with the appropriate callback. spin_once activates once the ros client and wait for the next event. It should be called in an infinite loop. spin handle the infinite loop directly (and should be preferred). For the talker, you have to generate events with timers: aNode.create_timer( duration, aCallback ) The call back function aCallBack will be called after a duration is terminated. Furthermore, the call-back functions generally require a context of execution. Some already defined and accessible elements (for instance in our case, the publisher). For that purpose, we implement our aCallBack as a method of a class. The new talker script will lookalike: def infiniteTalk(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"infTalker\" ) talker= ROSTalker() talker.initializeROSNode( aNode ) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSTalker: def __init__(self): self._i = 0 def initializeROSNode(self, rosNode): self._publisher= rosNode.create_publisher( String, 'testTopic', 10 ) self._timer = rosNode.create_timer(0.5, self.timer_callback) return self def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self._i self._publisher.publish(msg) self._i += 1 # Execute the function. if __name__ == \"__main__\": infiniteTalk() Now python3 playground/talker.py wil start a ROS node publishing \\(2\\) hello messages per second. A Ctr-C in the terminal should stop the process. Listener: a simple topic subcriber In a very similar way, listener node is implemented with topics subscription mechanisms associated with a callback. #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String def listen(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"listener\" ) listener= ROSListener() listener.initializeROSNode( aNode ) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSListener(): def initializeROSNode(self, rosNode): self._logger= rosNode.get_logger() self._subscription= rosNode.create_subscription( String, 'testTopic', self.listener_callback, 10 ) def listener_callback(self, msg): self._logger.info( 'I heard: ' + msg.data ) if __name__ == '__main__': listen() Run your listener as you run your talker. The \\(2\\) nodes work together and define a very simple first software architecture. Logs Notice that ROS provides its own, \" print \" function with a logger instance. The logger, attached to a node instance ( aRosNode.get_logger() ), records logs through different channels: info , debug , warning or error . More on docs.ros.org . Logs can be followed with rqt_console tool. Follow the official tutorial to learn how to use this tool.","title":"Nodes and Topics"},{"location":"tuto-kick-off/first-contact/#ros-basics","text":"The goal of this tutorial is to set up a workspace for all the resources required to install and develop as part of our journey to control robots. We will work with ROS (the most widely used open middleware for robotics) on a Linux-Ubuntu computer, which is the best-supported operating system for ROS . This tutorial assumes you are using an Ubuntu-like 22.04 computer with configured Python and C++ APIs. It is based on ROS2 Iron . The wiki.ros.org website primarily refers to ROS1 . While ROS1 and ROS2 share similarities, there are significant differences between them. Be cautious when interpreting search results on the internet, as they might refer to ROS1 instead of ROS2 .","title":"ROS Basics"},{"location":"tuto-kick-off/first-contact/#nodes-and-topics","text":"Robots are complex cybernetic systems that require complex software to handle complex missions. That for, the main feature proposed by ROS is to develop programs in a modular way. Pieces of programs are dedicated to atomic functionalities and the overall control program is composed of several interconnected pieces. A piece of program is called node and nodes exchange data by communicating through topics . This tutorial part relies directly on ROS documentation docs.ros.org . Configuring environment : You should well understand how to configure your Linux environment First contact : Then, you can play with the hello world example on ROS2 . Notice that sudo commands suppose you have administrator privilege on the computer Understanding nodes Understanding topics At this point you should be comfortable with the core component of ROS , nodes and topics. You realized that you will interact a lot with your software architecture throuh your terminal. So a good practice consists of turning your terminal prompt to clearly state your ROS configuration at any time... Add the following 2 lines into your ~/.bashrc file. The bash run command file ( ~/.bashrc ) is a good tool to configure all your terminal with common environment variables and configurations. # Tunned prompt: PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:($ROS_AUTOMATIC_DISCOVERY_RANGE::$ROS_DOMAIN_ID)\\[\\033[01;34m\\]\\w\\[\\033[00m\\].\\n\\$ '","title":"Nodes and Topics"},{"location":"tuto-kick-off/first-contact/#python-api","text":"ROS include more elements, components like services and actions , features like parameters or tools to debug ( rqt_console ), visualize ( rviz2 ), launch configurations of nodes (launch) or recording topics ( bag ). But let see the Python API ( Application Programming Interface ) on nodes and topics first, before to visit other ROS elements. Notice that the purpose of this lecture and its tutorials is not to cover all ROS elements. Furthermore, the tutorials are designed with ament_cmake build tool, mostly python node and yaml launch files. Other solutions are provided by ROS2 .","title":"Python API"},{"location":"tuto-kick-off/first-contact/#setup-a-dev-environment","text":"First, you should work on a dedicated directory to regroup the ROS things you need and you develop, let say your ros_space or ros_ws . The things would generally be packages but we will see that later. In this workspace, we will work for now on a playground directory and mode specifically on ROS talker and listener python script. To set up our environment in a terminal: cd # Go to the home directory of the current user. mkdir ros_space # Create a directory cd ros_space # Enter it mkdir playground # Create a directory touch playground/talker.py # Create a file (if it does not exist) touch playground/listener.py # Create a file (if it does not exist) We recommend working with a complete extensible IDE, typically Visual Studio Code . And open the entire ros_space as a project, plus a terminal inside the IDE. At this point, your IDE should have \\(3\\) areas: the explorer on the left side, a text editor on top of a terminal . The explorer should contain the playground directory with your \\(2\\) python scripts inside.","title":"Setup a Dev. Environment"},{"location":"tuto-kick-off/first-contact/#talker-a-simple-topic-publisher","text":"The goal of our first program is to send a message on a topic. For that, we need the ROS client Python package ( rclpy ) for initializing ROS and creating nodes, and all the packages defining all the types of messages the program should manipulate (only simple string message on our example). At the end the first hello world program will look like: #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String def oneTalk(): # Initialize ROS client rclpy.init() # Create a node aNode= Node( \"simpleTalker\" ) # Attach a publisher to the node, with a specific type, the name of the topic, a history depth aPublisher= aNode.create_publisher( String, 'testTopic', 10 ) # Create a message to send msg = String() msg.data = 'Hello World' # Add the message to the list of messages to publish aPublisher.publish(msg) # Activate the ROS client with the node # (that will publish the message on testTopic topic) rclpy.spin_once(aNode, timeout_sec= 10.0) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() # Execute the function. if __name__ == \"__main__\": oneTalk() Run the solution on a first terminal. python3 playground/talker.py Listen to the topic in another terminal (you have to run again the talker to see a message...). ros2 topic echo testTopic The talker node need to be active when echo testTopic is started otherwise ros2 topic end because no node is connected to testTopic . Also, the String message description (ie. composed by a unique data attribute) come from index.ros.org , aside from the other standard message type of std_msgs . The main ROS packages defining message types are grouped on common_interfaces with, for example geometry_msgs for objects defined on the Cartesian coordinate system, visualization_msgs focused on message types for visualization or sensor_msgs , well, for sensor-based data.","title":"Talker: a simple topic publisher"},{"location":"tuto-kick-off/first-contact/#continuous-talker","text":"In a second version, we aim to create a node publishing continuously messages. For that purpose, we use an Event-Driven Programming style. Technically in ROS, it is managed by the ros client, with the spin_once or spin functions. These \\(2\\) functions connect events (the reception of a message for instance) with the appropriate callback. spin_once activates once the ros client and wait for the next event. It should be called in an infinite loop. spin handle the infinite loop directly (and should be preferred). For the talker, you have to generate events with timers: aNode.create_timer( duration, aCallback ) The call back function aCallBack will be called after a duration is terminated. Furthermore, the call-back functions generally require a context of execution. Some already defined and accessible elements (for instance in our case, the publisher). For that purpose, we implement our aCallBack as a method of a class. The new talker script will lookalike: def infiniteTalk(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"infTalker\" ) talker= ROSTalker() talker.initializeROSNode( aNode ) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSTalker: def __init__(self): self._i = 0 def initializeROSNode(self, rosNode): self._publisher= rosNode.create_publisher( String, 'testTopic', 10 ) self._timer = rosNode.create_timer(0.5, self.timer_callback) return self def timer_callback(self): msg = String() msg.data = 'Hello World: %d' % self._i self._publisher.publish(msg) self._i += 1 # Execute the function. if __name__ == \"__main__\": infiniteTalk() Now python3 playground/talker.py wil start a ROS node publishing \\(2\\) hello messages per second. A Ctr-C in the terminal should stop the process.","title":"Continuous Talker"},{"location":"tuto-kick-off/first-contact/#listener-a-simple-topic-subcriber","text":"In a very similar way, listener node is implemented with topics subscription mechanisms associated with a callback. #!/usr/bin/python3 import rclpy from rclpy.node import Node from std_msgs.msg import String def listen(): # Initialize ROS node with ROS client rclpy.init() aNode= Node( \"listener\" ) listener= ROSListener() listener.initializeROSNode( aNode ) # Start infinite loop rclpy.spin(aNode) # Clean everything and switch the light off aNode.destroy_node() rclpy.shutdown() class ROSListener(): def initializeROSNode(self, rosNode): self._logger= rosNode.get_logger() self._subscription= rosNode.create_subscription( String, 'testTopic', self.listener_callback, 10 ) def listener_callback(self, msg): self._logger.info( 'I heard: ' + msg.data ) if __name__ == '__main__': listen() Run your listener as you run your talker. The \\(2\\) nodes work together and define a very simple first software architecture.","title":"Listener: a simple topic subcriber"},{"location":"tuto-kick-off/first-contact/#logs","text":"Notice that ROS provides its own, \" print \" function with a logger instance. The logger, attached to a node instance ( aRosNode.get_logger() ), records logs through different channels: info , debug , warning or error . More on docs.ros.org . Logs can be followed with rqt_console tool. Follow the official tutorial to learn how to use this tool.","title":"Logs"},{"location":"tuto-kick-off/move/","text":"Move a robot This tutorial aims to take control of a pibot robot. A pibot is a turtlebot2 robot (based on a kobuki platform) equipped with a laser range to navigate in a cluttered environment and a Rapberry Pi3 to provide a ROS2 interface to the robot. Connect the pibot : The Rapberry Pi3 on each pibot are pre-configured with Ubuntu Server 22.04 , ROS2 Iron , and all the necessary drivers for the robot. By default, the pibot connects to the IoT WiFi network . The robot's control nodes are automatically launched at pibot startup via the mb6-tbot service, which executes the minimal_launch.yaml launch file from a tbot_node package. (More details about the IMT driver packages for turtlebot2 on github ) To avoid any conflict in mesages, each pibot has its own ROS_DOMAIN_ID . ROS environment has to be configured proprely to interact with a robot. For instance for the pibot20: export ROS_AUTOMATIC_DISCOVERY_RANGE=SUBNET export ROS_DOMAIN_ID=20 ros2 node list ros2 topic list Reminder: You can also explore the existing node with rqt_graph . Finally, you can try to take control in an available terminal with keyboard control: ros2 run teleop_twist_keyboard teleop_twist_keyboard cmd_vel:=/multi/cmd_teleop Close everything with ctrl-c . The teleop publishes a geometry_msgs twist message. It is composed of two vectors \\((x, y, z)\\) , one for linear speed \\((m/s)\\) , and the second for angular speed \\((rad/s)\\) . However a nonholonomic ground robot as the tbot would move only on x and turn only on z (It is not as free as a drone). Try to control the robot with ros2 topic pub command publishing in the navigation topic ( /multi/cmd_nav ). Tbot drivers integrate a subsumption multiplexer. The node listens different topics with different priorities (by default: /multi/cmd_nav and /multi/cmd_telop ) and filter the appropriate commands to send to the robot. The topics cmd_nav and cmd_telop stend for autonomous navigation and operator teleoperate. The human operator has a higher priority. Publishing messages into cmd_telop makes the multiplexer to trash the cmd_nav commands. A dedicated node to control The goal is to create a process connecting a topic and publishing velocities as a twist message: This tutorial supposes that you already perform the previous package tutorial and that you have a first package tutorial_pkg to work on it. First we have to create a file for our script. touch tutorial_pkg/scripts/test_move The script state that it requires python3 interpreter and it depends on several ROS2 resources. #!/usr/bin/python3 import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist print(\"test_move :: START...\") You can try that everything is correct by turning the script file executable and execute it: In a shell: chmod +x tutorial_pkg/scripts/test_move ./tutorial_pkg/scripts/test_move Finally, we must declare our node as one of the package node into CMakeList.txt and build again your packages. You can also add geometry_msgs as dependency on the package.xml file. colcon build source ./install/local_setup.bash Then, our node should be accessible from ros2 commands. ros2 run tutorial_pkg test_move A Simple Move order The node we aim to generate is similar to talker , except that we publish a twist message on the appropriate topic. The callback method of our class would be def timer_callback(self): velocity = Twist() # Feed Twist velocity values ... # Publish self._publisher.publish(velocity) A look at the geometry_msgs at index.ros.org Inform that Twist is composed by \\(2\\) Vector3 attributs also in geometry_msgs . Velocities are in \\(m.s^{-1}\\) and \\(\\mathit{rad}.s^{-1}\\) . Now, a ros2 run tutorial_pkg test_move should continuously move the robot, with a constant speed. Notice that you can also test your code on turtlesim by changing the name of the topic. Move Script At the end we want our robot to follow a predefined choreography. Something like moving forward for \\(X \\mathit{cm}\\) backward, turn left then turn right, etc. The choreography should end on the exact position it begins to perform an infinite loop safely.","title":"Move the Robot"},{"location":"tuto-kick-off/move/#move-a-robot","text":"This tutorial aims to take control of a pibot robot. A pibot is a turtlebot2 robot (based on a kobuki platform) equipped with a laser range to navigate in a cluttered environment and a Rapberry Pi3 to provide a ROS2 interface to the robot.","title":"Move a robot"},{"location":"tuto-kick-off/move/#connect-the-pibot","text":"The Rapberry Pi3 on each pibot are pre-configured with Ubuntu Server 22.04 , ROS2 Iron , and all the necessary drivers for the robot. By default, the pibot connects to the IoT WiFi network . The robot's control nodes are automatically launched at pibot startup via the mb6-tbot service, which executes the minimal_launch.yaml launch file from a tbot_node package. (More details about the IMT driver packages for turtlebot2 on github ) To avoid any conflict in mesages, each pibot has its own ROS_DOMAIN_ID . ROS environment has to be configured proprely to interact with a robot. For instance for the pibot20: export ROS_AUTOMATIC_DISCOVERY_RANGE=SUBNET export ROS_DOMAIN_ID=20 ros2 node list ros2 topic list Reminder: You can also explore the existing node with rqt_graph . Finally, you can try to take control in an available terminal with keyboard control: ros2 run teleop_twist_keyboard teleop_twist_keyboard cmd_vel:=/multi/cmd_teleop Close everything with ctrl-c . The teleop publishes a geometry_msgs twist message. It is composed of two vectors \\((x, y, z)\\) , one for linear speed \\((m/s)\\) , and the second for angular speed \\((rad/s)\\) . However a nonholonomic ground robot as the tbot would move only on x and turn only on z (It is not as free as a drone). Try to control the robot with ros2 topic pub command publishing in the navigation topic ( /multi/cmd_nav ). Tbot drivers integrate a subsumption multiplexer. The node listens different topics with different priorities (by default: /multi/cmd_nav and /multi/cmd_telop ) and filter the appropriate commands to send to the robot. The topics cmd_nav and cmd_telop stend for autonomous navigation and operator teleoperate. The human operator has a higher priority. Publishing messages into cmd_telop makes the multiplexer to trash the cmd_nav commands.","title":"Connect the pibot:"},{"location":"tuto-kick-off/move/#a-dedicated-node-to-control","text":"The goal is to create a process connecting a topic and publishing velocities as a twist message: This tutorial supposes that you already perform the previous package tutorial and that you have a first package tutorial_pkg to work on it. First we have to create a file for our script. touch tutorial_pkg/scripts/test_move The script state that it requires python3 interpreter and it depends on several ROS2 resources. #!/usr/bin/python3 import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist print(\"test_move :: START...\") You can try that everything is correct by turning the script file executable and execute it: In a shell: chmod +x tutorial_pkg/scripts/test_move ./tutorial_pkg/scripts/test_move Finally, we must declare our node as one of the package node into CMakeList.txt and build again your packages. You can also add geometry_msgs as dependency on the package.xml file. colcon build source ./install/local_setup.bash Then, our node should be accessible from ros2 commands. ros2 run tutorial_pkg test_move","title":"A dedicated node to control"},{"location":"tuto-kick-off/move/#a-simple-move-order","text":"The node we aim to generate is similar to talker , except that we publish a twist message on the appropriate topic. The callback method of our class would be def timer_callback(self): velocity = Twist() # Feed Twist velocity values ... # Publish self._publisher.publish(velocity) A look at the geometry_msgs at index.ros.org Inform that Twist is composed by \\(2\\) Vector3 attributs also in geometry_msgs . Velocities are in \\(m.s^{-1}\\) and \\(\\mathit{rad}.s^{-1}\\) . Now, a ros2 run tutorial_pkg test_move should continuously move the robot, with a constant speed. Notice that you can also test your code on turtlesim by changing the name of the topic.","title":"A Simple Move order"},{"location":"tuto-kick-off/move/#move-script","text":"At the end we want our robot to follow a predefined choreography. Something like moving forward for \\(X \\mathit{cm}\\) backward, turn left then turn right, etc. The choreography should end on the exact position it begins to perform an infinite loop safely.","title":"Move Script"},{"location":"tuto-kick-off/package/","text":"ROS Packages A ROS package regroups node definitions and other resources into a same location. It relies on colcon tool to build, install and manage the package dependancies. Documention on colcon.readthedocs.io . The colcon command colcon is a meta command with several sub-command like ros2 or git . Its main sub-command is build . The colcon build command searches for all compliant packages included in a directory tree, at any depth. It should be processes at the root directory, so in your ros_space . cd ~/ros_space colcon build ls The build process generates three directories: build with temporaly generated files by the build process ; install with generated ressources of the packages ; log for logs. At this time, there is no packages on your workspace, the build , install and log directories are almost empty. The results of colcon list (listing packages in the directory tree) or colcon graph (list with dependancies) are also empty. colcon list colcon graph Create a new package A ROS package interpreted by colcon build is simplely a directory with configurations files. There is \\(2\\) files by using cmake build tool. package.xml : describing the package and its dependancies CMakeLists.txt : describing the build process (based on famous cmake tool) mkdir a-package touch a-package/package.xml touch a-package/CMakeLists.txt Naturally the package.xml and the CMakeLists.txt should include minimal informations in a correct way. rm -fr a-package colcon build ROS2 , with the pkg sub-command, provide a tool to generate the directory structure of a new package. Let create a tutorial_pkg package for instance. ros2 pkg create --build-type ament_cmake tutorial_pkg colcon list colcon build Configuration files The package.xml provides metadata on the package (title, description, author licence). It should be manually completed. Its also give the build tool to use (a variation of cmake named ament-cmake in our exemple) and permit to specify dependancies with the mark <depend> . Typically, our new tutorial_pkg should depend from rclpy and std_msgs package and lines <depend>rclpy</depend> and <depend>std_msgs</depend> need to be added after the license information. To learn more about XML format: www.w3schools.com/xml Open the CMakeLists.txt file. At this step, it is almost empty. It only include information about the project to build, the C/C++ compiler to use, and some element relative to ament tool trigering action relative to ROS working environement. This file need to be update every time a new ressource (node or other) is added to the package project. Python Based Nodes For convenient reason, our python scripts, coding new ros nodes, will be saved on a scripts directory. Typically, we can install there your talker and listerner from previous tutorial. mkdir tutorial_pkg/scripts cp playground/talker.py tutorial_pkg/scripts/talker cp playground/listener.py tutorial_pkg/scripts/listener Also, we need to inform which interpreter can process our script (ie python3). Add a #!/usr/bin/python3 at the first line in talker and listerner scripts. #! is a shebang meaning that the rest of the line is used to determine the program to interpret the content of the current file. /usr/bin/python3 is simplely the result of the command whereis python3 . Next, we have to modify CMakeLists.txt to state that talker and listerner should be installed as program (ie. in the appropriate destination to make them reachable by ros2 command). So add a python scripts section to your CMakeLists.txt file: # Python scripts install( PROGRAMS scripts/talker DESTINATION lib/${PROJECT_NAME} ) install( PROGRAMS scripts/listener DESTINATION lib/${PROJECT_NAME} ) You can now build again your ros workspace. The install directory contain a tutorial_pkg with every think=gs inside. colcon build ls install/ ls install/tutorial_pkg/lib/tutorial_pkg/ To use your package with ros2 commands, you have to update your bash environment. Then it would be possible to start your node with ros2 run command. source ./install/local_setup.bash ros2 run tutorial_pkg talker #or listener Notice that, if your python node script relies on a local python package ( myLocalPkg for instance). This one should be install aside of your script with the following cmake instruction: install( DIRECTORY scripts/myLocalPkg DESTINATION lib/${PROJECT_NAME} ) Launch file ROS proposes a launch mechanism to start in one command a configuration of several nodes. Launch file can be defined with markup language ( XML or YAML ) or python3 for more complex launch scenarios. Yaml provides the simplest syntax to write launch files. The yaml.org gives an example of a yaml resources. Similarly to Python , it relies on indentation to mark the ownership of elements. Another example: aMark: aMarkedAttribute: \"a string value\" aSecondAttribut: [\"a\", \"list\", \"of\", \"string\", \"element\"] 'a third attribute': - 1 - 2 - 12 again: anotherMark: aMarkedAttribute: 36.8 ... ROS yaml relies on predefinite marks, key works: 'launch' as the first element and composed by a list of node s. Minimal configuration for a node include pkg and exec attributes to identify the node to start. A simple launch file for talker/listener will be: launch: - node: pkg: \"tutorial_pkg\" exec: \"talker\" - node: pkg: \"tutorial_pkg\" exec: \"listener\" The file has to be set on a launch directory into your package and with a name ending by _launch.yaml , converse_launch.yaml for instance. At this point the launch file can be run using ros2 launch commands. ros2 launch ./tutorial_pkg/launch/converse_launch.yaml By adding launch resources to your package with CMakeList.txt configuration file, you make launch files easier to find. In CMakeList.txt file: # Install resource files. install(DIRECTORY launch DESTINATION share/${PROJECT_NAME}/ ) Then in the terminal: colcon build ros2 launch tutorial_pkg converse_launch.yaml Notice that other resources can be added to your package. Typically rviz for visualization configuration, world for simulation configuration, etc. # Install resource files. install(DIRECTORY launch rviz world DESTINATION share/${PROJECT_NAME}/ )","title":"ROS Package"},{"location":"tuto-kick-off/package/#ros-packages","text":"A ROS package regroups node definitions and other resources into a same location. It relies on colcon tool to build, install and manage the package dependancies. Documention on colcon.readthedocs.io .","title":"ROS Packages"},{"location":"tuto-kick-off/package/#the-colcon-command","text":"colcon is a meta command with several sub-command like ros2 or git . Its main sub-command is build . The colcon build command searches for all compliant packages included in a directory tree, at any depth. It should be processes at the root directory, so in your ros_space . cd ~/ros_space colcon build ls The build process generates three directories: build with temporaly generated files by the build process ; install with generated ressources of the packages ; log for logs. At this time, there is no packages on your workspace, the build , install and log directories are almost empty. The results of colcon list (listing packages in the directory tree) or colcon graph (list with dependancies) are also empty. colcon list colcon graph","title":"The colcon command"},{"location":"tuto-kick-off/package/#create-a-new-package","text":"A ROS package interpreted by colcon build is simplely a directory with configurations files. There is \\(2\\) files by using cmake build tool. package.xml : describing the package and its dependancies CMakeLists.txt : describing the build process (based on famous cmake tool) mkdir a-package touch a-package/package.xml touch a-package/CMakeLists.txt Naturally the package.xml and the CMakeLists.txt should include minimal informations in a correct way. rm -fr a-package colcon build ROS2 , with the pkg sub-command, provide a tool to generate the directory structure of a new package. Let create a tutorial_pkg package for instance. ros2 pkg create --build-type ament_cmake tutorial_pkg colcon list colcon build","title":"Create a new package"},{"location":"tuto-kick-off/package/#configuration-files","text":"The package.xml provides metadata on the package (title, description, author licence). It should be manually completed. Its also give the build tool to use (a variation of cmake named ament-cmake in our exemple) and permit to specify dependancies with the mark <depend> . Typically, our new tutorial_pkg should depend from rclpy and std_msgs package and lines <depend>rclpy</depend> and <depend>std_msgs</depend> need to be added after the license information. To learn more about XML format: www.w3schools.com/xml Open the CMakeLists.txt file. At this step, it is almost empty. It only include information about the project to build, the C/C++ compiler to use, and some element relative to ament tool trigering action relative to ROS working environement. This file need to be update every time a new ressource (node or other) is added to the package project.","title":"Configuration files"},{"location":"tuto-kick-off/package/#python-based-nodes","text":"For convenient reason, our python scripts, coding new ros nodes, will be saved on a scripts directory. Typically, we can install there your talker and listerner from previous tutorial. mkdir tutorial_pkg/scripts cp playground/talker.py tutorial_pkg/scripts/talker cp playground/listener.py tutorial_pkg/scripts/listener Also, we need to inform which interpreter can process our script (ie python3). Add a #!/usr/bin/python3 at the first line in talker and listerner scripts. #! is a shebang meaning that the rest of the line is used to determine the program to interpret the content of the current file. /usr/bin/python3 is simplely the result of the command whereis python3 . Next, we have to modify CMakeLists.txt to state that talker and listerner should be installed as program (ie. in the appropriate destination to make them reachable by ros2 command). So add a python scripts section to your CMakeLists.txt file: # Python scripts install( PROGRAMS scripts/talker DESTINATION lib/${PROJECT_NAME} ) install( PROGRAMS scripts/listener DESTINATION lib/${PROJECT_NAME} ) You can now build again your ros workspace. The install directory contain a tutorial_pkg with every think=gs inside. colcon build ls install/ ls install/tutorial_pkg/lib/tutorial_pkg/ To use your package with ros2 commands, you have to update your bash environment. Then it would be possible to start your node with ros2 run command. source ./install/local_setup.bash ros2 run tutorial_pkg talker #or listener Notice that, if your python node script relies on a local python package ( myLocalPkg for instance). This one should be install aside of your script with the following cmake instruction: install( DIRECTORY scripts/myLocalPkg DESTINATION lib/${PROJECT_NAME} )","title":"Python Based Nodes"},{"location":"tuto-kick-off/package/#launch-file","text":"ROS proposes a launch mechanism to start in one command a configuration of several nodes. Launch file can be defined with markup language ( XML or YAML ) or python3 for more complex launch scenarios. Yaml provides the simplest syntax to write launch files. The yaml.org gives an example of a yaml resources. Similarly to Python , it relies on indentation to mark the ownership of elements. Another example: aMark: aMarkedAttribute: \"a string value\" aSecondAttribut: [\"a\", \"list\", \"of\", \"string\", \"element\"] 'a third attribute': - 1 - 2 - 12 again: anotherMark: aMarkedAttribute: 36.8 ... ROS yaml relies on predefinite marks, key works: 'launch' as the first element and composed by a list of node s. Minimal configuration for a node include pkg and exec attributes to identify the node to start. A simple launch file for talker/listener will be: launch: - node: pkg: \"tutorial_pkg\" exec: \"talker\" - node: pkg: \"tutorial_pkg\" exec: \"listener\" The file has to be set on a launch directory into your package and with a name ending by _launch.yaml , converse_launch.yaml for instance. At this point the launch file can be run using ros2 launch commands. ros2 launch ./tutorial_pkg/launch/converse_launch.yaml By adding launch resources to your package with CMakeList.txt configuration file, you make launch files easier to find. In CMakeList.txt file: # Install resource files. install(DIRECTORY launch DESTINATION share/${PROJECT_NAME}/ ) Then in the terminal: colcon build ros2 launch tutorial_pkg converse_launch.yaml Notice that other resources can be added to your package. Typically rviz for visualization configuration, world for simulation configuration, etc. # Install resource files. install(DIRECTORY launch rviz world DESTINATION share/${PROJECT_NAME}/ )","title":"Launch file"},{"location":"tuto-kick-off/simulation/","text":"Simulation in ROS Simulating robotic systems is crucial before doing real experiments. It exsits several simulators: https://gazebosim.org/home https://www.coppeliarobotics.com https://carla.org https://github.com/osrf/vrx In this tutorial, we will use two of them: Stage (2d) and Gazebo (3d). Stage Simulator Installing Stage Here the commands: sudo apt install -y git cmake g++ libjpeg8-dev libpng-dev libglu1-mesa-dev libltdl-dev libfltk1.1-dev mkdir pkg-stage cd pkg-stage git clone --branch ros2 https://github.com/tuw-robotics/Stage.git git clone --branch humble https://github.com/tuw-robotics/stage_ros2.git cd .. colcon build --cmake-args -DOpenGL_GL_PREFERENCE=LEGACY colcon build --packages-select stage_ros2 First Launch & Exercices ros2 launch stage_ros2 stage.launch.py world:=cave Note: by pressing 'r' you can see a 3d view of the scene in stage. Question : What are the topics published by stage? Exercise : Launch rviz2 and display the robot position and its laser scan data Teleoperate the robot using the keyboard by running: ros2 run teleop_twist_keyboard teleop_twist_keyboard Ensure that the robot is correctly moving in stage and that the laser data are correctly displayed in rviz2. Question : Why the robot is moving in the simulator? Gazebo Simulator Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI systems using realistic scenarios. Gazebo is integrated with ROS and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available. Gazebo Installation Verify that Gazebo is installed using: dpkg -l | grep gazebo You should have at least the following packages: ii gazebo 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Binaries ii gazebo-common 11.10.2+dfsg-1 all Open Source Robotics Simulator - Shared files ii gazebo-plugin-base 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo-dev 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Development Files ii libgazebo11:amd64 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - shared library ii ros-iron-gazebo-dev 3.7.0-3jammy.20230622.191804 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.7.0-3jammy.20231117.090251 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.7.0-3jammy.20231117.111548 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.7.0-3jammy.20231117.104944 amd64 Utilities to interface with Gazebo through ROS. ii ros-iron-gazebo-ros-pkgs 3.7.0-3jammy.20231117.114324 amd64 Interface for using ROS with the Gazebo simulator. ii ros-iron-turtlebot3-gazebo 2.2.5-4jammy.20231117.114359 amd64 Gazebo simulation package for the TurtleBot3 Install missing packages using: sudo apt install <pakage_name> Launch your first Gazebo Simulation We propose some configuration into a pkg-tsim project, including a tbot_sim ROS2 package. cd ~/ros_space git clone https://github.com/imt-mobisyst/pkg-tsim colcon build source ./install/setup.bash Then, you can launch a preconfigured simulation: ros2 launch tbot_sim challenge-1.launch.py Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera (kinect). The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipment. Quiz on challenge 1 While the challenge 1 simulation is running: Question: which topics are available i.e published by Gazebo? Hint: an infinite loop safely` is a versatile and powerful tool to display data published in topics. Launch it: rviz2 Question: How to configure rviz2 to visualize the laser scans? Be careful, ensure that Global Option / Fixed frame is correctly set to base_link . Question: why is this important? (hint: check your tf using ros2 run tf2_tools view_frames ) You can also display the tf in rviz2 directly. Question: How to visualize camera images using rqt ? Controlling the Simulated Robot Launch a simple node to control the simulated robot using keyboard: ros2 run teleop_twist_keyboard teleop_twist_keyboard tuto_sim Create a launch file that starts the appropriate configuration: the challenge-1 , a configured rviz2 displaying laser scans and the teleop . We will prefer YAML format for launch file. All the information you need are in the tutorials : - Launch file documentation - Launch examples Create $ROS_WORKSPACE/pkg-tsim/tbot_sim/launch/tutosim_launch.yaml Add this code into this file to: launch: - include: file: \"$(find-pkg-share tbot_sim)/launch/challenge-1.launch.py\" - node: pkg: \"rviz2\" exec: \"rviz2\" name: \"rviz2\" - executable: cmd: gnome-terminal --tab -e 'ros2 run teleop_twist_keyboard teleop_twist_keyboard' Exercise: modify this launch file so that rviz loads a saved configuration file when it starts. This configuration file should add the laser data, ...","title":"Simulation"},{"location":"tuto-kick-off/simulation/#simulation-in-ros","text":"Simulating robotic systems is crucial before doing real experiments. It exsits several simulators: https://gazebosim.org/home https://www.coppeliarobotics.com https://carla.org https://github.com/osrf/vrx In this tutorial, we will use two of them: Stage (2d) and Gazebo (3d).","title":"Simulation in ROS"},{"location":"tuto-kick-off/simulation/#stage-simulator","text":"","title":"Stage Simulator"},{"location":"tuto-kick-off/simulation/#installing-stage","text":"Here the commands: sudo apt install -y git cmake g++ libjpeg8-dev libpng-dev libglu1-mesa-dev libltdl-dev libfltk1.1-dev mkdir pkg-stage cd pkg-stage git clone --branch ros2 https://github.com/tuw-robotics/Stage.git git clone --branch humble https://github.com/tuw-robotics/stage_ros2.git cd .. colcon build --cmake-args -DOpenGL_GL_PREFERENCE=LEGACY colcon build --packages-select stage_ros2","title":"Installing Stage"},{"location":"tuto-kick-off/simulation/#first-launch-exercices","text":"ros2 launch stage_ros2 stage.launch.py world:=cave Note: by pressing 'r' you can see a 3d view of the scene in stage. Question : What are the topics published by stage? Exercise : Launch rviz2 and display the robot position and its laser scan data Teleoperate the robot using the keyboard by running: ros2 run teleop_twist_keyboard teleop_twist_keyboard Ensure that the robot is correctly moving in stage and that the laser data are correctly displayed in rviz2. Question : Why the robot is moving in the simulator?","title":"First Launch &amp; Exercices"},{"location":"tuto-kick-off/simulation/#gazebo-simulator","text":"Gazebo is a 3D simulator. It makes it possible to rapidly test algorithms, design robots, perform regression testing, and train AI systems using realistic scenarios. Gazebo is integrated with ROS and supports various robots out of the box. Gazebo is heavily used by the DARPA challenges (cf. Wikipedia ). You can see videos online ( example ) and even load the maps and robot model that are available.","title":"Gazebo Simulator"},{"location":"tuto-kick-off/simulation/#gazebo-installation","text":"Verify that Gazebo is installed using: dpkg -l | grep gazebo You should have at least the following packages: ii gazebo 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Binaries ii gazebo-common 11.10.2+dfsg-1 all Open Source Robotics Simulator - Shared files ii gazebo-plugin-base 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - base plug-ins ii libgazebo-dev 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - Development Files ii libgazebo11:amd64 11.10.2+dfsg-1 amd64 Open Source Robotics Simulator - shared library ii ros-iron-gazebo-dev 3.7.0-3jammy.20230622.191804 amd64 Provides a cmake config for the default version of Gazebo for the ROS distribution. ii ros-iron-gazebo-msgs 3.7.0-3jammy.20231117.090251 amd64 Message and service data structures for interacting with Gazebo from ROS2. ii ros-iron-gazebo-plugins 3.7.0-3jammy.20231117.111548 amd64 Robot-independent Gazebo plugins for sensors, motors and dynamic reconfigurable components. ii ros-iron-gazebo-ros 3.7.0-3jammy.20231117.104944 amd64 Utilities to interface with Gazebo through ROS. ii ros-iron-gazebo-ros-pkgs 3.7.0-3jammy.20231117.114324 amd64 Interface for using ROS with the Gazebo simulator. ii ros-iron-turtlebot3-gazebo 2.2.5-4jammy.20231117.114359 amd64 Gazebo simulation package for the TurtleBot3 Install missing packages using: sudo apt install <pakage_name>","title":"Gazebo Installation"},{"location":"tuto-kick-off/simulation/#launch-your-first-gazebo-simulation","text":"We propose some configuration into a pkg-tsim project, including a tbot_sim ROS2 package. cd ~/ros_space git clone https://github.com/imt-mobisyst/pkg-tsim colcon build source ./install/setup.bash Then, you can launch a preconfigured simulation: ros2 launch tbot_sim challenge-1.launch.py Look at the content of this launch file here . We can see that Gazebo/ROS supports loading a world file describing the simulation environment and spawn elements such as robots. This simulation spawns a robot configured like a tbot i.e. it is equipped with a laser range finder and a camera (kinect). The interaction with the simulation will operate through ROS topics as it would be with a real robot with real equipment.","title":"Launch your first Gazebo Simulation"},{"location":"tuto-kick-off/simulation/#quiz-on-challenge-1","text":"While the challenge 1 simulation is running: Question: which topics are available i.e published by Gazebo? Hint: an infinite loop safely` is a versatile and powerful tool to display data published in topics. Launch it: rviz2 Question: How to configure rviz2 to visualize the laser scans? Be careful, ensure that Global Option / Fixed frame is correctly set to base_link . Question: why is this important? (hint: check your tf using ros2 run tf2_tools view_frames ) You can also display the tf in rviz2 directly. Question: How to visualize camera images using rqt ?","title":"Quiz on challenge 1"},{"location":"tuto-kick-off/simulation/#controlling-the-simulated-robot","text":"Launch a simple node to control the simulated robot using keyboard: ros2 run teleop_twist_keyboard teleop_twist_keyboard","title":"Controlling the Simulated Robot"},{"location":"tuto-kick-off/simulation/#tuto_sim","text":"Create a launch file that starts the appropriate configuration: the challenge-1 , a configured rviz2 displaying laser scans and the teleop . We will prefer YAML format for launch file. All the information you need are in the tutorials : - Launch file documentation - Launch examples Create $ROS_WORKSPACE/pkg-tsim/tbot_sim/launch/tutosim_launch.yaml Add this code into this file to: launch: - include: file: \"$(find-pkg-share tbot_sim)/launch/challenge-1.launch.py\" - node: pkg: \"rviz2\" exec: \"rviz2\" name: \"rviz2\" - executable: cmd: gnome-terminal --tab -e 'ros2 run teleop_twist_keyboard teleop_twist_keyboard' Exercise: modify this launch file so that rviz loads a saved configuration file when it starts. This configuration file should add the laser data, ...","title":"tuto_sim"},{"location":"tuto-level-up/camera-driver/","text":"Camera Driver This tutorial cover the basis of the integration of a new sensor in ROS 2 environment. In our case we will integrate Realsense D400 RGBDi camera. Drivers First, the integration of a sensor require to identify the driver (the piece of code permiting to communicate with a devices - hardware level) and the API (Application Programming interface). Concretly, we mostly seek for the appropriate librairies correctly integrated to our system. Id\u00e9aly the community already support the desired librairies (like for libsdl2 for instance, simple C lib \"to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware\"). By searching for libsdl2 with Ubuntu-Aptitude we will find several packages ready to be installed: apt search libsdl2 libsdl2-x.x runing librairies (installed if programs use SDL2 ) libsdl2-dev development file (to install if you plan to develop a program based on SDL2 ) and some extra libs. Other wise, we have to build/compile the driver from source code. In case of realsense, librealsense recommand to use vcpkg to build and install it. Normally, after installation, you can run a small script to request the cam (more on dev.intelrealsense.com ): #!/usr/bin/env python3 ############################################### ## Simple Request ## ############################################### import pyrealsense2 as rs # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) Copy the code on a test-camera.py file and process it ( python3 test-camera.py ). Try this script with severals cameras, not all the Realsense provide for IMU (accelerations) information. OpenCV2 - the queen of the vision librairie. Next we can try to visualise the image flux in a windows. For that we will use OpenCV2 librairy (an open source computer vision library). The next script, adapted from the oficial documentation, connect the camera, and display both the image and distance image in an infinite loop ( while True ). Based on librealsense, the script activates the expected data flux, with the wanted configuration (848x480 imagein a given format at 60 Hertz): config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) Start the aquisition process ( pipeline.start(config) ) Still with librealsense, the script wait for incomming data and get them: frames = pipeline.wait_for_frames() depth_frame = frames.first(rs.stream.depth) color_frame = frames.first(rs.stream.color) Then, the reminder of the script consists in converting and displaying the data based on Numpy and OpenCV #!/usr/bin/env python3 ## Doc: https://dev.intelrealsense.com/docs/python2 ############################################### ## Open CV and Numpy integration ## ############################################### import pyrealsense2 as rs import signal, time, numpy as np import sys, cv2, rclpy # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) found_rgb = True for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) if s.get_info(rs.camera_info.name) == 'RGB Camera': found_rgb = True if not (found_rgb): print(\"Depth camera equired !!!\") exit(0) config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) # Capture ctrl-c event isOk= True def signalInteruption(signum, frame): global isOk print( \"\\nCtrl-c pressed\" ) isOk= False signal.signal(signal.SIGINT, signalInteruption) # Start streaming pipeline.start(config) count= 1 refTime= time.process_time() freq= 60 sys.stdout.write(\"-\") while isOk: # Wait for a coherent tuple of frames: depth, color and accel frames = pipeline.wait_for_frames() color_frame = frames.first(rs.stream.color) depth_frame = frames.first(rs.stream.depth) if not (depth_frame and color_frame): continue # Convert images to numpy arrays depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # Apply colormap on depth image (image must be converted to 8-bit per pixel first) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape sys.stdout.write( f\"\\r- {color_colormap_dim} - {depth_colormap_dim} - ({round(freq)} fps)\" ) # Show images images = np.hstack((color_image, depth_colormap)) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 # Stop streaming print(\"\\nEnding...\") pipeline.stop() To notice the use of signal python librairy that permit to catch and process interuption signal ( ctrl-c ). Publish sensor-data Stating from the previous script, the goal is to encapsulated the connection to the camera into a ROS2 Node in a tuto_vision package (only the camera image flux). Considering previous developed ROS2 Node : We want to keep the control on the infinite loop. We will publish sensor_msgs/image . The ros2 documentation is verry poor, but ROS1 wiki remains valuable. Node structure: To control the infinite loop we will prefer spin_once to spin . Notice that spin_once process once the ROS2 instructions. It blocks until an event occurs. It is possible to overpass that by specifying a timeout ( spin_once(myNode, timeout_sec=0.01) ). At the end we want a ROS2 Node that connect the camera and publish continously the images (color and depth images). The python function of the Node will look like: # Node processes: def mainFunction(): # Initialize ROS Node connected to the camera rclpy.init( signal_handler_options= SignalHandlerOptions.NO ) rosNode= Node( \"RealSense_driver\" ) camera= Realsense() camera.initializeROSNode( rosNode ) # Start infinite loop while isOk and rclpy.ok() : camera.read_imgs() camera.publish_imgs() rclpy.spin_once(rosNode, timeout_sec=0.001) # Clean end camera.disconnect() rosNode.destroy_node() rclpy.shutdown() To notice that, we aims to control the program loop and its possible interruption. So, at ROS initialization, we desactivate the signal handler mecanism by using SignalHandlerOptions ( from rclpy.signals import SignalHandlerOptions ). This solution relies on a Realsense class to create. # Realsense Node: class Realsense(): def __init__(self, fps= 60): # Initialize attributes # Connect the camera pass def initializeROSNode( self, aROSNode ): # Initialize publishers / subscribers and timers pass def read_imgs(self): # Read data from camera pass def publish_imgs(self): # Send data into ROS Topics pass def disconnect(self): # disconnect the camera, free the resource. pass Finally, do not forget to execute the main function... # Execute the main function. if __name__ == \"__main__\": mainFunction() Realsense class conponents will be filled mostlly by the element from the previous script. Starting from the blanc class Realsence , you can fill the differents methods step by steps (testing your code at each step). Publish Images: sensor_msgs include header to state for spacio-temporal information. Mainly the reference frame (ie. cam for instance) and time. For time stamp, get_clock() permits to get a clock of a Node instance ( node= Node() or self in case of ineritance) then now() and to_msg() methods respectivelly provide curent time() and convert it into a msg compliant format. msg.header.stamp = node.get_clock().now().to_msg() Then it is possible to feed sensor_msgs/image attributs (starting with msg.encoding= \"bgr8\" seems a good idea.) However, a librairy provides some tool to work both with ROS and OpenCV ( cv_bridge ). The code for image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() msg_image = self.bridge.cv2_to_imgmsg(color_image,\"bgr8\") msg_image.header.stamp = self.get_clock().now().to_msg() msg_image.header.frame_id = \"image\" self.image_publisher.publish(msg_image) The code for depth image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() # Utilisation de colormap sur l'image depth de la Realsense (image convertie en 8-bit par pixel) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,\"bgr8\") msg_depth.header.stamp = msg_image.header.stamp msg_depth.header.frame_id = \"depth\" self.depth_publisher.publish(msg_depth) Some test: At this point it is not relevant any more to show the images inside a CV2 window or to compute and print a frequency. The frequency can be conputed with ROS2 tool: ros2 topic hz \\img . The images are displayable into rviz2 program. Going futher: Play with the other streams provided with the RealSens sensor. Code to add both infrared channels self.config.enable_stream(rs.stream.infrared, 1, 848, 480, rs.format.y8, 60) self.config.enable_stream(rs.stream.infrared, 2, 848, 480, rs.format.y8, 60) self.infra_publisher_1 = self.create_publisher(Image, 'infrared_1',10) self.infra_publisher_2 = self.create_publisher(Image, 'infrared_2',10) infra_image_1 = np.asanyarray(infra_frame_1.get_data()) infra_image_2 = np.asanyarray(infra_frame_2.get_data()) in the loop : infra_frame_1 = frames.get_infrared_frame(1) infra_frame_2 = frames.get_infrared_frame(2) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_1 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_1, alpha=0.03), cv2.COLORMAP_JET) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_2 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_2, alpha=0.03), cv2.COLORMAP_JET) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_1,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_1\" self.infra_publisher_1.publish(msg_infra) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_2,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_2\" self.infra_publisher_2.publish(msg_infra)","title":"Camera Driver"},{"location":"tuto-level-up/camera-driver/#camera-driver","text":"This tutorial cover the basis of the integration of a new sensor in ROS 2 environment. In our case we will integrate Realsense D400 RGBDi camera.","title":"Camera Driver"},{"location":"tuto-level-up/camera-driver/#drivers","text":"First, the integration of a sensor require to identify the driver (the piece of code permiting to communicate with a devices - hardware level) and the API (Application Programming interface). Concretly, we mostly seek for the appropriate librairies correctly integrated to our system. Id\u00e9aly the community already support the desired librairies (like for libsdl2 for instance, simple C lib \"to provide low level access to audio, keyboard, mouse, joystick, and graphics hardware\"). By searching for libsdl2 with Ubuntu-Aptitude we will find several packages ready to be installed: apt search libsdl2 libsdl2-x.x runing librairies (installed if programs use SDL2 ) libsdl2-dev development file (to install if you plan to develop a program based on SDL2 ) and some extra libs. Other wise, we have to build/compile the driver from source code. In case of realsense, librealsense recommand to use vcpkg to build and install it. Normally, after installation, you can run a small script to request the cam (more on dev.intelrealsense.com ): #!/usr/bin/env python3 ############################################### ## Simple Request ## ############################################### import pyrealsense2 as rs # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) Copy the code on a test-camera.py file and process it ( python3 test-camera.py ). Try this script with severals cameras, not all the Realsense provide for IMU (accelerations) information.","title":"Drivers"},{"location":"tuto-level-up/camera-driver/#opencv2-the-queen-of-the-vision-librairie","text":"Next we can try to visualise the image flux in a windows. For that we will use OpenCV2 librairy (an open source computer vision library). The next script, adapted from the oficial documentation, connect the camera, and display both the image and distance image in an infinite loop ( while True ). Based on librealsense, the script activates the expected data flux, with the wanted configuration (848x480 imagein a given format at 60 Hertz): config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) Start the aquisition process ( pipeline.start(config) ) Still with librealsense, the script wait for incomming data and get them: frames = pipeline.wait_for_frames() depth_frame = frames.first(rs.stream.depth) color_frame = frames.first(rs.stream.color) Then, the reminder of the script consists in converting and displaying the data based on Numpy and OpenCV #!/usr/bin/env python3 ## Doc: https://dev.intelrealsense.com/docs/python2 ############################################### ## Open CV and Numpy integration ## ############################################### import pyrealsense2 as rs import signal, time, numpy as np import sys, cv2, rclpy # Configure depth and color streams pipeline = rs.pipeline() config = rs.config() # Get device product line for setting a supporting resolution pipeline_wrapper = rs.pipeline_wrapper(pipeline) pipeline_profile = config.resolve(pipeline_wrapper) device = pipeline_profile.get_device() device_product_line = str(device.get_info(rs.camera_info.product_line)) print( f\"Connect: {device_product_line}\" ) found_rgb = True for s in device.sensors: print( \"Name:\" + s.get_info(rs.camera_info.name) ) if s.get_info(rs.camera_info.name) == 'RGB Camera': found_rgb = True if not (found_rgb): print(\"Depth camera equired !!!\") exit(0) config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60) config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60) # Capture ctrl-c event isOk= True def signalInteruption(signum, frame): global isOk print( \"\\nCtrl-c pressed\" ) isOk= False signal.signal(signal.SIGINT, signalInteruption) # Start streaming pipeline.start(config) count= 1 refTime= time.process_time() freq= 60 sys.stdout.write(\"-\") while isOk: # Wait for a coherent tuple of frames: depth, color and accel frames = pipeline.wait_for_frames() color_frame = frames.first(rs.stream.color) depth_frame = frames.first(rs.stream.depth) if not (depth_frame and color_frame): continue # Convert images to numpy arrays depth_image = np.asanyarray(depth_frame.get_data()) color_image = np.asanyarray(color_frame.get_data()) # Apply colormap on depth image (image must be converted to 8-bit per pixel first) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape sys.stdout.write( f\"\\r- {color_colormap_dim} - {depth_colormap_dim} - ({round(freq)} fps)\" ) # Show images images = np.hstack((color_image, depth_colormap)) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 # Stop streaming print(\"\\nEnding...\") pipeline.stop() To notice the use of signal python librairy that permit to catch and process interuption signal ( ctrl-c ).","title":"OpenCV2 - the queen of the vision librairie."},{"location":"tuto-level-up/camera-driver/#publish-sensor-data","text":"Stating from the previous script, the goal is to encapsulated the connection to the camera into a ROS2 Node in a tuto_vision package (only the camera image flux). Considering previous developed ROS2 Node : We want to keep the control on the infinite loop. We will publish sensor_msgs/image . The ros2 documentation is verry poor, but ROS1 wiki remains valuable.","title":"Publish sensor-data"},{"location":"tuto-level-up/camera-driver/#node-structure","text":"To control the infinite loop we will prefer spin_once to spin . Notice that spin_once process once the ROS2 instructions. It blocks until an event occurs. It is possible to overpass that by specifying a timeout ( spin_once(myNode, timeout_sec=0.01) ). At the end we want a ROS2 Node that connect the camera and publish continously the images (color and depth images). The python function of the Node will look like: # Node processes: def mainFunction(): # Initialize ROS Node connected to the camera rclpy.init( signal_handler_options= SignalHandlerOptions.NO ) rosNode= Node( \"RealSense_driver\" ) camera= Realsense() camera.initializeROSNode( rosNode ) # Start infinite loop while isOk and rclpy.ok() : camera.read_imgs() camera.publish_imgs() rclpy.spin_once(rosNode, timeout_sec=0.001) # Clean end camera.disconnect() rosNode.destroy_node() rclpy.shutdown() To notice that, we aims to control the program loop and its possible interruption. So, at ROS initialization, we desactivate the signal handler mecanism by using SignalHandlerOptions ( from rclpy.signals import SignalHandlerOptions ). This solution relies on a Realsense class to create. # Realsense Node: class Realsense(): def __init__(self, fps= 60): # Initialize attributes # Connect the camera pass def initializeROSNode( self, aROSNode ): # Initialize publishers / subscribers and timers pass def read_imgs(self): # Read data from camera pass def publish_imgs(self): # Send data into ROS Topics pass def disconnect(self): # disconnect the camera, free the resource. pass Finally, do not forget to execute the main function... # Execute the main function. if __name__ == \"__main__\": mainFunction() Realsense class conponents will be filled mostlly by the element from the previous script. Starting from the blanc class Realsence , you can fill the differents methods step by steps (testing your code at each step).","title":"Node structure:"},{"location":"tuto-level-up/camera-driver/#publish-images","text":"sensor_msgs include header to state for spacio-temporal information. Mainly the reference frame (ie. cam for instance) and time. For time stamp, get_clock() permits to get a clock of a Node instance ( node= Node() or self in case of ineritance) then now() and to_msg() methods respectivelly provide curent time() and convert it into a msg compliant format. msg.header.stamp = node.get_clock().now().to_msg() Then it is possible to feed sensor_msgs/image attributs (starting with msg.encoding= \"bgr8\" seems a good idea.) However, a librairy provides some tool to work both with ROS and OpenCV ( cv_bridge ). The code for image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() msg_image = self.bridge.cv2_to_imgmsg(color_image,\"bgr8\") msg_image.header.stamp = self.get_clock().now().to_msg() msg_image.header.frame_id = \"image\" self.image_publisher.publish(msg_image) The code for depth image to ROS message is: from cv_bridge import CvBridge self.bridge=CvBridge() # Utilisation de colormap sur l'image depth de la Realsense (image convertie en 8-bit par pixel) depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,\"bgr8\") msg_depth.header.stamp = msg_image.header.stamp msg_depth.header.frame_id = \"depth\" self.depth_publisher.publish(msg_depth)","title":"Publish Images:"},{"location":"tuto-level-up/camera-driver/#some-test","text":"At this point it is not relevant any more to show the images inside a CV2 window or to compute and print a frequency. The frequency can be conputed with ROS2 tool: ros2 topic hz \\img . The images are displayable into rviz2 program.","title":"Some test:"},{"location":"tuto-level-up/camera-driver/#going-futher","text":"Play with the other streams provided with the RealSens sensor. Code to add both infrared channels self.config.enable_stream(rs.stream.infrared, 1, 848, 480, rs.format.y8, 60) self.config.enable_stream(rs.stream.infrared, 2, 848, 480, rs.format.y8, 60) self.infra_publisher_1 = self.create_publisher(Image, 'infrared_1',10) self.infra_publisher_2 = self.create_publisher(Image, 'infrared_2',10) infra_image_1 = np.asanyarray(infra_frame_1.get_data()) infra_image_2 = np.asanyarray(infra_frame_2.get_data()) in the loop : infra_frame_1 = frames.get_infrared_frame(1) infra_frame_2 = frames.get_infrared_frame(2) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_1 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_1, alpha=0.03), cv2.COLORMAP_JET) # Utilisation de colormap sur l'image infrared de la Realsense (image convertie en 8-bit par pixel) infra_colormap_2 = cv2.applyColorMap(cv2.convertScaleAbs(infra_image_2, alpha=0.03), cv2.COLORMAP_JET) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_1,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_1\" self.infra_publisher_1.publish(msg_infra) msg_infra = self.bridge.cv2_to_imgmsg(infra_colormap_2,\"bgr8\") msg_infra.header.stamp = msg_image.header.stamp msg_infra.header.frame_id = \"infrared_2\" self.infra_publisher_2.publish(msg_infra)","title":"Going futher:"},{"location":"tuto-level-up/parameters/","text":"Handle ROS parameters Node piece of programs accepts the argument to be parametrized, speed, name of the topic, frequencies or whatever. This tutorial visits how ROS2 handle parameters... But first, let see how to read Python3 script arguments. Python3 arguments It is very common to add arguments in the command line while starting a program or a script. Most of the Linux commands accept arguments. In python, the arguments can be read with the sys python package, on a argv list (argument values). lets initialize a test-script.py import sys print( sys.argv ) and try: python3 test-script.py -r salut nounou 42 You notice that the executed script itself is the first argument (at indice \\(0\\) ). Use ROS2 Parameters ROS2 provides parameter management on top of command arguments. Typically, it is possible to list the parameters with ros2 param list command. Start a turtlesim in a terminal an search for trutlesim parameters in another. To activate parameters (and topic remapping) ros2 run command needs --ros-args attributes, then the syntax is -p parameter_name:=value . For instance, to set a beautiful orange on the turtlesim background: ros2 run turtlesim turtlesim_node --ros-args \\ -p background_r:=100 \\ -p background_g:=60 \\ -p background_b:=0 In YAML launch file parameters should be defined on a param list. launch: - node: pkg: turtlesim exec: turtlesim_node name: orange_simu param: - { name: \"background_r\", value: 100 } - { name: \"background_g\", value: 60 } - { name: \"background_b\", value: 0 } Python3 Integration The ROS2 client API (Python3 as C++) provides tools to declare and read ROS2 parameters value. declare_parameter method attached to a Node instance, declare a ROS variable. On a simple usage, it takes 2 parameters: The variable name to use. A default value (ATTENTION, ROS parameter has type, so \\(10\\) is different of \\(10.0\\) ) get_parameter( 'variable_name' ) returning a parameter instance, with aParameter.value the value. (To notice that a aParameter.get_parameter_value() exists and returns an object containing the parameter value on diferent possible types: [bool_value, integer_value, double_value, string_value, byte_array_value, bool_array_value, integer_array_value, double_array_value, string_array_value] ) For instance, a node initialization with an integer parameters lookalike: # Initialize ROS node with ROS client rclpy.init() rosNode= Node( \"myOneParameterNode\" ) # Node Parameters : rosNode.declare_parameter( 'my_parameter', 10 ) aProgramVariable= rosNode.get_parameter( 'my_parameter' ).value print( f\"> Parameter value: {aProgramVariable}\" ) # Start infinite loop rclpy.spin(rosNode) # Clean everything and switch the light off rosNode.destroy_node() rclpy.shutdown() Add this node to your tutorial_pkg and test it. # Shell-1 ros2 run tutorial_pkg oneParamNode # Shell-2 ros2 param list # Shell-1 ros2 run tutorial_pkg oneParamNode --ros-args -p my_parameter:=42 # Shell-1 ros2 run tutorial_pkg oneParamNode --ros-args -p my_parameter:=\"42\" # Shell-1 ros2 run tutorial_pkg oneParamNode --ros-args -p my_parameter:=\"quarante-deux\" Implements ROS parameters to set-up the message to be sent by your talker node (cf. Node and Topic tutorial). Ideally, the node parameter is read each times, before to publish the message to guaranty to sent the last message in case of parameter modification. # With Node: 'myTalker' and parameter 'message' ros2 param set myTalker message \"New important information to broadcast...\"","title":"Parameters"},{"location":"tuto-level-up/parameters/#handle-ros-parameters","text":"Node piece of programs accepts the argument to be parametrized, speed, name of the topic, frequencies or whatever. This tutorial visits how ROS2 handle parameters... But first, let see how to read Python3 script arguments.","title":"Handle ROS parameters"},{"location":"tuto-level-up/parameters/#python3-arguments","text":"It is very common to add arguments in the command line while starting a program or a script. Most of the Linux commands accept arguments. In python, the arguments can be read with the sys python package, on a argv list (argument values). lets initialize a test-script.py import sys print( sys.argv ) and try: python3 test-script.py -r salut nounou 42 You notice that the executed script itself is the first argument (at indice \\(0\\) ).","title":"Python3 arguments"},{"location":"tuto-level-up/parameters/#use-ros2-parameters","text":"ROS2 provides parameter management on top of command arguments. Typically, it is possible to list the parameters with ros2 param list command. Start a turtlesim in a terminal an search for trutlesim parameters in another. To activate parameters (and topic remapping) ros2 run command needs --ros-args attributes, then the syntax is -p parameter_name:=value . For instance, to set a beautiful orange on the turtlesim background: ros2 run turtlesim turtlesim_node --ros-args \\ -p background_r:=100 \\ -p background_g:=60 \\ -p background_b:=0 In YAML launch file parameters should be defined on a param list. launch: - node: pkg: turtlesim exec: turtlesim_node name: orange_simu param: - { name: \"background_r\", value: 100 } - { name: \"background_g\", value: 60 } - { name: \"background_b\", value: 0 }","title":"Use ROS2 Parameters"},{"location":"tuto-level-up/parameters/#python3-integration","text":"The ROS2 client API (Python3 as C++) provides tools to declare and read ROS2 parameters value. declare_parameter method attached to a Node instance, declare a ROS variable. On a simple usage, it takes 2 parameters: The variable name to use. A default value (ATTENTION, ROS parameter has type, so \\(10\\) is different of \\(10.0\\) ) get_parameter( 'variable_name' ) returning a parameter instance, with aParameter.value the value. (To notice that a aParameter.get_parameter_value() exists and returns an object containing the parameter value on diferent possible types: [bool_value, integer_value, double_value, string_value, byte_array_value, bool_array_value, integer_array_value, double_array_value, string_array_value] ) For instance, a node initialization with an integer parameters lookalike: # Initialize ROS node with ROS client rclpy.init() rosNode= Node( \"myOneParameterNode\" ) # Node Parameters : rosNode.declare_parameter( 'my_parameter', 10 ) aProgramVariable= rosNode.get_parameter( 'my_parameter' ).value print( f\"> Parameter value: {aProgramVariable}\" ) # Start infinite loop rclpy.spin(rosNode) # Clean everything and switch the light off rosNode.destroy_node() rclpy.shutdown() Add this node to your tutorial_pkg and test it. # Shell-1 ros2 run tutorial_pkg oneParamNode # Shell-2 ros2 param list # Shell-1 ros2 run tutorial_pkg oneParamNode --ros-args -p my_parameter:=42 # Shell-1 ros2 run tutorial_pkg oneParamNode --ros-args -p my_parameter:=\"42\" # Shell-1 ros2 run tutorial_pkg oneParamNode --ros-args -p my_parameter:=\"quarante-deux\" Implements ROS parameters to set-up the message to be sent by your talker node (cf. Node and Topic tutorial). Ideally, the node parameter is read each times, before to publish the message to guaranty to sent the last message in case of parameter modification. # With Node: 'myTalker' and parameter 'message' ros2 param set myTalker message \"New important information to broadcast...\"","title":"Python3 Integration"},{"location":"tuto-level-up/range-sensor/","text":"Range sensor Range sensors are robot sensor permitting to detect obstacles and determine a distance to it. Basic range sensors (infrared, ultrasonic, laser) produce a unique measure considering a given direction at a time. By making the sensor rotating, it is possible to get measurements on a plan around the sensor. Hokuhyo, equipping the Tbot , is typically a kind of rotating lidar sensor ( l aser i maging or li ght d etection a nd r anging). The goal here is to integrate an almost 360 obstacle detection to generate safe robot movement. More complex lidar permits 3D measurements (i.e. in several plans at a time). Read Scan Data The pibots already run urg_node_driver ros node. This node publish laser scan into a \\scan topic. It is possible to check it with ros2 topic list and ros2 topic echo scan . Gazebo simulator also simulate laser scan, but the scan topic and/or the laser-scan frame can have different names. ros2 launch tbot_sim challenge-1.launch.py Now you can visualize it on rviz2 program. Start rviz2 , add a flux laserScan and configure it in /scan topic. Nothing appears and it is normal. Rviz2 global option is configured on map frame, and nothing permits to set the position of the laser sensor in the map. The laser-scan frame is named laser . Change this information into global options and set the laser-scan size to 0,1 for a better display. A first node logging the scan First, we will initialize a node scan_echo to print the scan data into a terminal. Edit a new script file scan_echo en configure your package to recognize it as a ROS2 node. Test your scan_echo node: ros2 run tutorial_pkg scan_echo Connect sensor_msgs LaserScan . Log continuously the received data #!python3 import rclpy from rclpy.node import Node from sensor_msgs.msg import LaserScan rosNode= None def scan_callback(scanMsg): global rosNode rosNode.get_logger().info( f\"scan:\\n{scanMsg}\" ) rclpy.init() rosNode= Node('scan_interpreter') rosNode.create_subscription( LaserScan, 'scan', scan_callback, 10) rclpy.spin( rosNode ) scanInterpret.destroy_node() rclpy.shutdown() Test your scan_echo node, with the pibot and on simulations. You can notice that ros_logger print information into info channel (informative), but other channel exits for errors and warnings. At this point your echo node print the entire scan message, but we do not care about the distance measurements, we want only some meta-data. Modify the logger to print the information into the header and the number of ranges. From LaserScan to PointCloud LaserScan provides both the recorded bean distances (ranges) and the meta information permitting converting the distances on points in a regular Cartesian frame (i.e. the angle between beans). In a python, the conversion would look like this: obstacles= [] angle= scanMsg.angle_min for aDistance in scanMsg.ranges : if 0.1 < aDistance and aDistance < 5.0 : aPoint= [ math.cos(angle) * aDistance, math.sin(angle) * aDistance ] obstacles.append(aPoint) angle+= scanMsg.angle_increment The exercise consists in modifying the scan callback function to generate the point cloud list. To log a sample of the point cloud: sample= [ [ round(p[0], 2), round(p[1], 2) ] for p in obstacles[10:20] ] self.get_logger().info( f\" obs({len(obstacles)}) ...{sample}...\" ) Finally, it is possible to publish this result in a PointCloud message and to visualize it on rviz2 in a superposition of the LaserScan. PointCloud is based on geometry_msgs.Point32 with float coordinate. The creation of Point32 will require explicite cast. aPoint= Point32() aPoint.x= (float)(math.cos(angle) * aDistance) aPoint.y= (float)(math.sin( angle ) * aDistance) aPoint.z= (float)(0) PointCloud mesage as part of the sensor_msgs pkg (yes we are using the deprecated one, and we should not)","title":"Range Sensor"},{"location":"tuto-level-up/range-sensor/#range-sensor","text":"Range sensors are robot sensor permitting to detect obstacles and determine a distance to it. Basic range sensors (infrared, ultrasonic, laser) produce a unique measure considering a given direction at a time. By making the sensor rotating, it is possible to get measurements on a plan around the sensor. Hokuhyo, equipping the Tbot , is typically a kind of rotating lidar sensor ( l aser i maging or li ght d etection a nd r anging). The goal here is to integrate an almost 360 obstacle detection to generate safe robot movement. More complex lidar permits 3D measurements (i.e. in several plans at a time).","title":"Range sensor"},{"location":"tuto-level-up/range-sensor/#read-scan-data","text":"The pibots already run urg_node_driver ros node. This node publish laser scan into a \\scan topic. It is possible to check it with ros2 topic list and ros2 topic echo scan . Gazebo simulator also simulate laser scan, but the scan topic and/or the laser-scan frame can have different names. ros2 launch tbot_sim challenge-1.launch.py Now you can visualize it on rviz2 program. Start rviz2 , add a flux laserScan and configure it in /scan topic. Nothing appears and it is normal. Rviz2 global option is configured on map frame, and nothing permits to set the position of the laser sensor in the map. The laser-scan frame is named laser . Change this information into global options and set the laser-scan size to 0,1 for a better display.","title":"Read Scan Data"},{"location":"tuto-level-up/range-sensor/#a-first-node-logging-the-scan","text":"First, we will initialize a node scan_echo to print the scan data into a terminal. Edit a new script file scan_echo en configure your package to recognize it as a ROS2 node. Test your scan_echo node: ros2 run tutorial_pkg scan_echo Connect sensor_msgs LaserScan . Log continuously the received data #!python3 import rclpy from rclpy.node import Node from sensor_msgs.msg import LaserScan rosNode= None def scan_callback(scanMsg): global rosNode rosNode.get_logger().info( f\"scan:\\n{scanMsg}\" ) rclpy.init() rosNode= Node('scan_interpreter') rosNode.create_subscription( LaserScan, 'scan', scan_callback, 10) rclpy.spin( rosNode ) scanInterpret.destroy_node() rclpy.shutdown() Test your scan_echo node, with the pibot and on simulations. You can notice that ros_logger print information into info channel (informative), but other channel exits for errors and warnings. At this point your echo node print the entire scan message, but we do not care about the distance measurements, we want only some meta-data. Modify the logger to print the information into the header and the number of ranges.","title":"A first node logging the scan"},{"location":"tuto-level-up/range-sensor/#from-laserscan-to-pointcloud","text":"LaserScan provides both the recorded bean distances (ranges) and the meta information permitting converting the distances on points in a regular Cartesian frame (i.e. the angle between beans). In a python, the conversion would look like this: obstacles= [] angle= scanMsg.angle_min for aDistance in scanMsg.ranges : if 0.1 < aDistance and aDistance < 5.0 : aPoint= [ math.cos(angle) * aDistance, math.sin(angle) * aDistance ] obstacles.append(aPoint) angle+= scanMsg.angle_increment The exercise consists in modifying the scan callback function to generate the point cloud list. To log a sample of the point cloud: sample= [ [ round(p[0], 2), round(p[1], 2) ] for p in obstacles[10:20] ] self.get_logger().info( f\" obs({len(obstacles)}) ...{sample}...\" ) Finally, it is possible to publish this result in a PointCloud message and to visualize it on rviz2 in a superposition of the LaserScan. PointCloud is based on geometry_msgs.Point32 with float coordinate. The creation of Point32 will require explicite cast. aPoint= Point32() aPoint.x= (float)(math.cos(angle) * aDistance) aPoint.y= (float)(math.sin( angle ) * aDistance) aPoint.z= (float)(0) PointCloud mesage as part of the sensor_msgs pkg (yes we are using the deprecated one, and we should not)","title":"From LaserScan to PointCloud"},{"location":"tuto-level-up/reactive/","text":"Reactive Move The goal is to create a straight node that permits the robot to move straight until obstacles are detected then turn left or right to avoid the obstacle. Node Structure The straight node is our first real complex node. It will process scan messages to characterize the local environment and regularly send coherent move instructions. That for, we want to set up a node with \\(2\\) callback function. A sensing callback and a control callback. In facts, it is not recommended to send control messages only went sensing messages are received. This way, the robot will be capable of dealing failures as interruption in the sensing messages. So, the straight script will be structured like this: #!/usr/bin/python3 import rclpy from rclpy.node import Node from sensor_msgs.msg import LaserScan from geometry_msgs.msg import Twist import sys # Ros Node process: def main(): # Initialize ROS and a ROS node rclpy.init(args=sys.argv) node= Node( 'basic_move' ) # Initialize our control: control= StraightCtrl() control.initializeRosNode( node ) # infinite Loop: rclpy.spin( node ) # clean end node.destroy_node() rclpy.shutdown() # Ros Node Class: class StraightCtrl : def initializeRosNode(self, rosNode ): # Get logger from the node: self._logger= rosNode.get_logger() # Initialize publisher: self._pubVelocity= rosNode.create_publisher( Twist, '/multi/cmd_nav', 10 ) # Initialize scan callback: self._subToScan= rosNode.create_subscription( LaserScan, '/scan', self.scan_callback, 10 ) # Initialize control callback: self._timForCtrl= rosNode.create_timer( 0.05, self.control_callback ) def scan_callback(self, scanMsg ): self._logger.info( '> get scan' ) def control_callback(self): self._logger.info( '< define control' ) # Go: if __name__ == '__main__' : main() Include your straight node into your package, and test it: ros2 run tutorial_pkg straight Scan Callback Implement the scan_callback method in a way that it fills \\(2\\) boolean class variables: self.obstacle_left and self.obstacle_right . The self._obstacle_left should be True if an obstacle is detected front-left of the robot and Flase else. The self._obstacle_right should be True if an obstacle is detected front-right of the robot and Flase else. We consider obstacles only if there are at \\(1\\) meter or less to the robot. Control Callback Implement the control_callback method in a way that it sent velocity message coherent with the presence of obstacles on the left and/or right side of the robot. The move straight until an obstacle is found in a range of 1 meter in front of it ( self.obstacle_left or self.obstacle_right is True ). The robot rotate with a positive or negative value depending on which of the obstacle variable is true. For security reason, the control should not send velocity messages while scan messages were not received and should stop if scan message stop. Smart Control At this point, the control is defined on \\(4\\) states, with on/off velocity control. The states are : state action no scan empty no obstacle go straight left obstacle turn right right obstacle turn left both obstacle turn left As a side effect, the robot behavior is not really efficient. First we can increase the state definition to handle more possibilities. For instance, if the robot faces a both obstacle situation while it is turning right (i.e. avoiding a left obstacle ) it would be more interesting to continue to turn on the same direction. For that the robot state should remenber the last performed action. Another example would be to differentiate close and distant obstacles to apply different control velocities (move and turn in a same time while possible), and generate fluid trajectories. Finally, the desired speed can be reached with several steps, to force a soft acceleration.","title":"Reactive Move"},{"location":"tuto-level-up/reactive/#reactive-move","text":"The goal is to create a straight node that permits the robot to move straight until obstacles are detected then turn left or right to avoid the obstacle.","title":"Reactive Move"},{"location":"tuto-level-up/reactive/#node-structure","text":"The straight node is our first real complex node. It will process scan messages to characterize the local environment and regularly send coherent move instructions. That for, we want to set up a node with \\(2\\) callback function. A sensing callback and a control callback. In facts, it is not recommended to send control messages only went sensing messages are received. This way, the robot will be capable of dealing failures as interruption in the sensing messages. So, the straight script will be structured like this: #!/usr/bin/python3 import rclpy from rclpy.node import Node from sensor_msgs.msg import LaserScan from geometry_msgs.msg import Twist import sys # Ros Node process: def main(): # Initialize ROS and a ROS node rclpy.init(args=sys.argv) node= Node( 'basic_move' ) # Initialize our control: control= StraightCtrl() control.initializeRosNode( node ) # infinite Loop: rclpy.spin( node ) # clean end node.destroy_node() rclpy.shutdown() # Ros Node Class: class StraightCtrl : def initializeRosNode(self, rosNode ): # Get logger from the node: self._logger= rosNode.get_logger() # Initialize publisher: self._pubVelocity= rosNode.create_publisher( Twist, '/multi/cmd_nav', 10 ) # Initialize scan callback: self._subToScan= rosNode.create_subscription( LaserScan, '/scan', self.scan_callback, 10 ) # Initialize control callback: self._timForCtrl= rosNode.create_timer( 0.05, self.control_callback ) def scan_callback(self, scanMsg ): self._logger.info( '> get scan' ) def control_callback(self): self._logger.info( '< define control' ) # Go: if __name__ == '__main__' : main() Include your straight node into your package, and test it: ros2 run tutorial_pkg straight","title":"Node Structure"},{"location":"tuto-level-up/reactive/#scan-callback","text":"Implement the scan_callback method in a way that it fills \\(2\\) boolean class variables: self.obstacle_left and self.obstacle_right . The self._obstacle_left should be True if an obstacle is detected front-left of the robot and Flase else. The self._obstacle_right should be True if an obstacle is detected front-right of the robot and Flase else. We consider obstacles only if there are at \\(1\\) meter or less to the robot.","title":"Scan Callback"},{"location":"tuto-level-up/reactive/#control-callback","text":"Implement the control_callback method in a way that it sent velocity message coherent with the presence of obstacles on the left and/or right side of the robot. The move straight until an obstacle is found in a range of 1 meter in front of it ( self.obstacle_left or self.obstacle_right is True ). The robot rotate with a positive or negative value depending on which of the obstacle variable is true. For security reason, the control should not send velocity messages while scan messages were not received and should stop if scan message stop.","title":"Control Callback"},{"location":"tuto-level-up/reactive/#smart-control","text":"At this point, the control is defined on \\(4\\) states, with on/off velocity control. The states are : state action no scan empty no obstacle go straight left obstacle turn right right obstacle turn left both obstacle turn left As a side effect, the robot behavior is not really efficient. First we can increase the state definition to handle more possibilities. For instance, if the robot faces a both obstacle situation while it is turning right (i.e. avoiding a left obstacle ) it would be more interesting to continue to turn on the same direction. For that the robot state should remenber the last performed action. Another example would be to differentiate close and distant obstacles to apply different control velocities (move and turn in a same time while possible), and generate fluid trajectories. Finally, the desired speed can be reached with several steps, to force a soft acceleration.","title":"Smart Control"},{"location":"tuto-level-up/vision/","text":"Introduction to Image Processing Presentation: pdf Images: Scripts: tools.py LARM_BASE LARM_HISTO LARM_SEG LARM_OP LARM_AVANCE","title":"Vision"},{"location":"tuto-level-up/vision/#introduction-to-image-processing","text":"Presentation: pdf","title":"Introduction to Image Processing"},{"location":"tuto-level-up/vision/#images","text":"","title":"Images:"},{"location":"tuto-level-up/vision/#scripts","text":"tools.py LARM_BASE LARM_HISTO LARM_SEG LARM_OP LARM_AVANCE","title":"Scripts:"},{"location":"tuto-mastering/interfaces/","text":"Interfaces in ROS2 Mesage definition Services","title":"Interfaces"},{"location":"tuto-mastering/interfaces/#interfaces-in-ros2","text":"","title":"Interfaces in ROS2"},{"location":"tuto-mastering/interfaces/#mesage-definition","text":"","title":"Mesage definition"},{"location":"tuto-mastering/interfaces/#services","text":"","title":"Services"},{"location":"tuto-mastering/slam/","text":"What is SLAM in a Nutshell? Mobile robots rely heavily on accurate representations of the environment (i.e maps ) to fulfill their tasks (autonomous navigation, exploration, ...). Inside buildings, GPS signals are too weak to be used to localize robots. Hence we face a so-called Chicken-and-Egg-Problem, as localization requires a map, and map building (i.e. mapping ) requires the current location. One solution consists in doing Simultaneous Localization and Mapping (a.k.a. SLAM) using a SLAM algorithm that typically reaches centimetric precision. There are many different flavors of SLAM especially regarding the map format. The dominating 2D map format is the occupancy grid, also called grid map. A grid map is a matrix whose cells represents a defined region of the real world; this is the resolution of the grid map (typically a square of 5cm). A cell holds the estimated probability that the space it represents is traversable (free space) or not (obstacle). The simplest format is the 3-state occupancy grid in which a cell has 3 different possible values: 0 (free space), 0.5 (unknown) and 1 (obstacle). Transformations doc Map building using slam_toolbox There are a lot of different SLAM algorithms and some implementations are open source and available on OpenSLAM . We will use here the slam_toolbox ROS implementation (documentation is here ). Launch slam_toolbox ros2 launch tbot_sim challenge-1.launch.py ros2 launch slam_toolbox online_sync_launch.py --ros-args -p use_sim_time:=True rviz2 --ros-args -p use_sim_time:=True Question: using all the tools you already know ( rviz2 , rqt_graph , tf, ...), what are the input and output data of slam_toolbox ? Manual Mapping Launch a teleop: # keyboard ros2 run teleop_twist_keyboard teleop_twist_keyboard # or xbox ros2 launch teleop_twist_joy teleop-launch.py joy_config:=xbox Now, while moving the robot around the simulated environment, you should see the result of slam_toolbox (both the map and robot pose) updated in rviz2 . It is possible to save the map by using slam_toolbox service call: ros2 service call /slam_toolbox/save_map slam_toolbox/srv/SaveMap \"name: {data: '/home/user.name/path/to/map'}\" Load a map and Localize in it To load a map and localize in it, you should: execute: ros2 launch nav2_bringup localization_launch.py map:=/home/bot/map.yaml autostart:=True publish an approximate initial pose into the topic /initialpose (using rviz2 for example) move the robot around (teleop or so) to improve the acuracy of the localization In simulation , we do the same steps, but the time is simulated: ros2 launch nav2_bringup localization_launch.py map:=/home/bot/map.yaml autostart:=True use_sim_time:=True rviz2 --ros-args --remap use_sim_time:=True Bag files ( ros2 bag ) Working in simulation is nice but we can do better and work directly on real data using the ros2 bag command tool. The idea is to record some topics (all data that goes through) into a bag file and play them later on. Bag files are really useful to test algorithms on real data sets that have been recorded in a specific location and with specific sensors. Moreover, there are a lot of public datasets available: [http://radish.sourceforge.net/] [https://vision.in.tum.de/data/datasets/rgbd-dataset/download] [http://www.ipb.uni-bonn.de/datasets/] [http://car.imt-lille-douai.fr/polyslam/] Use the tutorial on rosbag to learn how to use it. Work: Launch the challenge 2 simulation, record /tf and /scan into a rosbag and teleop the robot all around the map. You now have a dataset ready to do offline SLAM. Offline Mapping Work: Play the rosbag created before and launch slam_toolbox in offline mode to create the map and then save it. You can now load this map to localize the robot into it. Map a real arena Re-do exactly the same things to map an existing arena: Put the robot inside the arena Record a bag file with all needed data to achieve SLAM Manually teleoperate the robot everywhere in the arena Save the bag file Play the bag file and launch the SLAM to create a map and save it (offline mapping) Put the robot inside the arena and load the map to localize the robot inside the arena Hint: slam_toolbox comes with a RViz plugin (Panels->Add New Panel->slam_toolbox->SlamToolboxPlugin) that might help Tuning map quality SLAM algorithms usually have a lot of parameters that can be tuned to improve performance or map quality. slam_toolbox is no exception. Some hints here .","title":"S.L.A.M"},{"location":"tuto-mastering/slam/#what-is-slam-in-a-nutshell","text":"Mobile robots rely heavily on accurate representations of the environment (i.e maps ) to fulfill their tasks (autonomous navigation, exploration, ...). Inside buildings, GPS signals are too weak to be used to localize robots. Hence we face a so-called Chicken-and-Egg-Problem, as localization requires a map, and map building (i.e. mapping ) requires the current location. One solution consists in doing Simultaneous Localization and Mapping (a.k.a. SLAM) using a SLAM algorithm that typically reaches centimetric precision. There are many different flavors of SLAM especially regarding the map format. The dominating 2D map format is the occupancy grid, also called grid map. A grid map is a matrix whose cells represents a defined region of the real world; this is the resolution of the grid map (typically a square of 5cm). A cell holds the estimated probability that the space it represents is traversable (free space) or not (obstacle). The simplest format is the 3-state occupancy grid in which a cell has 3 different possible values: 0 (free space), 0.5 (unknown) and 1 (obstacle). Transformations doc","title":"What is SLAM in a Nutshell?"},{"location":"tuto-mastering/slam/#map-building-using-slam_toolbox","text":"There are a lot of different SLAM algorithms and some implementations are open source and available on OpenSLAM . We will use here the slam_toolbox ROS implementation (documentation is here ).","title":"Map building using slam_toolbox"},{"location":"tuto-mastering/slam/#launch-slam_toolbox","text":"ros2 launch tbot_sim challenge-1.launch.py ros2 launch slam_toolbox online_sync_launch.py --ros-args -p use_sim_time:=True rviz2 --ros-args -p use_sim_time:=True Question: using all the tools you already know ( rviz2 , rqt_graph , tf, ...), what are the input and output data of slam_toolbox ?","title":"Launch slam_toolbox"},{"location":"tuto-mastering/slam/#manual-mapping","text":"Launch a teleop: # keyboard ros2 run teleop_twist_keyboard teleop_twist_keyboard # or xbox ros2 launch teleop_twist_joy teleop-launch.py joy_config:=xbox Now, while moving the robot around the simulated environment, you should see the result of slam_toolbox (both the map and robot pose) updated in rviz2 . It is possible to save the map by using slam_toolbox service call: ros2 service call /slam_toolbox/save_map slam_toolbox/srv/SaveMap \"name: {data: '/home/user.name/path/to/map'}\"","title":"Manual Mapping"},{"location":"tuto-mastering/slam/#load-a-map-and-localize-in-it","text":"To load a map and localize in it, you should: execute: ros2 launch nav2_bringup localization_launch.py map:=/home/bot/map.yaml autostart:=True publish an approximate initial pose into the topic /initialpose (using rviz2 for example) move the robot around (teleop or so) to improve the acuracy of the localization In simulation , we do the same steps, but the time is simulated: ros2 launch nav2_bringup localization_launch.py map:=/home/bot/map.yaml autostart:=True use_sim_time:=True rviz2 --ros-args --remap use_sim_time:=True","title":"Load a map and Localize in it"},{"location":"tuto-mastering/slam/#bag-files-ros2-bag","text":"Working in simulation is nice but we can do better and work directly on real data using the ros2 bag command tool. The idea is to record some topics (all data that goes through) into a bag file and play them later on. Bag files are really useful to test algorithms on real data sets that have been recorded in a specific location and with specific sensors. Moreover, there are a lot of public datasets available: [http://radish.sourceforge.net/] [https://vision.in.tum.de/data/datasets/rgbd-dataset/download] [http://www.ipb.uni-bonn.de/datasets/] [http://car.imt-lille-douai.fr/polyslam/] Use the tutorial on rosbag to learn how to use it. Work: Launch the challenge 2 simulation, record /tf and /scan into a rosbag and teleop the robot all around the map. You now have a dataset ready to do offline SLAM.","title":"Bag files (ros2 bag)"},{"location":"tuto-mastering/slam/#offline-mapping","text":"Work: Play the rosbag created before and launch slam_toolbox in offline mode to create the map and then save it. You can now load this map to localize the robot into it.","title":"Offline Mapping"},{"location":"tuto-mastering/slam/#map-a-real-arena","text":"Re-do exactly the same things to map an existing arena: Put the robot inside the arena Record a bag file with all needed data to achieve SLAM Manually teleoperate the robot everywhere in the arena Save the bag file Play the bag file and launch the SLAM to create a map and save it (offline mapping) Put the robot inside the arena and load the map to localize the robot inside the arena Hint: slam_toolbox comes with a RViz plugin (Panels->Add New Panel->slam_toolbox->SlamToolboxPlugin) that might help","title":"Map a real arena"},{"location":"tuto-mastering/slam/#tuning-map-quality","text":"SLAM algorithms usually have a lot of parameters that can be tuned to improve performance or map quality. slam_toolbox is no exception. Some hints here .","title":"Tuning map quality"},{"location":"tuto-mastering/transform/","text":"Move To The idea here is to develop a move strategy to permits a robot to reach positions successively, in a cluttered environment. To do that, the node subscribes to a topic goals to get in a position to reach. The main difficulty here, consists in following positioning kwoledge of the goals while the robot is moving. Record a goal position It supposes that you play with at least 2 frames. A local frame is attached to the robot and is moving with it. A global frame permits to localize the goal at a fixed position in the environement and the robot (i.e. the local frame). It supposes that your global frame is fixed in the environment. Classically, we use the map frame for global referring system, but without map it is possible to use the odom (from robot odometer). The robot is defined with different frame: base_link at the gravity center of the robot. base_footprint as a projection of base_link on the floor. Understand frame and transformations By starting any sensor as the laser, the publishing data is in its own frame. It would be impossible for rviz2 to display the laser information into map frame ( fixed frame ). The map and laser frames are independent. Start the laser and rviz2: # First console ros2 run urg_node urg_node_driver --ros-args -p serial_port:=/dev/ttyACM0 # Second console rviz2 In rviz, connect to scan topic, but nothing appears. Try by modifying the global frame with the frame of the laser. Transform tools The package tf2_tools provides with a process that generates a graph of the connection between the frames. #third console ros2 run tf2_tools view_frames.py evince frames.pdf In ROS tf stand for transformation. It is a central tool permitting getting space-temporal information from a frame to another. It supposes that specific messages are published into dedicated topic tf . At this step, no transform are published... It is possible to generate a static transformation (it supposes that the laser is fixed in the environment at a specific position) ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 0 1 \"map\" \"laser\" You can validate with view_frame that the 2 frames ares connected and that laser scan are displayed in rviz . The first 3 numbers fix the translation. It is the potion of the laser center into map . The next 4 numbers give the rotation. In fact, the publisher generates a TransfromStamped mesage and the rotation is based on quaternion definition (cf. wikipedia for details...) Display the frames in rviz2 (add > axes > set reference frame) and play with different configurations (kill and restart the static_transform_publisher ). tbot configuration For a simple robot, it can be dozens of frames and it grows with robot parts (legs, arms). ROS provide a tool (state publisher) to publish transform regarding how the frames are interconnected. The tbot launch file of the tbot_start package already starts state publisher based on a description of the tbot (kobuki robot in IMT Nord Europe configuration). Spot every thing but rviz and start the tbot launch file: tbot_start minimal.launch.py . Generate the frame graph ( view_frame ). In basic configuration, the robot provides a first pose estimation in global odom frame (ie. transformation between odom and base_link ). So set the fixed frame in rviz on odom , the laser scans appear. Connect to tf topic and all the frame axis appear too. Oficial documentation about tf2: docs.ros.org . Bonus: it is possible to visualize the robot in rviz2 : add > robot description (select the appropriate topic). Publish a pose in a specific frame. Naturally, ROS also provide C++ and Python library to manipulate transformation and permits developers to get pose coordinate from a frame to another. The idea is to a declare a tf2 listener, an object that will subscribe to transform topics and maintain transformation data. Then it is possible to recompute pose coordinates in any connected frames. More on : wiki.ros.org . The idea in our context is to develop a node localGoal that will remember a pose in a global frame and publish at a given frequence the pose in another local frame. For our new node, we have to declare the elements permitting the node to listen and keep the transformation available, a listerner and a buffer . # Transform tool: self.tf_buffer = tf2_ros.Buffer() self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self) ... Do not forget to import tf2_ros into your script and to add the reference into the package dependencies ( package.xml ). The node is also defined with it onw attrituts: for instance a target local frame, a goal pose (position orientation). ... # Node Attribute: self.local_frame= 'base_link' self.global_goal= Pose() self.global_goal.position.x= (float)(1) self.global_goal.position.y= (float)(2) ... And finally we require a timer with a callback function to publish continuously the goal pose. ... node.create_timer(0.1, self.publish_goal) ... We can now address the interesting question: How to transform a position defined into a frame in another frame ? It consists in building a Transform object from the reference and target frames. While a listener was declared on a transform buffer, it is possible to create this object from that tool (if exist). The Transform object is generated with the method lookup_transform(target_frame, reference_frame, time) . This method gets a target_frame (the frame id in which we want the pose) a reference_frame (the frame id in which the pose is currently defined) and a time. In fact, the transformations are dynamically. The position and orientation of elements (the robot(s), robot parts, ...) change continuously and it is possible to get transformation in the present or in the past. To get the current transformation a node.time.Time() permits to get the current time. The lookup is not guaranteed to achieve. It can fail in case of a gap in the transforms or obsolete transforms. In case of fail, an exception is thrown accordingly the python exception manager (more on w3schools ). Finally, inside our publish_goal call back, getting a transform will look like: def publish_goal(self): currentTime= rclpy.time.Time() # Get Transformation try: stampedTransform= self.tf_buffer.lookup_transform( self.local_frame, 'odom', currentTime) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):TransformException as tex: self._node.get_logger().info( f'Could not transform the goal into {self.local_frame}: {tex}') return ... The transform is a stamped transform (ie. defined in a given time) defined by geometry_msgs package. The pose transformation is already defined in a ros method of tf2_geometry_msgs package and it require the installation of python3-tf2-geometry-msgs (things are never simple in ros...): sudo apt update sudo apt install python3-tf2-geometry-msgs def publish_goal(self): ... # Compute goal into local coordinates localGoal = tf2_geometry_msgs.do_transform_pose( self.global_goal, stampedTransform ) ... Permit Autonomous Navigation The goal poses itself is not interesting. The objective now is to include this code into the reactive move node in order to permits the robot to reach a decided destination, by avoiding obstacles. Get the goal pose from a topic pose (a pose can be published with rviz). Convert the received goal pose into a global frame ( odom ). Control the robot to reach the goal (the control is in the local frame) Stop the robot if it is close enough to the position. Going Further - Path following Rather than a unique pose, it could be interesting to define a succession of pose to follow (a path). That for the reactive move has to manage a list of pose and switch from a pose to the next one each time it is expected.","title":"Move To"},{"location":"tuto-mastering/transform/#move-to","text":"The idea here is to develop a move strategy to permits a robot to reach positions successively, in a cluttered environment. To do that, the node subscribes to a topic goals to get in a position to reach. The main difficulty here, consists in following positioning kwoledge of the goals while the robot is moving.","title":"Move To"},{"location":"tuto-mastering/transform/#record-a-goal-position","text":"It supposes that you play with at least 2 frames. A local frame is attached to the robot and is moving with it. A global frame permits to localize the goal at a fixed position in the environement and the robot (i.e. the local frame). It supposes that your global frame is fixed in the environment. Classically, we use the map frame for global referring system, but without map it is possible to use the odom (from robot odometer). The robot is defined with different frame: base_link at the gravity center of the robot. base_footprint as a projection of base_link on the floor.","title":"Record a goal position"},{"location":"tuto-mastering/transform/#understand-frame-and-transformations","text":"By starting any sensor as the laser, the publishing data is in its own frame. It would be impossible for rviz2 to display the laser information into map frame ( fixed frame ). The map and laser frames are independent. Start the laser and rviz2: # First console ros2 run urg_node urg_node_driver --ros-args -p serial_port:=/dev/ttyACM0 # Second console rviz2 In rviz, connect to scan topic, but nothing appears. Try by modifying the global frame with the frame of the laser.","title":"Understand frame and transformations"},{"location":"tuto-mastering/transform/#transform-tools","text":"The package tf2_tools provides with a process that generates a graph of the connection between the frames. #third console ros2 run tf2_tools view_frames.py evince frames.pdf In ROS tf stand for transformation. It is a central tool permitting getting space-temporal information from a frame to another. It supposes that specific messages are published into dedicated topic tf . At this step, no transform are published... It is possible to generate a static transformation (it supposes that the laser is fixed in the environment at a specific position) ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 0 1 \"map\" \"laser\" You can validate with view_frame that the 2 frames ares connected and that laser scan are displayed in rviz . The first 3 numbers fix the translation. It is the potion of the laser center into map . The next 4 numbers give the rotation. In fact, the publisher generates a TransfromStamped mesage and the rotation is based on quaternion definition (cf. wikipedia for details...) Display the frames in rviz2 (add > axes > set reference frame) and play with different configurations (kill and restart the static_transform_publisher ).","title":"Transform tools"},{"location":"tuto-mastering/transform/#tbot-configuration","text":"For a simple robot, it can be dozens of frames and it grows with robot parts (legs, arms). ROS provide a tool (state publisher) to publish transform regarding how the frames are interconnected. The tbot launch file of the tbot_start package already starts state publisher based on a description of the tbot (kobuki robot in IMT Nord Europe configuration). Spot every thing but rviz and start the tbot launch file: tbot_start minimal.launch.py . Generate the frame graph ( view_frame ). In basic configuration, the robot provides a first pose estimation in global odom frame (ie. transformation between odom and base_link ). So set the fixed frame in rviz on odom , the laser scans appear. Connect to tf topic and all the frame axis appear too. Oficial documentation about tf2: docs.ros.org . Bonus: it is possible to visualize the robot in rviz2 : add > robot description (select the appropriate topic).","title":"tbot configuration"},{"location":"tuto-mastering/transform/#publish-a-pose-in-a-specific-frame","text":"Naturally, ROS also provide C++ and Python library to manipulate transformation and permits developers to get pose coordinate from a frame to another. The idea is to a declare a tf2 listener, an object that will subscribe to transform topics and maintain transformation data. Then it is possible to recompute pose coordinates in any connected frames. More on : wiki.ros.org . The idea in our context is to develop a node localGoal that will remember a pose in a global frame and publish at a given frequence the pose in another local frame. For our new node, we have to declare the elements permitting the node to listen and keep the transformation available, a listerner and a buffer . # Transform tool: self.tf_buffer = tf2_ros.Buffer() self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self) ... Do not forget to import tf2_ros into your script and to add the reference into the package dependencies ( package.xml ). The node is also defined with it onw attrituts: for instance a target local frame, a goal pose (position orientation). ... # Node Attribute: self.local_frame= 'base_link' self.global_goal= Pose() self.global_goal.position.x= (float)(1) self.global_goal.position.y= (float)(2) ... And finally we require a timer with a callback function to publish continuously the goal pose. ... node.create_timer(0.1, self.publish_goal) ... We can now address the interesting question: How to transform a position defined into a frame in another frame ? It consists in building a Transform object from the reference and target frames. While a listener was declared on a transform buffer, it is possible to create this object from that tool (if exist). The Transform object is generated with the method lookup_transform(target_frame, reference_frame, time) . This method gets a target_frame (the frame id in which we want the pose) a reference_frame (the frame id in which the pose is currently defined) and a time. In fact, the transformations are dynamically. The position and orientation of elements (the robot(s), robot parts, ...) change continuously and it is possible to get transformation in the present or in the past. To get the current transformation a node.time.Time() permits to get the current time. The lookup is not guaranteed to achieve. It can fail in case of a gap in the transforms or obsolete transforms. In case of fail, an exception is thrown accordingly the python exception manager (more on w3schools ). Finally, inside our publish_goal call back, getting a transform will look like: def publish_goal(self): currentTime= rclpy.time.Time() # Get Transformation try: stampedTransform= self.tf_buffer.lookup_transform( self.local_frame, 'odom', currentTime) except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):TransformException as tex: self._node.get_logger().info( f'Could not transform the goal into {self.local_frame}: {tex}') return ... The transform is a stamped transform (ie. defined in a given time) defined by geometry_msgs package. The pose transformation is already defined in a ros method of tf2_geometry_msgs package and it require the installation of python3-tf2-geometry-msgs (things are never simple in ros...): sudo apt update sudo apt install python3-tf2-geometry-msgs def publish_goal(self): ... # Compute goal into local coordinates localGoal = tf2_geometry_msgs.do_transform_pose( self.global_goal, stampedTransform ) ...","title":"Publish a pose in a specific frame."},{"location":"tuto-mastering/transform/#permit-autonomous-navigation","text":"The goal poses itself is not interesting. The objective now is to include this code into the reactive move node in order to permits the robot to reach a decided destination, by avoiding obstacles. Get the goal pose from a topic pose (a pose can be published with rviz). Convert the received goal pose into a global frame ( odom ). Control the robot to reach the goal (the control is in the local frame) Stop the robot if it is close enough to the position.","title":"Permit Autonomous Navigation"},{"location":"tuto-mastering/transform/#going-further-path-following","text":"Rather than a unique pose, it could be interesting to define a succession of pose to follow (a path). That for the reactive move has to manage a list of pose and switch from a pose to the next one each time it is expected.","title":"Going Further - Path following"},{"location":"tuto-mastering/vision/","text":"Introduction to image processing. Vision provides a rich information about the immediate environment around the robot. However it requires to process the images to extract pertinent information like the objects in the scene. The goal here is to detect the specific colored objects that are positioned in the scene along the path of your robot in order to count and place them in the map. A first approache coub be to use color analysis techniques to detect the objects. If objects are textured, more sophisticated techniques could be used. This tutorial gives you some tips to help you with this detection task. Setup our programming environment This tutorial focus on OpenCV librairy, addressed in Python development language on a Linux machine. First we want to be sure that Python3 and some libraries (Numpy, Matplot, Sklearn, Scipy, OpenCV etc.) are installed. The command whereis permit to localize a command (like python3 interpreter). If python3 is correctly installed, its execution would not be empty: whereis python3 Python uses its own package managers pip to install libraries. So just to be sure you can: sudo apt install python3 pip Then we can use pip to install required modules: pip3 install numpy opencv-python opencv-contrib-python scikit-learn scipy matplotlib psutil scikit-image I you plan to use neural networks techniques (a GPU is almost always required to provide real-time detection): pip3 install tensorflow Now normally you can load the different modules, for instance: import cv2 import numpy as np import os from sklearn.svm import LinearSVC from scipy.cluster.vq import * from sklearn.preprocessing import StandardScaler from sklearn import preprocessing Image acquisition Image from simulation It is possible to access the camera's video streams via the Realsense sensor (if it is available) or simulation. Gazebo is capable of simulating robot vision (classical and 3D sensor). Launch a simulation : '''bash roslaunch larm chalenge-2.launch ''' Observe the topic and mainly the one published the images ( camera/rgb/image_raw ) with rostopic list , rostopic info and rostopic hz . Image are published as 'sensor_msgs/Image' message type in camera/rgb/image_raw ROS doc If you echo the messages of the image topic ( rostopic echo camera/rgb/image_raw ), you can observe the following structure of the message std_msgs/Header header uint32 height uint32 width string encoding uint8 is_bigendian uint32 step uint8[] data The pixel values are stored in a uint8 array. Several tools are availbale to convert a ROS image to OpenCV image (for instance cv_bridge ) Image from Realsense camera sensor The following code is able to connect the Realsense camera and to acquire the depth and the color stream. A configuration step is done. Both depth and color images are aligned to make that all pixels of a depth image correpond to all pixels of the corresponding color image. import pyrealsense2 as rs import numpy as np import math import cv2,time,sys pipeline = rs.pipeline() config = rs.config() colorizer = rs.colorizer() # Configuration of depth and color image size (840x480), the format of the pixel values and the frame rates (30 fps) config.enable_stream(rs.stream.depth, 840, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 840, 480, rs.format.bgr8, 30) pipeline.start(config) align_to = rs.stream.depth align = rs.align(align_to) rayon=10 count=1 refTime= time.process_time() freq= 60 try: while True: # This call waits until a new coherent set of frames is available on a device frames = pipeline.wait_for_frames() # Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() if not depth_frame or not aligned_color_frame: continue # Two ways to colorized the depth map # first : using colorizer of pyrealsense colorized_depth = colorizer.colorize(depth_frame) depth_colormap = np.asanyarray(colorized_depth.get_data()) # Get the intrinsic parameters color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics color_image = np.asanyarray(aligned_color_frame.get_data()) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape # Show images images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (840x480 for example) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL) # Resize the Window cv2.resizeWindow('RealSense', 960, 720) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 except Exception as e: print(e) pass finally: pipeline.stop() You have already done the code to send both depth and color image on a ROS topic ( camera_driver section). During the challenges you will be able to compute both streams via the ROS image topics or directly after image acquisition ( pipeline.wait_for_frames() in the previous code). yYu will have to pay close attention to the time required to ensure the execution of the processing chains of the two video streams in order to guarantee rapid detection. Basic codes using OpenCV framework This section introduces some of the functions and techniques illustrated by some codes that you can use to complete the proposed challenges. First, you will test each proposal on images downloaded from the Internet or from the Vision section of the tutorial. You will then have to integrate or adapt them to process images from the Realsense camera on board the robot. Some codes example propose to acquire and compute the webcam video stream. Segmentation d'images couleur par seuillage des composantes et Gestion de la souris Here are a few lines of code to extract a region of interest using the mouse. With these few lines, you can calculate the mean and variance of each component of the image, which is useful for the segmentation stage. In this example, we first acquire an image from the laptop's webcam. We then use the mouse to define a section of the captured image. You can then easily calculate the desired statistical metrics for this section. The mean and variance define a Gaussian model for each component of the clipping. import cv2 import numpy as np # connect to a sensor (0: webcam) cap=cv2.VideoCapture(0) # capture an image ret, frame=cap.read() # Select ROI r = cv2.selectROI(frame) # Crop image imCrop = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])] average_h = np.mean(imCrop[:,:,0]) average_s = np.mean(imCrop[:,:,1]) average_v = np.mean(imCrop[:,:,2]) print(average_h,average_s,average_v) # Display cropped image cv2.imshow(\"Image\", imCrop) cv2.waitKey(0) In this example we will create a mask of pixels whose HSV components are between the variables lo and hi . In this example, by clicking with the left or right mouse button, you decrease or increase the hue h of the two variables lo and hi . You will also see that in our example this two thresholds differ not only in their hue but also in their saturation. You can test this script on an image of your face. These few lines of code also illustrate how to manage actions on the mouse. They manage mouse events such as mouse move (cv2.EVENT_MOUSEMOVE), double middle click (EVENT_MBUTTONDBLCLK), right click (EVENT_RBUTTONDOWN) and left click (EVENT_LBUTTONDOWN). import cv2 import numpy as np def souris(event, x, y, flags, param): global lo, hi, color, hsv_px if event == cv2.EVENT_MOUSEMOVE: # Conversion des trois couleurs RGB sous la souris en HSV px = frame[y,x] px_array = np.uint8([[px]]) hsv_px = cv2.cvtColor(px_array,cv2.COLOR_BGR2HSV) if event==cv2.EVENT_MBUTTONDBLCLK: color=image[y, x][0] if event==cv2.EVENT_LBUTTONDOWN: if color>5: color-=1 if event==cv2.EVENT_RBUTTONDOWN: if color<250: color+=1 lo[0]=color-10 hi[0]=color+10 color=100 lo=np.array([color-5, 100, 50]) hi=np.array([color+5, 255,255]) color_info=(0, 0, 255) cap=cv2.VideoCapture(0) cv2.namedWindow('Camera') cv2.setMouseCallback('Camera', souris) hsv_px = [0,0,0] # Creating morphological kernel kernel = np.ones((3, 3), np.uint8) while True: ret, frame=cap.read() image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) mask=cv2.inRange(image, lo, hi) mask=cv2.erode(mask, kernel, iterations=1) mask=cv2.dilate(mask, kernel, iterations=1) image2=cv2.bitwise_and(frame, frame, mask= mask) cv2.putText(frame, \"Couleur: {:d}\".format(color), (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Affichage des composantes HSV sous la souris sur l'image pixel_hsv = \" \".join(str(values) for values in hsv_px) font = cv2.FONT_HERSHEY_SIMPLEX cv2.putText(frame, \"px HSV: \"+pixel_hsv, (10, 260), font, 1, (255, 255, 255), 1, cv2.LINE_AA) cv2.imshow('Camera', frame) cv2.imshow('image2', image2) cv2.imshow('Mask', mask) if cv2.waitKey(1)&0xFF==ord('q'): break cap.release() cv2.destroyAllWindows() In general, it is very interesting to change the colour space to better target the space in which the object of interest is distinguishable: image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) After creating the mask with mask=cv2.inRange(image, lo, hi) it is sometimes relevant to denoise the resulting image by blurring or by some morphological operations. This allows the shapes to be closed and filled: # Flouttage de l'image image=cv2.blur(image, (7, 7)) # Erosion d'un mask mask=cv2.erode(mask, None, iterations=4) # dilatation d'un mask mask=cv2.dilate(mask, None, iterations=4) In the segmentation code of a colour image provided earlier, you will play with the kernel size (3x3 in our example), you will add a blurring step for each channel by playing with the neighbourhood size (7x7 in our example). Finally, play with the erosion and dilation steps by varying the number of times each morphological operator is applied (4 times in our example). The segmentation code of a colour image provided previously allows you to define a binary mask of pixels whose HSV components are in the interval [lo,hi] . It is then possible to detect the connected elements in the mask to extract certain information, in this case the minEnclosingCircle. Other functions may be useful. You can find them here: https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html . Assuming that an object of interest is represented by a set of connected pixels whose colour is in the interval [lo,hi] , it is then possible to define constraints on a set of features that allow the classification of the objects thus detected. You will add the following lines to the previous segmentation code. elements=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] if len(elements) > 0: c=max(elements, key=cv2.contourArea) ((x, y), rayon)=cv2.minEnclosingCircle(c) if rayon>30: cv2.circle(image2, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(frame, (int(x), int(y)), 5, color_info, 10) cv2.line(frame, (int(x), int(y)), (int(x)+150, int(y)), color_info, 2) cv2.putText(frame, \"Objet !!!\", (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) Extraction de r\u00e9gions dans une image binaris\u00e9e Here are some lines in Python to extract regions of connected pixels in a binarised image label() . From these regions some properties are extracted regionprops() . This code works strategically in the same way as the previous segmentation script, but using the skimage library. import cv2 import numpy as np import matplotlib.pyplot as plt from skimage.measure import label, regionprops import math image = cv2.imread('./imageasegmenter.jpg') # passage en niveau de gris gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ###### extration des r\u00e9gions avec la lib skimage # Binarisation de l'image ret, thresh = cv2.threshold(gray, 127, 255, 1) cv2.imshow(\"image seuill\u00e9e\",thresh) cv2.waitKey(0) # extraction des r\u00e9gions et des propri\u00e9t\u00e9s des r\u00e9gions label_img = label(thresh) regions = regionprops(label_img) print(regions) cv2.waitKey(0) # affichage des r\u00e9gions et des boites englobantes fig, ax = plt.subplots() ax.imshow(thresh, cmap=plt.cm.gray) for props in regions: y0, x0 = props.centroid orientation = props.orientation x1 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length y1 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5) ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5) ax.plot(x0, y0, '.g', markersize=15) minr, minc, maxr, maxc = props.bbox bx = (minc, maxc, maxc, minc, minc) by = (minr, minr, maxr, maxr, minr) ax.plot(bx, by, '-b', linewidth=2.5) ax.axis((0, 600, 600, 0)) plt.show() cv2.waitKey(0) D\u00e9tection d'objets par template matching Il est possible de d\u00e9tecter un ou plusieurs objets dans une image en appliquant une proc\u00e9dure de matching d'un template de chaque objet \u00e0 d\u00e9tecter. Un template est une image du ou des objets en question. La fonction \u00e0 utiliser est cv.matchTemplate(img_gray,template,parametre) . Plusieurs parametre de matching sont possibles correspondant chacun \u00e0 une m\u00e9trique de corr\u00e9lation. Voici les lignes de codes que vous testerez. Vous testerez les parametres suivants afin de d\u00e9finir celui qui fournit les meilleurs r\u00e9sultats. Par ailleurs, vous adapterez le code afin de prendre un charge le flux des images de la Realsense et une image template de l'objet que vous voulez d\u00e9tect\u00e9. It is possible to detect one or more objects in an image by applying a matching procedure to a template of each object to be detected. A template is one or several images of the object to be detected. The function to use is cv.matchTemplate(img_gray,template,parameter) . Several matching parameters are possible, each corresponding to a correlation metric (cf. https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html ). Here are the lines of code you will test. You will test the following parameters to determine the one that gives the best results. You will also adapt the code to take a load of the Realsense image stream and a template image of the object you want to detect. import cv2 as cv import numpy as np from matplotlib import pyplot as plt # charger l'image dans laquelle on cherche l'objet img_rgb = cv.imread('car.png') img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY) # charger le template de l'objet \u00e0 rechercher template = cv.imread('template.png',0) # R\u00e9cup\u00e9ration des dimensions de l'image w, h = template.shape[::-1] # Application du template atching res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED) # S\u00e9lection des meilleurs matched objects threshold = 0.8 loc = np.where( res >= threshold) # Affichage de la boite englobante de chaque objet d\u00e9tect\u00e9 for pt in zip(*loc[::-1]): cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2) Segmentation des images par la m\u00e9thodes des k-moyennes (kmeans) Kmeans is a clustering algorithm whose objective is to divide a set of data points into several clusters. Each of point is assigned to a cluster with the nearest mean. The mean of each group is called the \"centroid\" or \"centre\". In total, k-means gives k distinct clusters of the original n data points. Data points within a particular cluster are considered to be \"more similar\" to each other than data points belonging to other groups. This algorithm can be applied to geometric, colourimetric and other original points. We will use this method to provide a colour segmentation of an image, i.e. it amounts to finding the domain colours in the image. from sklearn.cluster import KMeans import matplotlib.pyplot as plt import cv2 import numpy as np #Ensuite charger une image et la convertir de BGR \u00e0 RGB si n\u00e9cessaire et l\u2019afficher : image = cv2.imread('lena.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.figure() plt.axis(\"off\") plt.imshow(image) In order to compute the image as a data point, it must be converted from a raster form to a vector form (rgb color list) before applying the clustering function: n_clusters=5 image = image.reshape((image.shape[0] * image.shape[1], 3)) clt = KMeans(n_clusters = n_clusters ) clt.fit(image) To display the most dominant colours in the image, two functions must be defined: centroid_histogram() to retrieve the number of different clusters and create a histogram based on the number of pixels assigned to each cluster; and plot_colors() to initialise the bar graph representing the relative frequency of each colour. def centroid_histogram(clt): numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)>>> (hist, _) = np.histogram(clt.labels_, bins=numLabels) # normalize the histogram, such that it sums to one hist = hist.astype(\"float\") hist /= hist.sum() return hist def plot_colors(hist, centroids): bar = np.zeros((50, 300, 3), dtype=\"uint8\") startX = 0 # loop over the percentage of each cluster and the color of # each cluster for (percent, color) in zip(hist, centroids): # plot the relative percentage of each cluster endX = startX + (percent * 300) cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(\"uint8\").tolist(), -1) startX = endX return bar Now you just need to build a cluster histogram and then create a figure displaying the number of pixels labeled for each color. hist = centroid_histogram(clt) bar = plot_colors(hist, clt.cluster_centers_) plt.figure() plt.axis(\"off\") plt.imshow(bar) plt.show() Classification d'images par la mathode des K plus proches voisins (k-NN ou KNN) This exercise will allow you to learn a model from images in the CIFAR-10 database, downloadable here: https://www.cs.toronto.edu/~kriz/cifar.html . Unzip the files into a folder that you will use in the following script. As you can see, the CIFAR images are contained in the ./data folder. import numpy as np import cv2 basedir_data = \"./data/\" rel_path = basedir_data + \"cifar-10-batches-py/\" #D\u00e9s\u00e9rialiser les fichiers image afin de permettre l\u2019acc\u00e8s aux donn\u00e9es et aux labels: def unpickle(file): import pickle with open(file, 'rb') as fo: dict = pickle.load(fo,encoding='bytes') return dict X = unpickle(rel_path + 'data_batch_1') img_data = X[b'data'] img_label_orig = img_label = X[b'labels'] img_label = np.array(img_label).reshape(-1, 1) You can use teh following to check the r\u00e9sults : print(img_data) print('shape', img_data.shape) print(img_label) print('shape', img_label.shape) You should find a 10000x3072 numpy array of uint8s (the 3072 comes from the 3 x 1024). Each row of the array stores a 32x32 RGB colour image. The images are stored in row-major order, so the first 32 entries in the array correspond to the red channel values of the first row of the image. The labels are stored in a 10000 x 1 array. To check the labels: To load the test data, use the same procedure as before because the shape of the test data is the same as the shape of the training data: test_X = unpickle(rel_path + 'test_batch'); test_data = test_X[b'data'] test_label = test_X[b'labels'] test_label = np.array(test_label).reshape(-1, 1) print(test_data) print('shape', test_data.shape) print(test_label) print('shape', test_label.shape) Please note that the RGB components of the images are arranged in the form of a 1-dimensional vector. To display each image, it is therefore necessary to put it back in the form of a 2D RGB image. To do this, we operate as follows, considering that the images are of 32x32 resolution: one_img=sample_img_data[0,:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) cv2.imshow('Image CIFAR',rgb) cv2.waitKey(0) cv2.destroyAllWindows() Now, you will apply the k-NN algorithm on all the images of the training base img_data and their labels img_label_orig. You will use the fubnction KNeighborsClassifier of the sklearn python modules. from sklearn.neighbors import KNeighborsClassifier #def pred_label_fn(i, original): # return original + '::' + meta[YPred[i]].decode('utf-8') nbrs = KNeighborsClassifier(n_neighbors=3, algorithm='brute').fit(img_data, img_label_orig) # test sur les 10 premi\u00e8res images data_point_no = 10 sample_test_data = test_data[:data_point_no, :] YPred = nbrs.predict(sample_test_data) for i in range(0, len(YPred)): #show_im(sample_test_data, test_label, meta, i, label_fn=pred_label_fn) r = sample_test_data[i][:1024].reshape(32, 32) g = sample_test_data[i][1024:2048].reshape(32, 32) b = sample_test_data[i][2048:].reshape(32, 32) print(YPred[i]) cv2.imshow('image test',np.dstack([r, g, b])) neigh_dist,neigh_ind = nbrs.kneighbors([sample_test_data[i]]) print(neigh_ind) for j in range(0, len(neigh_ind[0])): one_img=img_data[neigh_ind[0][j],:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) cv2.imshow('K plus proche image',np.dstack([r, g, b])) cv2.waitKey(0) Gestion de la depth map et estimation de la distance This code measures the distance of the parts of the scene that are projected at each pixel of the camera by combining the depth image and the color image. It uses the following new part of code which was then integrated into the code you have already tested before. # Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) The complete code integrating the acquisition and visualization part is as follows. The position is marked by a circle in both streams with the measured distance: import pyrealsense2 as rs import numpy as np import math import cv2,time,sys pipeline = rs.pipeline() config = rs.config() colorizer = rs.colorizer() # fps plus bas (30) config.enable_stream(rs.stream.depth, 840, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 840, 480, rs.format.bgr8, 30) pipeline.start(config) align_to = rs.stream.depth align = rs.align(align_to) color_info=(0, 0, 255) rayon=10 count=1 refTime= time.process_time() freq= 60 try: while True: # This call waits until a new coherent set of frames is available on a device frames = pipeline.wait_for_frames() #Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() if not depth_frame or not aligned_color_frame: continue # Two ways to colorized the depth map # first : using colorizer of pyrealsense colorized_depth = colorizer.colorize(depth_frame) depth_colormap = np.asanyarray(colorized_depth.get_data()) # second : using opencv by applying colormap on depth image (image must be converted to 8-bit per pixel first) #depth_image = np.asanyarray(depth_frame.get_data()) #depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) # Get the intrinsic parameters color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics color_image = np.asanyarray(aligned_color_frame.get_data()) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape #Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) #print(\"Distance from camera to pixel:\", distance) #print(\"Z-depth from camera surface to pixel surface:\", depth) # Show images images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (640x480) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA) cv2.circle(images, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(images, (int(x+color_colormap_dim[1]), int(y)), int(rayon), color_info, 2) # Affichage distance au pixel (x,y) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x+color_colormap_dim[1])+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL) # Resize the Window cv2.resizeWindow('RealSense', 960, 720) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 except Exception as e: print(e) pass finally: pipeline.stop()","title":"Image processing"},{"location":"tuto-mastering/vision/#introduction-to-image-processing","text":"Vision provides a rich information about the immediate environment around the robot. However it requires to process the images to extract pertinent information like the objects in the scene. The goal here is to detect the specific colored objects that are positioned in the scene along the path of your robot in order to count and place them in the map. A first approache coub be to use color analysis techniques to detect the objects. If objects are textured, more sophisticated techniques could be used. This tutorial gives you some tips to help you with this detection task.","title":"Introduction to image processing."},{"location":"tuto-mastering/vision/#setup-our-programming-environment","text":"This tutorial focus on OpenCV librairy, addressed in Python development language on a Linux machine. First we want to be sure that Python3 and some libraries (Numpy, Matplot, Sklearn, Scipy, OpenCV etc.) are installed. The command whereis permit to localize a command (like python3 interpreter). If python3 is correctly installed, its execution would not be empty: whereis python3 Python uses its own package managers pip to install libraries. So just to be sure you can: sudo apt install python3 pip Then we can use pip to install required modules: pip3 install numpy opencv-python opencv-contrib-python scikit-learn scipy matplotlib psutil scikit-image I you plan to use neural networks techniques (a GPU is almost always required to provide real-time detection): pip3 install tensorflow Now normally you can load the different modules, for instance: import cv2 import numpy as np import os from sklearn.svm import LinearSVC from scipy.cluster.vq import * from sklearn.preprocessing import StandardScaler from sklearn import preprocessing","title":"Setup our programming environment"},{"location":"tuto-mastering/vision/#image-acquisition","text":"","title":"Image acquisition"},{"location":"tuto-mastering/vision/#image-from-simulation","text":"It is possible to access the camera's video streams via the Realsense sensor (if it is available) or simulation. Gazebo is capable of simulating robot vision (classical and 3D sensor). Launch a simulation : '''bash roslaunch larm chalenge-2.launch ''' Observe the topic and mainly the one published the images ( camera/rgb/image_raw ) with rostopic list , rostopic info and rostopic hz . Image are published as 'sensor_msgs/Image' message type in camera/rgb/image_raw ROS doc If you echo the messages of the image topic ( rostopic echo camera/rgb/image_raw ), you can observe the following structure of the message std_msgs/Header header uint32 height uint32 width string encoding uint8 is_bigendian uint32 step uint8[] data The pixel values are stored in a uint8 array. Several tools are availbale to convert a ROS image to OpenCV image (for instance cv_bridge )","title":"Image from simulation"},{"location":"tuto-mastering/vision/#image-from-realsense-camera-sensor","text":"The following code is able to connect the Realsense camera and to acquire the depth and the color stream. A configuration step is done. Both depth and color images are aligned to make that all pixels of a depth image correpond to all pixels of the corresponding color image. import pyrealsense2 as rs import numpy as np import math import cv2,time,sys pipeline = rs.pipeline() config = rs.config() colorizer = rs.colorizer() # Configuration of depth and color image size (840x480), the format of the pixel values and the frame rates (30 fps) config.enable_stream(rs.stream.depth, 840, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 840, 480, rs.format.bgr8, 30) pipeline.start(config) align_to = rs.stream.depth align = rs.align(align_to) rayon=10 count=1 refTime= time.process_time() freq= 60 try: while True: # This call waits until a new coherent set of frames is available on a device frames = pipeline.wait_for_frames() # Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() if not depth_frame or not aligned_color_frame: continue # Two ways to colorized the depth map # first : using colorizer of pyrealsense colorized_depth = colorizer.colorize(depth_frame) depth_colormap = np.asanyarray(colorized_depth.get_data()) # Get the intrinsic parameters color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics color_image = np.asanyarray(aligned_color_frame.get_data()) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape # Show images images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (840x480 for example) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL) # Resize the Window cv2.resizeWindow('RealSense', 960, 720) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 except Exception as e: print(e) pass finally: pipeline.stop() You have already done the code to send both depth and color image on a ROS topic ( camera_driver section). During the challenges you will be able to compute both streams via the ROS image topics or directly after image acquisition ( pipeline.wait_for_frames() in the previous code). yYu will have to pay close attention to the time required to ensure the execution of the processing chains of the two video streams in order to guarantee rapid detection.","title":"Image from Realsense camera sensor"},{"location":"tuto-mastering/vision/#basic-codes-using-opencv-framework","text":"This section introduces some of the functions and techniques illustrated by some codes that you can use to complete the proposed challenges. First, you will test each proposal on images downloaded from the Internet or from the Vision section of the tutorial. You will then have to integrate or adapt them to process images from the Realsense camera on board the robot. Some codes example propose to acquire and compute the webcam video stream.","title":"Basic codes using OpenCV framework"},{"location":"tuto-mastering/vision/#segmentation-dimages-couleur-par-seuillage-des-composantes-et-gestion-de-la-souris","text":"Here are a few lines of code to extract a region of interest using the mouse. With these few lines, you can calculate the mean and variance of each component of the image, which is useful for the segmentation stage. In this example, we first acquire an image from the laptop's webcam. We then use the mouse to define a section of the captured image. You can then easily calculate the desired statistical metrics for this section. The mean and variance define a Gaussian model for each component of the clipping. import cv2 import numpy as np # connect to a sensor (0: webcam) cap=cv2.VideoCapture(0) # capture an image ret, frame=cap.read() # Select ROI r = cv2.selectROI(frame) # Crop image imCrop = frame[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])] average_h = np.mean(imCrop[:,:,0]) average_s = np.mean(imCrop[:,:,1]) average_v = np.mean(imCrop[:,:,2]) print(average_h,average_s,average_v) # Display cropped image cv2.imshow(\"Image\", imCrop) cv2.waitKey(0) In this example we will create a mask of pixels whose HSV components are between the variables lo and hi . In this example, by clicking with the left or right mouse button, you decrease or increase the hue h of the two variables lo and hi . You will also see that in our example this two thresholds differ not only in their hue but also in their saturation. You can test this script on an image of your face. These few lines of code also illustrate how to manage actions on the mouse. They manage mouse events such as mouse move (cv2.EVENT_MOUSEMOVE), double middle click (EVENT_MBUTTONDBLCLK), right click (EVENT_RBUTTONDOWN) and left click (EVENT_LBUTTONDOWN). import cv2 import numpy as np def souris(event, x, y, flags, param): global lo, hi, color, hsv_px if event == cv2.EVENT_MOUSEMOVE: # Conversion des trois couleurs RGB sous la souris en HSV px = frame[y,x] px_array = np.uint8([[px]]) hsv_px = cv2.cvtColor(px_array,cv2.COLOR_BGR2HSV) if event==cv2.EVENT_MBUTTONDBLCLK: color=image[y, x][0] if event==cv2.EVENT_LBUTTONDOWN: if color>5: color-=1 if event==cv2.EVENT_RBUTTONDOWN: if color<250: color+=1 lo[0]=color-10 hi[0]=color+10 color=100 lo=np.array([color-5, 100, 50]) hi=np.array([color+5, 255,255]) color_info=(0, 0, 255) cap=cv2.VideoCapture(0) cv2.namedWindow('Camera') cv2.setMouseCallback('Camera', souris) hsv_px = [0,0,0] # Creating morphological kernel kernel = np.ones((3, 3), np.uint8) while True: ret, frame=cap.read() image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) mask=cv2.inRange(image, lo, hi) mask=cv2.erode(mask, kernel, iterations=1) mask=cv2.dilate(mask, kernel, iterations=1) image2=cv2.bitwise_and(frame, frame, mask= mask) cv2.putText(frame, \"Couleur: {:d}\".format(color), (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Affichage des composantes HSV sous la souris sur l'image pixel_hsv = \" \".join(str(values) for values in hsv_px) font = cv2.FONT_HERSHEY_SIMPLEX cv2.putText(frame, \"px HSV: \"+pixel_hsv, (10, 260), font, 1, (255, 255, 255), 1, cv2.LINE_AA) cv2.imshow('Camera', frame) cv2.imshow('image2', image2) cv2.imshow('Mask', mask) if cv2.waitKey(1)&0xFF==ord('q'): break cap.release() cv2.destroyAllWindows() In general, it is very interesting to change the colour space to better target the space in which the object of interest is distinguishable: image=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) After creating the mask with mask=cv2.inRange(image, lo, hi) it is sometimes relevant to denoise the resulting image by blurring or by some morphological operations. This allows the shapes to be closed and filled: # Flouttage de l'image image=cv2.blur(image, (7, 7)) # Erosion d'un mask mask=cv2.erode(mask, None, iterations=4) # dilatation d'un mask mask=cv2.dilate(mask, None, iterations=4) In the segmentation code of a colour image provided earlier, you will play with the kernel size (3x3 in our example), you will add a blurring step for each channel by playing with the neighbourhood size (7x7 in our example). Finally, play with the erosion and dilation steps by varying the number of times each morphological operator is applied (4 times in our example). The segmentation code of a colour image provided previously allows you to define a binary mask of pixels whose HSV components are in the interval [lo,hi] . It is then possible to detect the connected elements in the mask to extract certain information, in this case the minEnclosingCircle. Other functions may be useful. You can find them here: https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html . Assuming that an object of interest is represented by a set of connected pixels whose colour is in the interval [lo,hi] , it is then possible to define constraints on a set of features that allow the classification of the objects thus detected. You will add the following lines to the previous segmentation code. elements=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] if len(elements) > 0: c=max(elements, key=cv2.contourArea) ((x, y), rayon)=cv2.minEnclosingCircle(c) if rayon>30: cv2.circle(image2, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(frame, (int(x), int(y)), 5, color_info, 10) cv2.line(frame, (int(x), int(y)), (int(x)+150, int(y)), color_info, 2) cv2.putText(frame, \"Objet !!!\", (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA)","title":"Segmentation d'images couleur par seuillage des composantes et Gestion de la souris"},{"location":"tuto-mastering/vision/#extraction-de-regions-dans-une-image-binarisee","text":"Here are some lines in Python to extract regions of connected pixels in a binarised image label() . From these regions some properties are extracted regionprops() . This code works strategically in the same way as the previous segmentation script, but using the skimage library. import cv2 import numpy as np import matplotlib.pyplot as plt from skimage.measure import label, regionprops import math image = cv2.imread('./imageasegmenter.jpg') # passage en niveau de gris gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ###### extration des r\u00e9gions avec la lib skimage # Binarisation de l'image ret, thresh = cv2.threshold(gray, 127, 255, 1) cv2.imshow(\"image seuill\u00e9e\",thresh) cv2.waitKey(0) # extraction des r\u00e9gions et des propri\u00e9t\u00e9s des r\u00e9gions label_img = label(thresh) regions = regionprops(label_img) print(regions) cv2.waitKey(0) # affichage des r\u00e9gions et des boites englobantes fig, ax = plt.subplots() ax.imshow(thresh, cmap=plt.cm.gray) for props in regions: y0, x0 = props.centroid orientation = props.orientation x1 = x0 + math.cos(orientation) * 0.5 * props.minor_axis_length y1 = y0 - math.sin(orientation) * 0.5 * props.minor_axis_length x2 = x0 - math.sin(orientation) * 0.5 * props.major_axis_length y2 = y0 - math.cos(orientation) * 0.5 * props.major_axis_length ax.plot((x0, x1), (y0, y1), '-r', linewidth=2.5) ax.plot((x0, x2), (y0, y2), '-r', linewidth=2.5) ax.plot(x0, y0, '.g', markersize=15) minr, minc, maxr, maxc = props.bbox bx = (minc, maxc, maxc, minc, minc) by = (minr, minr, maxr, maxr, minr) ax.plot(bx, by, '-b', linewidth=2.5) ax.axis((0, 600, 600, 0)) plt.show() cv2.waitKey(0)","title":"Extraction de r\u00e9gions dans une image binaris\u00e9e"},{"location":"tuto-mastering/vision/#detection-dobjets-par-template-matching","text":"Il est possible de d\u00e9tecter un ou plusieurs objets dans une image en appliquant une proc\u00e9dure de matching d'un template de chaque objet \u00e0 d\u00e9tecter. Un template est une image du ou des objets en question. La fonction \u00e0 utiliser est cv.matchTemplate(img_gray,template,parametre) . Plusieurs parametre de matching sont possibles correspondant chacun \u00e0 une m\u00e9trique de corr\u00e9lation. Voici les lignes de codes que vous testerez. Vous testerez les parametres suivants afin de d\u00e9finir celui qui fournit les meilleurs r\u00e9sultats. Par ailleurs, vous adapterez le code afin de prendre un charge le flux des images de la Realsense et une image template de l'objet que vous voulez d\u00e9tect\u00e9. It is possible to detect one or more objects in an image by applying a matching procedure to a template of each object to be detected. A template is one or several images of the object to be detected. The function to use is cv.matchTemplate(img_gray,template,parameter) . Several matching parameters are possible, each corresponding to a correlation metric (cf. https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html ). Here are the lines of code you will test. You will test the following parameters to determine the one that gives the best results. You will also adapt the code to take a load of the Realsense image stream and a template image of the object you want to detect. import cv2 as cv import numpy as np from matplotlib import pyplot as plt # charger l'image dans laquelle on cherche l'objet img_rgb = cv.imread('car.png') img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY) # charger le template de l'objet \u00e0 rechercher template = cv.imread('template.png',0) # R\u00e9cup\u00e9ration des dimensions de l'image w, h = template.shape[::-1] # Application du template atching res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED) # S\u00e9lection des meilleurs matched objects threshold = 0.8 loc = np.where( res >= threshold) # Affichage de la boite englobante de chaque objet d\u00e9tect\u00e9 for pt in zip(*loc[::-1]): cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)","title":"D\u00e9tection d'objets par template matching"},{"location":"tuto-mastering/vision/#segmentation-des-images-par-la-methodes-des-k-moyennes-kmeans","text":"Kmeans is a clustering algorithm whose objective is to divide a set of data points into several clusters. Each of point is assigned to a cluster with the nearest mean. The mean of each group is called the \"centroid\" or \"centre\". In total, k-means gives k distinct clusters of the original n data points. Data points within a particular cluster are considered to be \"more similar\" to each other than data points belonging to other groups. This algorithm can be applied to geometric, colourimetric and other original points. We will use this method to provide a colour segmentation of an image, i.e. it amounts to finding the domain colours in the image. from sklearn.cluster import KMeans import matplotlib.pyplot as plt import cv2 import numpy as np #Ensuite charger une image et la convertir de BGR \u00e0 RGB si n\u00e9cessaire et l\u2019afficher : image = cv2.imread('lena.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.figure() plt.axis(\"off\") plt.imshow(image) In order to compute the image as a data point, it must be converted from a raster form to a vector form (rgb color list) before applying the clustering function: n_clusters=5 image = image.reshape((image.shape[0] * image.shape[1], 3)) clt = KMeans(n_clusters = n_clusters ) clt.fit(image) To display the most dominant colours in the image, two functions must be defined: centroid_histogram() to retrieve the number of different clusters and create a histogram based on the number of pixels assigned to each cluster; and plot_colors() to initialise the bar graph representing the relative frequency of each colour. def centroid_histogram(clt): numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)>>> (hist, _) = np.histogram(clt.labels_, bins=numLabels) # normalize the histogram, such that it sums to one hist = hist.astype(\"float\") hist /= hist.sum() return hist def plot_colors(hist, centroids): bar = np.zeros((50, 300, 3), dtype=\"uint8\") startX = 0 # loop over the percentage of each cluster and the color of # each cluster for (percent, color) in zip(hist, centroids): # plot the relative percentage of each cluster endX = startX + (percent * 300) cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(\"uint8\").tolist(), -1) startX = endX return bar Now you just need to build a cluster histogram and then create a figure displaying the number of pixels labeled for each color. hist = centroid_histogram(clt) bar = plot_colors(hist, clt.cluster_centers_) plt.figure() plt.axis(\"off\") plt.imshow(bar) plt.show()","title":"Segmentation des images par la m\u00e9thodes des k-moyennes (kmeans)"},{"location":"tuto-mastering/vision/#classification-dimages-par-la-mathode-des-k-plus-proches-voisins-k-nn-ou-knn","text":"This exercise will allow you to learn a model from images in the CIFAR-10 database, downloadable here: https://www.cs.toronto.edu/~kriz/cifar.html . Unzip the files into a folder that you will use in the following script. As you can see, the CIFAR images are contained in the ./data folder. import numpy as np import cv2 basedir_data = \"./data/\" rel_path = basedir_data + \"cifar-10-batches-py/\" #D\u00e9s\u00e9rialiser les fichiers image afin de permettre l\u2019acc\u00e8s aux donn\u00e9es et aux labels: def unpickle(file): import pickle with open(file, 'rb') as fo: dict = pickle.load(fo,encoding='bytes') return dict X = unpickle(rel_path + 'data_batch_1') img_data = X[b'data'] img_label_orig = img_label = X[b'labels'] img_label = np.array(img_label).reshape(-1, 1) You can use teh following to check the r\u00e9sults : print(img_data) print('shape', img_data.shape) print(img_label) print('shape', img_label.shape) You should find a 10000x3072 numpy array of uint8s (the 3072 comes from the 3 x 1024). Each row of the array stores a 32x32 RGB colour image. The images are stored in row-major order, so the first 32 entries in the array correspond to the red channel values of the first row of the image. The labels are stored in a 10000 x 1 array. To check the labels: To load the test data, use the same procedure as before because the shape of the test data is the same as the shape of the training data: test_X = unpickle(rel_path + 'test_batch'); test_data = test_X[b'data'] test_label = test_X[b'labels'] test_label = np.array(test_label).reshape(-1, 1) print(test_data) print('shape', test_data.shape) print(test_label) print('shape', test_label.shape) Please note that the RGB components of the images are arranged in the form of a 1-dimensional vector. To display each image, it is therefore necessary to put it back in the form of a 2D RGB image. To do this, we operate as follows, considering that the images are of 32x32 resolution: one_img=sample_img_data[0,:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) cv2.imshow('Image CIFAR',rgb) cv2.waitKey(0) cv2.destroyAllWindows() Now, you will apply the k-NN algorithm on all the images of the training base img_data and their labels img_label_orig. You will use the fubnction KNeighborsClassifier of the sklearn python modules. from sklearn.neighbors import KNeighborsClassifier #def pred_label_fn(i, original): # return original + '::' + meta[YPred[i]].decode('utf-8') nbrs = KNeighborsClassifier(n_neighbors=3, algorithm='brute').fit(img_data, img_label_orig) # test sur les 10 premi\u00e8res images data_point_no = 10 sample_test_data = test_data[:data_point_no, :] YPred = nbrs.predict(sample_test_data) for i in range(0, len(YPred)): #show_im(sample_test_data, test_label, meta, i, label_fn=pred_label_fn) r = sample_test_data[i][:1024].reshape(32, 32) g = sample_test_data[i][1024:2048].reshape(32, 32) b = sample_test_data[i][2048:].reshape(32, 32) print(YPred[i]) cv2.imshow('image test',np.dstack([r, g, b])) neigh_dist,neigh_ind = nbrs.kneighbors([sample_test_data[i]]) print(neigh_ind) for j in range(0, len(neigh_ind[0])): one_img=img_data[neigh_ind[0][j],:] r = one_img[:1024].reshape(32, 32) g = one_img[1024:2048].reshape(32, 32) b = one_img[2048:].reshape(32, 32) rgb = np.dstack([r, g, b]) cv2.imshow('K plus proche image',np.dstack([r, g, b])) cv2.waitKey(0)","title":"Classification d'images par la mathode des K plus proches voisins (k-NN ou KNN)"},{"location":"tuto-mastering/vision/#gestion-de-la-depth-map-et-estimation-de-la-distance","text":"This code measures the distance of the parts of the scene that are projected at each pixel of the camera by combining the depth image and the color image. It uses the following new part of code which was then integrated into the code you have already tested before. # Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) The complete code integrating the acquisition and visualization part is as follows. The position is marked by a circle in both streams with the measured distance: import pyrealsense2 as rs import numpy as np import math import cv2,time,sys pipeline = rs.pipeline() config = rs.config() colorizer = rs.colorizer() # fps plus bas (30) config.enable_stream(rs.stream.depth, 840, 480, rs.format.z16, 30) config.enable_stream(rs.stream.color, 840, 480, rs.format.bgr8, 30) pipeline.start(config) align_to = rs.stream.depth align = rs.align(align_to) color_info=(0, 0, 255) rayon=10 count=1 refTime= time.process_time() freq= 60 try: while True: # This call waits until a new coherent set of frames is available on a device frames = pipeline.wait_for_frames() #Aligning color frame to depth frame aligned_frames = align.process(frames) depth_frame = aligned_frames.get_depth_frame() aligned_color_frame = aligned_frames.get_color_frame() if not depth_frame or not aligned_color_frame: continue # Two ways to colorized the depth map # first : using colorizer of pyrealsense colorized_depth = colorizer.colorize(depth_frame) depth_colormap = np.asanyarray(colorized_depth.get_data()) # second : using opencv by applying colormap on depth image (image must be converted to 8-bit per pixel first) #depth_image = np.asanyarray(depth_frame.get_data()) #depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) # Get the intrinsic parameters color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics color_image = np.asanyarray(aligned_color_frame.get_data()) depth_colormap_dim = depth_colormap.shape color_colormap_dim = color_image.shape #Use pixel value of depth-aligned color image to get 3D axes x, y = int(color_colormap_dim[1]/2), int(color_colormap_dim[0]/2) depth = depth_frame.get_distance(x, y) dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [x,y], depth) distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2)) #print(\"Distance from camera to pixel:\", distance) #print(\"Z-depth from camera surface to pixel surface:\", depth) # Show images images = np.hstack((color_image, depth_colormap)) # supose that depth_colormap_dim == color_colormap_dim (640x480) otherwize: resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA) cv2.circle(images, (int(x), int(y)), int(rayon), color_info, 2) cv2.circle(images, (int(x+color_colormap_dim[1]), int(y)), int(rayon), color_info, 2) # Affichage distance au pixel (x,y) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x)+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) cv2.putText(images, \"D=\"+str(round(distance,2)), (int(x+color_colormap_dim[1])+10, int(y) -10), cv2.FONT_HERSHEY_DUPLEX, 1, color_info, 1, cv2.LINE_AA) # Show images cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL) # Resize the Window cv2.resizeWindow('RealSense', 960, 720) cv2.imshow('RealSense', images) cv2.waitKey(1) # Frequency: if count == 10 : newTime= time.process_time() freq= 10/((newTime-refTime)) refTime= newTime count= 0 count+= 1 except Exception as e: print(e) pass finally: pipeline.stop()","title":"Gestion de la depth map et estimation de la distance"}]}